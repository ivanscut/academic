<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>BDA on Dr. Zhijian He</title>
    <link>/tags/bda/</link>
    <description>Recent content in BDA on Dr. Zhijian He</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Zhijian He &amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 18 Nov 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/bda/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Approximate Bayesian Computation</title>
      <link>/post/abc/</link>
      <pubDate>Sun, 18 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/abc/</guid>
      <description>Bayesian inferenceOur goal is to estimate an expectation of \(a(\theta)\) under the posterior distribution
\[\pi(\theta|y_{obs})=\frac{\pi(\theta)p(y_{obs}|\theta)}{p(y_{obs})}\propto \pi(\theta)p(y_{obs}|\theta)\]
\(\pi(\theta)\) is the prior distribution
\(p(y|\theta)\) is the likelihood function
the constant \(p(y_{obs})\) is intractable
Suppose that one can generate samples \(\theta^{(1)},\dots,\theta^{(N)}\sim \pi(\theta|y_{obs})\), then
\[E[a(\theta)|y_{obs}]\approx \frac 1 N\sum_{i=1}^N a(\theta^{(i)})\]
This is the basic idea of Monte Carlo (MC).
Acceptance rejection (AR) methodsthe target density \(f(\theta)\)</description>
    </item>
    
    <item>
      <title>Quasi-Monte Carlo in ABC</title>
      <link>/post/qmc_abc/</link>
      <pubDate>Sun, 18 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/qmc_abc/</guid>
      <description>Ingredients for ABCsummary statistic \(S(y):\mathbb{R}^n\to \mathbb{R}^d\)
kernel function \(k(u_1,\dots,u_d)\)
bandwidth \(h&amp;gt;0\)
proposal density \(g(\theta)\)
ABC approximation
\[\pi_{ABC}(\theta|s_{obs})\propto \pi(\theta)\int \pi(s|\theta)K((s-s_{obs})/h) d s\to \pi(\theta|s_{obs})\]
If \(\pi(\theta|s_{obs})\approx \pi(\theta|y)\), then ABC density is a proper approximation of the posterior \(\pi(\theta|y)\).
ABC convergence ratesConsider the estimation of \(\mu=E[a(\theta)|s_{obs}]\). The acceptance-rejection (AR) based ABC estimate is given by\[\hat\mu=\frac{1}{N}\sum_{i=1}^Na(\theta^{(i)}).\]
Under regular conditions, ABC bias is
\[\mathrm{bias}=|E_{ABC}[a(\theta)|s_{obs}]-\mu|=O(h^2),\]
and the acceptance probability is \(R = O(h^{d}),\)</description>
    </item>
    
    <item>
      <title>Chapter 10: Bayesian computation</title>
      <link>/post/bayes_chap10/</link>
      <pubDate>Mon, 22 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/bayes_chap10/</guid>
      <description>Introduction to Bayesian computationThe goals are to estimate
the posterior distribution \(p(\theta|y)\propto p(\theta)p(y|\theta)\)
the posterior predictive distribution\[p(\tilde y|y) = \int p(\tilde y|\theta)p(\theta|y)d \theta =E[p(\tilde y|\theta)|y]\]
We are therefore insterested in estimating the posterior expectation\[\mu=E[h(\theta)|y]=\int h(\theta)p(\theta|y)d \theta\]
moments: \(h(\theta)=\theta^k\)probability: \(h(\theta)=1_A(\theta)\), \(A\subseteq \Theta\)predictive density: \(h(\theta)=p(\tilde y|\theta)\) for fixed \(\tilde y\)Monte Carlo methodsSuppose we can simulate \(\theta^{(i)},\dots,\theta^{(N)}\sim p(\theta|y)\) independently. Monte Carlo (MC) esimate is then the sample average:\[\hat{\mu}_N = \frac1N\sum_{i=1}^N h(\theta^{(i)})\]</description>
    </item>
    
    <item>
      <title>Chapter 5: Hierarchial models</title>
      <link>/post/bayes_chap05/</link>
      <pubDate>Sun, 07 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/bayes_chap05/</guid>
      <description>Introduction to hierarchial modelsMany statistical applications involve multiple parameters (say, \(\theta_1,\dots,\theta_J\)) that can be regarded as related or connected in some way by the structure of the problem.
for the group \(j\in 1{:}J\), we have the observed data \(y_{ij}\), \(i=1,\dots,n_j\) from the population distribution with unknown parameter \(\theta_j\)
we use a prior distribution in which the \(\theta_j\)’s are viewed as a sample from a common population distribution, say \(p(\theta|\phi)\), where \(\phi\) is known as hyperparameters.</description>
    </item>
    
    <item>
      <title>Chapter 4: Asymptotics and connections to non-Bayesian approaches</title>
      <link>/post/bayes_chap04/</link>
      <pubDate>Sat, 06 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/bayes_chap04/</guid>
      <description>Large-sample theoryAssumptions and notations:
true distribution: \(y_i\stackrel {iid}{\sim} f(\cdot)\)\(\theta\in\Theta\)prior distribution: \(p(\theta)\)model distribution: \(p(y_i|\theta)\)Kullback-Leibler divergence: a measure of ‘discrepancy’ between the model and the true distribution\[KL(\theta)= E\left[\log\left(\frac{f(y_i)}{p(y_i|\theta)}\right)\right]=\int \log\left(\frac{f(y_i)}{p(y_i|\theta)}\right)f(y_i)dy_i\]
\(\theta_0\): the unique minimizer of \(KL(\theta)\)if \(f(y_i) = p(y_i|\theta)\) then \(\theta=\theta_0\)
Convergence of the posterior distributionDiscrete parmeter space: If the parameter space \(\Theta\) is finite and \(P(\theta=\theta_0)&amp;gt;0\), then\[P(\theta=\theta_0|y)\to 1\text{ as }n\to \infty,\]where \(\theta_0=\arg_{\theta\in\Theta} KL(\theta)\).</description>
    </item>
    
    <item>
      <title>Chapter 3: Introduction to multiparameter models</title>
      <link>/post/bayes_chap03/</link>
      <pubDate>Fri, 05 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/bayes_chap03/</guid>
      <description>Nuisance parametersthere are more than one unknown or unobservable parametersconclusions will often be drawn about one, or only a few parameters at a timethere is no interest in making inferences about many of the unknown parameters – nuisance parameters
suppose \(\theta=(\theta_1,\theta_2)\)interest centers only on \(\theta_1\); \(\theta_2\) is a ‘nuisance’ parameter.the joint posterior density:\[p(\theta_1,\theta_2|y)\propto p(y|\theta_1,\theta_2)p(\theta_1,\theta_2)\]
the marginal posterior density:\[p(\theta_1|y)=\int p(\theta_1,\theta_2|y)d\theta_2=\int p(\theta_1|\theta_2,y)p(\theta_2|y)d\theta_2\]</description>
    </item>
    
    <item>
      <title>Chapter 2: Single-parameter models</title>
      <link>/post/bayes_chap02/</link>
      <pubDate>Thu, 30 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/bayes_chap02/</guid>
      <description>2.1 Binomial modelsEstimating a probability from binomial datalet \(\theta\) be the proportion of successes in the populationthe data \((y_1,\dots,y_n)\in \{0,1\}^n\)the total number of successes in the \(n\) trials is denoted by \(y\)the binomial model is\[p(y|\theta) = C_n^y\theta^y(1-\theta)^{n-y}\]
the posterior distribution is\[p(\theta|y) \propto p(\theta)p(y|\theta)\propto p(\theta)\theta^y(1-\theta)^{n-y}\]
Example: estimating the probability of a female birth
A total of 241,945 girls and 251,527 boys were born in Paris from 1745 to 1770.</description>
    </item>
    
    <item>
      <title>Chapter 1: Probability and Inference</title>
      <link>/post/bayes_chap01/</link>
      <pubDate>Wed, 29 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/bayes_chap01/</guid>
      <description>3 steps in BDAset up the statistical model
compute the posterior distribution
model checking and model improvement
Statistical inferenceGoal: draw conclusions about unobserved quantities from the data (observed)
potentially observable quantities, e.g., future observations of a process
not directly observable quantities, e.g., unobservable population parameters
Notations and assumptionsunobservable population parameters of interest: \(\theta=(\theta_1,\dots,\theta_m)\)
the observed data: \(y=(y_1,\dots,y_n)\)</description>
    </item>
    
  </channel>
</rss>