<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Slides on Dr. Zhijian He</title>
    <link>/tags/slides/</link>
    <description>Recent content in Slides on Dr. Zhijian He</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Mon, 22 Oct 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/slides/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Chapter 10: Bayesian computation</title>
      <link>/post/bayes_chap10/</link>
      <pubDate>Mon, 22 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/bayes_chap10/</guid>
      <description>Introduction to Bayesian computationThe goals are to estimate
the posterior distribution \(p(\theta|y)\propto p(\theta)p(y|\theta)\)
the posterior predictive distribution\[p(\tilde y|y) = \int p(\tilde y|\theta)p(\theta|y)d \theta =E[p(\tilde y|\theta)|y]\]
We are therefore insterested in estimating the posterior expectation\[\mu=E[h(\theta)|y]=\int h(\theta)p(\theta|y)d \theta\]
moments: \(h(\theta)=\theta^k\)probability: \(h(\theta)=1_A(\theta)\), \(A\subseteq \Theta\)predictive density: \(h(\theta)=p(\tilde y|\theta)\) for fixed \(\tilde y\)Monte Carlo methodsSuppose we can simulate \(\theta^{(i)},\dots,\theta^{(N)}\sim p(\theta|y)\) independently. Monte Carlo (MC) esimate is then the sample average:\[\hat{\mu}_N = \frac1N\sum_{i=1}^N h(\theta^{(i)})\]</description>
    </item>
    
    <item>
      <title>Chapter 5: Hierarchial models</title>
      <link>/post/bayes_chap05/</link>
      <pubDate>Sun, 07 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/bayes_chap05/</guid>
      <description>Introduction to hierarchial modelsMany statistical applications involve multiple parameters (say, \(\theta_1,\dots,\theta_J\)) that can be regarded as related or connected in some way by the structure of the problem.
for the group \(j\in 1{:}J\), we have the observed data \(y_{ij}\), \(i=1,\dots,n_j\) from the population distribution with unknown parameter \(\theta_j\)
we use a prior distribution in which the \(\theta_j\)’s are viewed as a sample from a common population distribution, say \(p(\theta|\phi)\), where \(\phi\) is known as hyperparameters.</description>
    </item>
    
    <item>
      <title>Chapter 4: Asymptotics and connections to non-Bayesian approaches</title>
      <link>/post/bayes_chap04/</link>
      <pubDate>Sat, 06 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/bayes_chap04/</guid>
      <description>Large-sample theoryAssumptions and notations:
true distribution: \(y_i\stackrel {iid}{\sim} f(\cdot)\)\(\theta\in\Theta\)prior distribution: \(p(\theta)\)model distribution: \(p(y_i|\theta)\)Kullback-Leibler divergence: a measure of ‘discrepancy’ between the model and the true distribution\[KL(\theta)= E\left[\log\left(\frac{f(y_i)}{p(y_i|\theta)}\right)\right]=\int \log\left(\frac{f(y_i)}{p(y_i|\theta)}\right)f(y_i)dy_i\]
\(\theta_0\): the unique minimizer of \(KL(\theta)\)if \(f(y_i) = p(y_i|\theta)\) then \(\theta=\theta_0\)
Convergence of the posterior distributionDiscrete parmeter space: If the parameter space \(\Theta\) is finite and \(P(\theta=\theta_0)&amp;gt;0\), then\[P(\theta=\theta_0|y)\to 1\text{ as }n\to \infty,\]where \(\theta_0=\arg_{\theta\in\Theta} KL(\theta)\).</description>
    </item>
    
    <item>
      <title>Chapter 3: Introduction to multiparameter models</title>
      <link>/post/bayes_chap03/</link>
      <pubDate>Fri, 05 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/bayes_chap03/</guid>
      <description>Nuisance parametersthere are more than one unknown or unobservable parametersconclusions will often be drawn about one, or only a few parameters at a timethere is no interest in making inferences about many of the unknown parameters – nuisance parameters
suppose \(\theta=(\theta_1,\theta_2)\)interest centers only on \(\theta_1\); \(\theta_2\) is a ‘nuisance’ parameter.the joint posterior density:\[p(\theta_1,\theta_2|y)\propto p(y|\theta_1,\theta_2)p(\theta_1,\theta_2)\]
the marginal posterior density:\[p(\theta_1|y)=\int p(\theta_1,\theta_2|y)d\theta_2=\int p(\theta_1|\theta_2,y)p(\theta_2|y)d\theta_2\]</description>
    </item>
    
    <item>
      <title>Chapter 2: Single-parameter models</title>
      <link>/post/bayes_chap02/</link>
      <pubDate>Thu, 30 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/bayes_chap02/</guid>
      <description>2.1 Binomial modelsEstimating a probability from binomial datalet \(\theta\) be the proportion of successes in the populationthe data \((y_1,\dots,y_n)\in \{0,1\}^n\)the total number of successes in the \(n\) trials is denoted by \(y\)the binomial model is\[p(y|\theta) = C_n^y\theta^y(1-\theta)^{n-y}\]
the posterior distribution is\[p(\theta|y) \propto p(\theta)p(y|\theta)\propto p(\theta)\theta^y(1-\theta)^{n-y}\]
Example: estimating the probability of a female birth
A total of 241,945 girls and 251,527 boys were born in Paris from 1745 to 1770.</description>
    </item>
    
    <item>
      <title>Chapter 1: Probability and Inference</title>
      <link>/post/bayes_chap01/</link>
      <pubDate>Wed, 29 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/bayes_chap01/</guid>
      <description>3 steps in BDAset up the statistical model
compute the posterior distribution
model checking and model improvement
Statistical inferenceGoal: draw conclusions about unobserved quantities from the data (observed)
potentially observable quantities, e.g., future observations of a process
not directly observable quantities, e.g., unobservable population parameters
Notations and assumptionsunobservable population parameters of interest: \(\theta=(\theta_1,\dots,\theta_m)\)
the observed data: \(y=(y_1,\dots,y_n)\)</description>
    </item>
    
  </channel>
</rss>