<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 2.4.0">
  <meta name="generator" content="Hugo 0.48" />
  <meta name="author" content="Zhijian He">

  
  
  
  
    
  
  <meta name="description" content="Simple linear models The linear model is given by $$y_i=\beta_0&#43;\beta_1x_i&#43;\epsilon_i,\ i=1,\dots,n.$$
 $\epsilon_i$ are random (need some assumptions) $x_i$ are fixed (independent/preditor variable) $y_i$ are random (dependent/response variable) $\beta_0$ is the intercept $\beta_1$ is the slope  Least square estimators Choose $\beta_0,\beta_1$ to minimize $$Q(\beta_0,\beta1) = \sum{i=1}^n(y_i-\beta_0-\beta_1x_i)^2.$$
The minimizers $\hat\beta_0,\hat\beta_1$ satisfy $$ \begin{cases} \frac{\partial Q}{\partial \beta0} = -2\sum{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)=0
\frac{\partial Q}{\partial \beta1} = -2\sum{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)x_i=0 \end{cases} $$
This gives $$\hat\beta1 = \frac{\sum{i=1}^n(y_i-\bar y)xi}{\sum{i=1}^n(x_i-\bar x)x_i},\ \hat\beta_0=\bar y-\hat\beta_1\bar x.">

  
  <link rel="alternate" hreflang="en-us" href="/post/chap04.utf8/">

  


  

  
  
  
  <meta name="theme-color" content="#0095eb">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha256-eSi1q2PG6J7g7ib17yAaWMcrr5GrtohYChqibrV7PBE=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" crossorigin="anonymous">
        
      
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono">
  

  <link rel="stylesheet" href="/styles.css">
  

  
  
  

  
  <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Dr. Zhijian He">
  <link rel="feed" href="/index.xml" type="application/rss+xml" title="Dr. Zhijian He">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/post/chap04.utf8/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Dr. Zhijian He">
  <meta property="og:url" content="/post/chap04.utf8/">
  <meta property="og:title" content="Chap 4 | Dr. Zhijian He">
  <meta property="og:description" content="Simple linear models The linear model is given by $$y_i=\beta_0&#43;\beta_1x_i&#43;\epsilon_i,\ i=1,\dots,n.$$
 $\epsilon_i$ are random (need some assumptions) $x_i$ are fixed (independent/preditor variable) $y_i$ are random (dependent/response variable) $\beta_0$ is the intercept $\beta_1$ is the slope  Least square estimators Choose $\beta_0,\beta_1$ to minimize $$Q(\beta_0,\beta1) = \sum{i=1}^n(y_i-\beta_0-\beta_1x_i)^2.$$
The minimizers $\hat\beta_0,\hat\beta_1$ satisfy $$ \begin{cases} \frac{\partial Q}{\partial \beta0} = -2\sum{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)=0
\frac{\partial Q}{\partial \beta1} = -2\sum{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)x_i=0 \end{cases} $$
This gives $$\hat\beta1 = \frac{\sum{i=1}^n(y_i-\bar y)xi}{\sum{i=1}^n(x_i-\bar x)x_i},\ \hat\beta_0=\bar y-\hat\beta_1\bar x.">
  <meta property="og:locale" content="en-us">
  
  
  
  

  

  

  <title>Chap 4 | Dr. Zhijian He</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Dr. Zhijian He</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/post">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#publications_selected">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  


  <div class="article-container">
    <h1 itemprop="name">Chap 4</h1>

    

<div class="article-metadata">

  
  
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Zhijian He">
  </span>
  

  <span class="article-date">
    
    <meta content="" itemprop="datePublished">
    <time datetime="" itemprop="dateModified">
      Jan 1, 0001
    </time>
  </span>
  <span itemscope itemprop="publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Zhijian He">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    10 min read
  </span>
  

  
  

  

  
  

  

</div>


    <div class="article-style" itemprop="articleBody">
      

<h2 id="simple-linear-models">Simple linear models</h2>

<p>The linear model is given by
$$y_i=\beta_0+\beta_1x_i+\epsilon_i,\ i=1,\dots,n.$$</p>

<ul>
<li>$\epsilon_i$ are random (need some assumptions)</li>
<li>$x_i$ are <strong>fixed</strong> (<em>independent/preditor</em> variable)</li>
<li>$y_i$ are random (<em>dependent/response</em> variable)</li>
<li>$\beta_0$ is the <em>intercept</em></li>
<li>$\beta_1$ is the <em>slope</em></li>
</ul>

<h3 id="least-square-estimators">Least square estimators</h3>

<p>Choose $\beta_0,\beta_1$ to minimize
$$Q(\beta_0,\beta<em>1) = \sum</em>{i=1}^n(y_i-\beta_0-\beta_1x_i)^2.$$</p>

<p>The minimizers $\hat\beta_0,\hat\beta_1$ satisfy
$$
\begin{cases}
\frac{\partial Q}{\partial \beta<em>0} = -2\sum</em>{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)=0<br />
\frac{\partial Q}{\partial \beta<em>1} = -2\sum</em>{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)x_i=0
\end{cases}
$$</p>

<p>This gives
$$\hat\beta<em>1 = \frac{\sum</em>{i=1}^n(y_i-\bar y)x<em>i}{\sum</em>{i=1}^n(x_i-\bar x)x_i},\ \hat\beta_0=\bar y-\hat\beta_1\bar x.$$</p>

<p>Define $$\ell<em>{xx} = \sum</em>{i=1}^n(x<em>i-\bar x)^2,$$ $$\ell</em>{yy} = \sum_{i=1}^n(y<em>i-\bar y)^2,$$ $$\ell</em>{xy} = \sum_{i=1}^n(x_i-\bar x)(y_i-\bar y).$$
We thus have
$$\hat\beta<em>1 = \frac{\sum</em>{i=1}^n(y_i-\bar y)(x<em>i-\bar x)}{\sum</em>{i=1}^n(x_i-\bar x)(x<em>i-\bar x)}=\frac{\ell</em>{xy}}{\ell<em>{xx}}=\frac{1}{\ell</em>{xx}}\sum_{i=1}^n(x_i-\bar x)y_i.$$</p>

<p><code>Regression function</code>: $\hat y=\hat\beta_0+\hat\beta_1x$.</p>

<h3 id="expected-values">Expected values</h3>

<p><code>Assumption A1</code>: $E[\epsilon_i]=0,i=1,\dots,n$.</p>

<p><code>Theorem 1</code>: Under Assumption A1, $\hat\beta_0,\hat\beta_1$ are unbiased estimators for $\beta_0,\beta_1$, respectively.</p>

<p><code>Proof</code>:
$$
\begin{align}
E[\hat \beta<em>1] &amp;= \frac{1}{\ell</em>{xx}}\sum_{i=1}^n(x_i-\bar x)E[y<em>i]<br />
&amp;=\frac{1}{\ell</em>{xx}}\sum_{i=1}^n(x_i-\bar x)(\beta_0+\beta_1x_i)<br />
&amp;=\frac{\beta<em>0}{\ell</em>{xx}}\sum_{i=1}^n(x_i-\bar x)+\frac{\beta<em>1}{\ell</em>{xx}}\sum_{i=1}^n(x_i-\bar x)x_i<br />
&amp;=\frac{\beta<em>1}{\ell</em>{xx}}\sum_{i=1}^n(x_i-\bar x)(x_i-\bar x)<br />
&amp;=\beta_1
\end{align}
$$</p>

<p>$$
\begin{align}
E[\hat \beta_0] &amp;= E[\bar y-\hat\beta_1\bar x]=E[\bar y]-\beta_1\bar x=\beta_0+\beta_1\bar x-\beta_1\bar x=\beta_0.
\end{align}
$$</p>

<h3 id="variances">Variances</h3>

<p><code>Assumption A2</code>: $Cov(\epsilon_i,\epsilon_j)=\sigma^21{i=j}$.</p>

<p><code>Theorem 2</code>: Under Assumption A2, we have
$$Var[\hat\beta<em>0] = \left(\frac 1n+\frac{\bar x^2}{\ell</em>{xx}}\right)\sigma^2,$$</p>

<p>$$Var[\hat\beta<em>1] =\frac{\sigma^2}{\ell</em>{xx}},$$</p>

<p>$$Cov(\hat\beta_0,\hat\beta<em>1) = \frac{-\bar x}{\ell</em>{xx}}\sigma^2.$$</p>

<p><code>Proof</code>: Since $Cov(\epsilon_i,\epsilon_j)=0$ for any $i\neq j$, $Cov(y_i,y_j)=0$. We thus have
$$
\begin{align}
Var[\hat\beta<em>1] &amp;= \frac{1}{\ell</em>{xx}^2}\sum_{i=1}^n(x_i-\bar x)^2Var[y<em>i]<br />
&amp;= \frac{\sigma^2}{\ell</em>{xx}^2}\sum_{i=1}^n(x<em>i-\bar x)^2=\frac{\sigma^2}{\ell</em>{xx}}.
\end{align}
$$</p>

<p>We next show that $Cov(\bar y,\hat \beta_1)=0$.
$$
\begin{align}
Cov(\bar y,\hat \beta<em>1) &amp;= Cov\left(\frac{1}{n}\sum</em>{i=1}^n y<em>i,\frac{1}{\ell</em>{xx}}\sum_{i=1}^n(x_i-\bar x)y<em>i\right)<br />
&amp;=\frac{1}{n\ell</em>{xx}}Cov\left(\sum_{i=1}^n y<em>i,\sum</em>{i=1}^n(x_i-\bar x)y<em>i\right)<br />
&amp;=\frac{1}{n\ell</em>{xx}}\sum_{i=1}^nCov(y_i,(x_i-\bar x)y<em>i)<br />
&amp;=\frac{1}{n\ell</em>{xx}}\sum_{i=1}^n(x_i-\bar x)\sigma^2 = 0.
\end{align}
$$</p>

<p>$$Var[\hat\beta_0] = Var[\bar y-\hat\beta_1\bar x]=Var[\bar y]+Var[\hat \beta<em>1\bar x]=\frac{\sigma^2}{n}+\frac{\bar x^2\sigma^2}{\ell</em>{xx}}.$$</p>

<p>$$Cov(\hat\beta_0,\hat\beta_1) = Cov(\bar y-\hat\beta_1\bar x,\hat\beta_1) = -\bar x Var[\hat\beta<em>1]=\frac{-\bar x}{\ell</em>{xx}}\sigma^2.$$</p>

<blockquote>
<p>So bigger $n$ is better. Get a bigger sample size if you can. Smaller $\sigma$ is better. The most interesting one is that bigger $\ell_{xx}$ is better. The more spread out the $x_i$ are the better we can
estimate the slope $\beta_1$. When you&rsquo;re picking the $x_i$, if you can spread them out more, then it is more informative.</p>
</blockquote>

<h3 id="estimation-of-sigma-2">Estimation of $\sigma^2$</h3>

<p>For Assumption A2, it is common that the variance $\sigma^2$ is unknown.
The next theorem gives an unbiased estimate of $\sigma^2$.</p>

<p><code>Definition</code>: The residual sum of squares (RSS) is defined by
$$\mathrm{RSS} = \sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)^2.$$</p>

<p><code>Theorem 3</code>: Let
$$\hat{\sigma}^2 := \frac{Q(\hat \beta_0,\hat\beta<em>1)}{n-2}=\frac{\sum</em>{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)^2}{n-2}=\frac{\mathrm{RSS}}{n-2}.$$
Under Assumptions A1 and A2, we have $E[\hat\sigma^2]=\sigma^2$.</p>

<p><code>Proof</code>: Let $\hat y_i = \hat\beta_0+\hat\beta_1x_i=\bar y+\hat\beta_1(x_i-\bar x)$.</p>

<p>$$
\begin{align}
E[Q(\hat \beta_0,\hat\beta<em>1)] &amp;= \sum</em>{i=1}^nE[(y_i-\hat y<em>i)^2]=\sum</em>{i=1}^nVar[y_i-\hat y_i]+(E[y_i]-E[\hat y<em>i])^2<br />
&amp;=\sum</em>{i=1}^n[Var[y_i]+Var[\hat y_i]-2Cov(y_i,\hat y_i)].
\end{align}
$$</p>

<p>$$
\begin{align}
Var[\hat y_i]&amp;= Var[\hat\beta_0+\hat\beta_1x_i]=Var[\bar y+\hat\beta_1(x_i-\bar x)]<br />
&amp;=Var[\bar y]+(x_i-\bar x)^2Var[\hat\beta_1]<br />
&amp;=\frac{\sigma^2}{n}+\frac{(x<em>i-\bar x)^2\sigma^2}{\ell</em>{xx}}.
\end{align}$$</p>

<p>$$
\begin{align}
Cov(y_i,\hat y_i)  &amp;= Cov(\beta_0+\beta_1x_i+\epsilon_i, \bar y+\hat\beta_1(x_i-\bar x))<br />
&amp;=Cov(\epsilon_i,\bar y)+(x_i-\bar x)Cov(\epsilon_i,\hat\beta_1)<br />
&amp;=\frac{\sigma^2}{n}+\frac{(x<em>i-\bar x)^2\sigma^2}{\ell</em>{xx}}.
\end{align}
$$</p>

<p>As a result, we have
$$E[Q(\hat \beta_0,\hat\beta<em>1)] = \sum</em>{i=1}^n \left[\sigma^2-\frac{\sigma^2}{n}+\frac{(x<em>i-\bar x)^2\sigma^2}{\ell</em>{xx}}\right]=(n-2)\sigma^2.$$</p>

<h3 id="normal-distributions">Normal distributions</h3>

<p><code>Assumption B</code>: $\epsilon_i\stackrel{iid}{\sim}N(0,\sigma^2),i=1,\dots,n$.</p>

<blockquote>
<p>Assumption B includes Assumptions A1 and A2.</p>
</blockquote>

<p><code>Theorem 4</code>: Under Assumption B, we have</p>

<p>(1). $\hat\beta_0\sim N(\beta<em>0,(\frac 1n+\frac{\bar x^2}{\ell</em>{xx}})\sigma^2)$</p>

<p>(2). $\hat\beta_1\sim N(\beta<em>1,\frac{\sigma^2}{\ell</em>{xx}})$</p>

<p>(3). $\frac{(n-2)\hat\sigma^2}{\sigma^2}=\frac{\sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)^2}{\sigma^2}\sim \chi^2(n-2)$</p>

<p>(4). $\hat\sigma^2$ is independent of $(\hat\beta_0,\hat\beta_1)$.</p>

<p><code>Proof</code>: Under Assumption B, $y_i=\beta_0+\beta_1x_i+\epsilon_i\sim N(\beta_0+\beta_1x_i,\sigma^2)$ independently. Both $\hat\beta_0,\hat\beta_1$ are linear combinations of $y_i$s. Consequently, they are normally distributed.  We have known their expected values and variances from Theorems 1 and 2. The claims (1) and (2) are thus verified. The proofs of claims (3) and (4) are deferred to the general case.</p>

<blockquote>
<p>It is $n-2$ degrees of freedom because we have fit two parameters to the $n$ data points.</p>
</blockquote>

<h3 id="confidence-intervals-and-hypothesis-tests">Confidence intervals and hypothesis tests</h3>

<p>For known $\sigma$ we can make tests and confidence
intervals using
$$\frac{\hat\beta_1-\beta<em>1}{\sigma/\sqrt{\ell</em>{xx}}}\sim N(0,1).$$</p>

<p>The $100(1-\alpha)\%$ confidence interval for $\beta_1$ is given by $\hat\beta<em>1\pm u</em>{1-\alpha/2}\sigma/\sqrt{\ell_{xx}}$. For testing
$$H_0:\beta_1=\beta_1^<em>\ vs.\ H_1:\beta_1\neq\beta_1^</em>,$$
we reject $H_0$ if $|\hat\beta_1-\beta<em>1^*|&gt;u</em>{1-\alpha/2}\sigma/\sqrt{\ell_{xx}}$ with the most popular hypothesized value being $\beta^*=0$ (i.e., the regession function is <strong>significant</strong> or not at significance level $\alpha$.)</p>

<p>In the more realistic setting of unknown $\sigma$, so long as $n &gt; 3$, using claims (2-4) gives
$$\frac{\hat\beta_1-\beta<em>1}{\hat{\sigma}/\sqrt{\ell</em>{xx}}}\sim t(n-2).$$
The $100(1-\alpha)\%$ confidence interval for $\beta_1$ is   $\hat\beta<em>1\pm t</em>{1-\alpha/2}(n-2)\hat{\sigma}/\sqrt{\ell_{xx}}$. For testing
$$H_0:\beta_1=\beta_1^<em>\ vs.\ H_1:\beta_1\neq\beta_1^</em>,$$
we reject $H_0$ if $|\hat\beta_1-\beta<em>1^*|&gt;t</em>{1-\alpha/2}(n-2)\hat\sigma/\sqrt{\ell_{xx}}$.</p>

<p>For drawing inferences about $\beta_0$, we can use $$\frac{\hat\beta_0-\beta<em>0}{\sigma\sqrt{1/n+\bar x^2\ell</em>{xx}}}\sim N(0,1),$$
$$\frac{\hat\beta_0-\beta<em>0}{\hat\sigma\sqrt{1/n+\bar x^2\ell</em>{xx}}}\sim t(n-2).$$</p>

<p>The $100(1-\alpha)\%$ confidence interval for $\sigma^2$ is
$$\left[\frac{(n-2)\hat\sigma^2}{\chi<em>{1-\alpha/2}^2(n-2)},\frac{(n-2)\hat\sigma^2}{\chi</em>{\alpha/2}^2(n-2)}\right]=\left[\frac{\mathrm{RSS}}{\chi<em>{1-\alpha/2}^2(n-2)},\frac{\mathrm{RSS}}{\chi</em>{\alpha/2}^2(n-2)}\right].$$</p>

<h3 id="case-study-1">Case study 1</h3>

<p>A manufacturer of air conditioning units is having assembly problems due to
the failure of a connecting rod to meet finished-weight specifications. Too many
rods are being completely tooled, then rejected as overweight. To reduce that
cost, the company’s quality-control department wants to quantify the relationship
between the weight of the <strong>finished rod</strong>, $y$, and that of the <strong>rough casting</strong>, $x$. Castings likely to produce rods that are too heavy can then be discarded
before undergoing the final (and costly) tooling process. The data are displayed below.</p>

<table>
<caption>rough weight vs. finished weight</caption>
 <thead>
  <tr>
   <th style="text-align:right;"> id </th>
   <th style="text-align:right;"> rough_weight </th>
   <th style="text-align:right;"> finished_weight </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 2.745 </td>
   <td style="text-align:right;"> 2.080 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 2 </td>
   <td style="text-align:right;"> 2.700 </td>
   <td style="text-align:right;"> 2.045 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 3 </td>
   <td style="text-align:right;"> 2.690 </td>
   <td style="text-align:right;"> 2.050 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 4 </td>
   <td style="text-align:right;"> 2.680 </td>
   <td style="text-align:right;"> 2.005 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 5 </td>
   <td style="text-align:right;"> 2.675 </td>
   <td style="text-align:right;"> 2.035 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 6 </td>
   <td style="text-align:right;"> 2.670 </td>
   <td style="text-align:right;"> 2.035 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 7 </td>
   <td style="text-align:right;"> 2.665 </td>
   <td style="text-align:right;"> 2.020 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 8 </td>
   <td style="text-align:right;"> 2.660 </td>
   <td style="text-align:right;"> 2.005 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 9 </td>
   <td style="text-align:right;"> 2.655 </td>
   <td style="text-align:right;"> 2.010 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 10 </td>
   <td style="text-align:right;"> 2.655 </td>
   <td style="text-align:right;"> 2.000 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 11 </td>
   <td style="text-align:right;"> 2.650 </td>
   <td style="text-align:right;"> 2.000 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 12 </td>
   <td style="text-align:right;"> 2.650 </td>
   <td style="text-align:right;"> 2.005 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 13 </td>
   <td style="text-align:right;"> 2.645 </td>
   <td style="text-align:right;"> 2.015 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 14 </td>
   <td style="text-align:right;"> 2.635 </td>
   <td style="text-align:right;"> 1.990 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 15 </td>
   <td style="text-align:right;"> 2.630 </td>
   <td style="text-align:right;"> 1.990 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 16 </td>
   <td style="text-align:right;"> 2.625 </td>
   <td style="text-align:right;"> 1.995 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 17 </td>
   <td style="text-align:right;"> 2.625 </td>
   <td style="text-align:right;"> 1.985 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 18 </td>
   <td style="text-align:right;"> 2.620 </td>
   <td style="text-align:right;"> 1.970 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 19 </td>
   <td style="text-align:right;"> 2.615 </td>
   <td style="text-align:right;"> 1.985 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 20 </td>
   <td style="text-align:right;"> 2.615 </td>
   <td style="text-align:right;"> 1.990 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 21 </td>
   <td style="text-align:right;"> 2.615 </td>
   <td style="text-align:right;"> 1.995 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 22 </td>
   <td style="text-align:right;"> 2.610 </td>
   <td style="text-align:right;"> 1.990 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 23 </td>
   <td style="text-align:right;"> 2.590 </td>
   <td style="text-align:right;"> 1.975 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 24 </td>
   <td style="text-align:right;"> 2.590 </td>
   <td style="text-align:right;"> 1.995 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 25 </td>
   <td style="text-align:right;"> 2.565 </td>
   <td style="text-align:right;"> 1.955 </td>
  </tr>
</tbody>
</table>

<p>Consider the linear model
$$y_i=\beta_0+\beta_1x_i+\epsilon_i,\ \epsilon_i\stackrel{iid}{\sim}N(0,\sigma^2).$$</p>

<p>The observed data gives $\bar x = 2.643$, $\bar y=2.0048$, $\ell<em>{xx}=0.0367$, $\ell</em>{xy}=0.023565$, $\hat\sigma = 0.0113$.
The least square estimates are
$$\hat\beta<em>1=\frac{\ell</em>{xy}}{\ell_{xx}}=\frac{0.023565}{0.0367}=0.642,\ \hat\beta_0=\bar y-\hat\beta_1\bar x=0.308.$$</p>

<p>The regession function $\hat y = 0.308+0.642 x$; see the blue line given below.</p>

<pre><code class="language-r">attach(rod)
par(mar=c(4,4,1,0.5))
plot(rough_weight,finished_weight,type=&quot;p&quot;,pch=16,
     xlab = &quot;Rough Weight&quot;,ylab = &quot;Finished Weight&quot;)
lm.rod = lm(finished_weight~rough_weight)
abline(coef(lm.rod),col=&quot;blue&quot;)
</code></pre>

<p><img src="chap04_files/figure-latex/unnamed-chunk-2-1.pdf" alt="" /><!-- --></p>

<pre><code class="language-r">summary(lm.rod) #output the results
</code></pre>

<pre><code>## 
## Call:
## lm(formula = finished_weight ~ rough_weight)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.023558 -0.008242  0.001074  0.008179  0.024231 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   0.30773    0.15608   1.972   0.0608 .  
## rough_weight  0.64210    0.05905  10.874 1.54e-10 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.01131 on 23 degrees of freedom
## Multiple R-squared:  0.8372, Adjusted R-squared:  0.8301 
## F-statistic: 118.3 on 1 and 23 DF,  p-value: 1.536e-10
</code></pre>

<h3 id="assessing-the-fit">Assessing the Fit</h3>

<p>As an aid in assessing the quality of the fit, we will make extensive use of the residuals,
which are the differences between the observed and fitted values:
$$\hat e_i = y_i-\hat\beta_0-\hat\beta_1x_i,\ i=1,\dots,n.$$</p>

<p>It is most useful to examine the residuals graphically. Plots of the residuals versus the
$x$ values may reveal systematic misfit or ways in which the data do not conform to
the fitted model. Ideally, the residuals should show no relation to the $x$ values, and the plot should look like a horizontal blur. The residuals for case study 1 are plotted below.</p>

<pre><code class="language-r">par(mar=c(4,4,2,1))
plot(lm.rod$fitted.values,lm.rod$residuals,&quot;p&quot;,
     xlab=&quot;Fitted values&quot;,ylab = &quot;Residuals&quot;)
</code></pre>

<p><img src="chap04_files/figure-latex/unnamed-chunk-3-1.pdf" alt="" /><!-- -->
Standardized Residuals are graphed below. The key command is <code>rstandard</code>.</p>

<pre><code class="language-r">par(mar=c(4,4,2,1))
plot(lm.rod$fitted.values,rstandard(lm.rod),&quot;p&quot;,
     xlab=&quot;Fitted values&quot;,ylab = &quot;Standardized Residuals&quot;)
abline(h=c(-2,2),lty=c(5,5))
</code></pre>

<p><img src="chap04_files/figure-latex/unnamed-chunk-4-1.pdf" alt="" /><!-- --></p>

<h3 id="drawing-inferences-about-e-y">Drawing Inferences about $E[y]$</h3>

<p>For given $x$, we want the estimate the expected value of $y$, i.e., $E[y]=\beta_0+\beta_1x.$ A natural unbiased estimate is $\hat y = \hat\beta_0+\hat\beta<em>1x$. From the proof of Theorem 3, we have the variance
$$Var[\hat y] = \left(\frac{1}{n}+\frac{(x-\bar x)^2}{\ell</em>{xx}}\right)\sigma^2.$$
Under Assumption B, by Theorem 4, we have
$$\hat y\sim N(\beta_0+\beta<em>1x,(1/n+(x-\bar x)^2/\ell</em>{xx})\sigma^2),$$
$$\frac{\hat y-E[\hat y]}{\hat{\sigma}\sqrt{1/n+(x-\bar x)/\ell_{xx}}}\sim t(n-2)$$
We thus have the following results.</p>

<p><code>Theorem 5</code>: Suppose Assumption B is satisfied. Then we have
$$\hat y = \hat\beta_0+\hat\beta_1x \sim N(\beta_0+\beta<em>1x,[1/n+(x-\bar x)^2/\ell</em>{xx}]\sigma^2).$$
A $100(1−\alpha)\%$ confidence interval for $E[y]=\beta_0+\beta<em>1x$ is
given by
$$\hat y\pm t</em>{1-\alpha/2}(n-2)\hat{\sigma}\sqrt{\frac{1}{n}+\frac{(x-\bar x)^2}{\ell_{xx}}}.$$</p>

<blockquote>
<p>Notice from the formula in Theorem 5 that the width of a confidence
interval for $E[y]$ increases as the value of $x$ becomes more extreme. That
is, we are better able to predict the location of the regression line for an $x$-value
close to $\bar x$ than we are for $x$-values that are either very small or very large.</p>
</blockquote>

<p>For case study 1, we plot the lower and upper limits for the $95\%$ confidence interval for $E[y]$.</p>

<pre><code class="language-r">x = seq(2.5,2.8,by=0.001)
newdata = data.frame(rough_weight= x)
pred_x = predict(lm.rod,newdata,interval = &quot;confidence&quot;)
par(mar=c(4,4,2,1))
matplot(x,pred_x,type=&quot;l&quot;,lty = c(1,5,5),
        col=c(&quot;blue&quot;,&quot;red&quot;,&quot;red&quot;),lwd=2,
        xlab=&quot;Rough Weight&quot;,ylab=&quot;Finished Weight&quot;)
abline(v=mean(rough_weight),lty=5)
points(rough_weight,finished_weight,pch=16)
legend(2.5,2.1,c(&quot;Fitted&quot;,&quot;Lower limit&quot;,&quot;Upper limit&quot;),
       lty = c(1,5,5),col=c(&quot;blue&quot;,&quot;red&quot;,&quot;red&quot;))
</code></pre>

<p><img src="chap04_files/figure-latex/unnamed-chunk-5-1.pdf" alt="" /><!-- --></p>

<h3 id="drawing-inferences-about-future-observations">Drawing Inferences about Future Observations</h3>

<p>We now give a <strong>prediction interval</strong> for the future observation $y$ rather than its expected value $E[y]$. Note that here $y$ is no longer a fixed parameter, which is assumed to be independent of $y_i$&rsquo;s. A prediction interval is a range of numbers that
contains $y$ with a specified probability. Consider $y-\hat y$. If Assumption A1 is satisfied, then
$$E[y-\hat y] = E[y]-E[\hat y]= 0.$$</p>

<p>If Assumption A2 is satisfied, then
$$Var[y-\hat y] = Var[y]+Var[\hat y]=\left(1+\frac{1}{n}+\frac{(x-\bar x)^2}{\ell_{xx}}\right)\sigma^2.$$</p>

<p>If Assumption B is satisfied, $y-\hat y$ is then normally distributed.</p>

<p><code>Theorem 6</code>: Suppose Assumption B is satisfied. Let $y=\beta_0+\beta_1x+\epsilon$, where $\epsilon\sim N(0,\sigma^2)$ is independent of $\epsilon<em>i$&rsquo;s. A $100(1−\alpha)\%$ prediction interval for $y$ is
given by
$$\hat y\pm t</em>{1-\alpha/2}(n-2)\hat{\sigma}\sqrt{1+\frac{1}{n}+\frac{(x-\bar x)^2}{\ell_{xx}}}.$$</p>

<p>For case study 1, we plot the lower and upper limits for the $95\%$ prediction interval for $y$.</p>

<pre><code class="language-r">x = seq(2.5,2.8,by=0.001)
newdata = data.frame(rough_weight= x)
pred_x = predict(lm.rod,newdata,interval = &quot;prediction&quot;)
par(mar=c(4,4,2,1))
matplot(x,pred_x,type=&quot;l&quot;,lty = c(1,5,5),
        col=c(&quot;blue&quot;,&quot;red&quot;,&quot;red&quot;),lwd=2,
        xlab=&quot;Rough Weight&quot;,ylab=&quot;Finished Weight&quot;)
abline(v=mean(rough_weight),lty=5)
points(rough_weight,finished_weight,pch=16)
legend(2.5,2.1,c(&quot;Fitted&quot;,&quot;Lower limit&quot;,&quot;Upper limit&quot;),
       lty = c(1,5,5),col=c(&quot;blue&quot;,&quot;red&quot;,&quot;red&quot;))
</code></pre>

<p><img src="chap04_files/figure-latex/unnamed-chunk-6-1.pdf" alt="" /><!-- --></p>

<h3 id="how-to-control-y">How to control y?</h3>

<p>Consider case study 1 again. Castings likely to produce rods that are too heavy can then be discarded before undergoing the final (and costly) tooling process. The company’s quality-control department wants to produce the rod $y$ with weights no large than 2.05. How to choose the rough casting?</p>

<p>Now we want $y\le y<em>0=2.05$ with probability $1-\alpha$. Similarly to Theorem 6, we can construct one-side confidence interval for $y$, that is
$$\bigg(-\infty,\hat y+t</em>{1-\alpha}(n-2)\hat{\sigma}\sqrt{1+\frac{1}{n}+\frac{(x-\bar x)^2}{\ell_{xx}}}\bigg].$$
This implies $$\hat\beta_0+\hat\beta<em>1x+t</em>{1-\alpha}(n-2)\hat{\sigma}\sqrt{1+\frac{1}{n}+\frac{(x-\bar x)^2}{\ell_{xx}}}\le y_0.$$</p>

<p><img src="chap04_files/figure-latex/unnamed-chunk-7-1.pdf" alt="" /><!-- --></p>

<h2 id="multiple-linear-regression">Multiple linear regression</h2>

<p>With problems more complex than fitting a straight line, it is very useful to approach the linear least squares analysis via linear algebra.
Consider a model of the form
$$y_i=\beta_0+\beta<em>1x</em>{i1}+\beta<em>2x</em>{i2}+\dots+\beta<em>{p-1}x</em>{i,p-1}+\epsilon_i,\ i=1,\dots,n.$$</p>

<p>Let $Y=(y_1,\dots,y_n)^\top$, $\beta=(\beta<em>0,\dots,\beta</em>{p-1})^\top$, $\epsilon=(\epsilon_1,\dots,\epsilon<em>n)^\top$, and let $X$ be the $n\times p$ matrix
$$
X=
\left[
\begin{matrix}
1 &amp; x</em>{11} &amp; x<em>{12} &amp; \cdots &amp; x</em>{1,p-1}<br />
1 &amp; x<em>{21} &amp; x</em>{22} &amp; \cdots &amp; x<em>{2,p-1}<br />
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; <br />
1 &amp; x</em>{n1} &amp; x<em>{n2} &amp; \cdots &amp; x</em>{n,p-1}<br />
\end{matrix}
\right].
$$</p>

<p>The model can be rewritten as $$Y=X\beta+\epsilon,$$</p>

<ul>
<li><p>the matrix $X$ is called the <strong>design matrix</strong>,</p></li>

<li><p>assume that $p&lt;n$.</p></li>
</ul>

<p>The
least squares problem can then be phrased as follows: Find $\beta$ to minimize</p>

<p>$$Q(\beta)=\sum_{i=1}^n(y_i-\beta_0-\beta<em>1x</em>{i1}-\dots-\beta<em>{p-1}x</em>{i,p-1})^2:=||Y-X\beta||^2,$$</p>

<p>where $||\cdot||$ is the Euclidean  norm.</p>

<p>Note that $$Q=(Y-X\beta)^\top(Y-X\beta) = Y^\top Y-2Y^\top X\beta+\beta^\top X^\top X\beta.$$
If we differentiate $Q$ with respect to each $\beta_i$ and set the derivatives equal to zero, we see that the minimizers $\hat\beta<em>0,\dots,\hat\beta</em>{p-1}$ satisfy</p>

<p>$$\frac{\partial Q}{\partial \beta_i}=-2(Y^\top X)<em>i+2(X^{\top}X)</em>{i\cdot}\hat\beta=0.$$</p>

<p>We thus arrive at $$X^\top X\hat\beta = X^\top Y.$$</p>

<p>If the design matrix $X^\top X$ is <strong>nonsingular</strong>, the formal solution is
$$\hat\beta = (X^\top X)^{-1}X^\top Y.$$</p>

<p>The following lemma gives a criterion for the existence and uniqueness of solutions
of the normal equations.</p>

<p><code>Lemma 1</code>: The design matrix $X^\top X$ is nonsingular if and only if $\mathrm{rank}(X)=p$.</p>

<p><code>Proof</code>: First suppose that $X^\top X$ is singular. There exists a nonzero vector $u$ such that
$X^\top Xu = 0$. Multiplying the left-hand side of this equation by $u^\top$, we have $0=u^\top X^\top Xu=(Xu)^\top (Xu)$
So $Xu=0$, the columns of $X$ are linearly dependent, and the rank of $X$ is less
than $p$.</p>

<p>Next, suppose that the rank of $X$ is less than $p$ so that there exists a nonzero
vector $u$ such that $Xu = 0$. Then $X^\top Xu = 0$, and hence $X^\top X$ is singular.</p>

<blockquote>
<p>In what follows, we assume that $\mathrm{rank}(X)=p&lt;n$.</p>
</blockquote>

<h3 id="expected-values-and-variances">Expected values and variances</h3>

<p><code>Assumption A</code>: Assume that $E[\epsilon]=0$ and $Var[\epsilon]=\sigma^2I_p$.</p>

<p><code>Theorem 7</code>: Suppose that Assumption A is satisfied and $\mathrm{rank}(X)=p&lt;n$, we have</p>

<p>(1). $E[\hat\beta]=\beta,$</p>

<p>(2). $Var[\hat\beta]=\sigma^2(X^\top X)^{-1}$,</p>

<p><code>Proof</code>:</p>

<p>$$
\begin{align}
E[\hat\beta]&amp;= E[(X^\top X)^{-1}X^{\top}Y] <br />
&amp;= (X^\top X)^{-1}X^{\top}E[Y]<br />
&amp;=(X^\top X)^{-1}X^{\top}(X\beta)=\beta.
\end{align}$$</p>

<p>$$
\begin{align}
Var[\hat\beta] &amp;= Var[(X^\top X)^{-1}X^{\top}Y]<br />
&amp;=(X^\top X)^{-1}X^{\top}Var(Y)X(X^\top X)^{-1}<br />
&amp;=\sigma^2(X^\top X)^{-1}.
\end{align}
$$</p>

<p>We used the fact that $Var(AY) = AVar(Y)A^\top$ for any fixed matrix $A$, and $X^\top X$ and therefore $(X^\top X)^{-1}$ are symmetric.</p>

<h3 id="estimation-of-sigma-2-1">Estimation of $\sigma^2$</h3>

<p>The residual sum of squares (RSS) is defined by
 $$\mathrm{RSS}=Q(\hat\beta)=||Y-X\hat\beta||^2=||\hat\epsilon||^2,$$</p>

<p>where the vector of residuals is
$$\hat\epsilon = Y-X(X^\top X)^{-1}X^\top Y:=Y-PY=Y(I_n-P),$$</p>

<ul>
<li>$P=X(X^\top X)^{-1}X^\top$ is an $n\times n$ matrix (called the <strong>projection matrix</strong>).</li>
</ul>

<p>Two useful properties of $P$ are given in the following lemma.</p>

<p><code>Lemma 2</code>: Let $P$ be defined as before. Then
$$P = P^\top=P^2$$</p>

<p>$$I_n-P = (I_n-P)^\top=(I_n-P)^2.$$</p>

<blockquote>
<p>We may think
geometrically of the fitted values, $\hat Y=X\hat\beta $, as being the projection of $Y$ onto the subspace
spanned by the columns of $X$.</p>
</blockquote>

<p>The sum of squared residuals is then
$$
\begin{align}
\mathrm{SSE} = ||(I_n-P)Y||^2 = Y^\top(I_n-P)^\top(I_n-P)Y=Y^\top(I_n-P)Y.
\end{align}
$$</p>

<p><code>Lemma 3</code>: The cyclic property of the trace, that is, $\mathrm{trace}(AB)=\mathrm{trace}(BA)$.</p>

<p>Using Lemma 3, we have</p>

<p>$$
\begin{align}
E[\mathrm{RSS}]&amp;= E[Y^\top(I_n-P)Y]=E[\mathrm{trace}(Y^\top(I_n-P)Y)] \&amp;= E[\mathrm{trace}((I_n-P)YY^\top)]=\mathrm{trace}((I_n-P)E[YY^\top])<br />
&amp;=\mathrm{trace}((I_n-P)(Var[Y]+E[Y]E[Y^\top]))<br />
&amp;=\mathrm{trace}((I_n-P)(\sigma^2 I_n))+\mathrm{trace}((I_n-P)X\beta\beta^\top X^\top)<br />
&amp;=\sigma^2(n-\mathrm{trace}(P))
\end{align}
$$
where we used $(I_n-P)X=X-X(X^\top X)^{-1}X^\top X=0$. Using the cyclic property of the trace again gives</p>

<p>$$
\begin{align}
\mathrm{trace}(P)&amp;= \mathrm{trace}(X(X^\top X)^{-1}X^\top)<br />
&amp;=\mathrm{trace}(X^\top X(X^\top X)^{-1})=\mathrm{trace}(I_p)=p.
\end{align}
$$</p>

<p>We therefore have $E[\mathrm{RSS}]=(n-p)\sigma^2$.</p>

<p><code>Theorem 8</code>: Suppose that Assumption A is satisfied and $\mathrm{rank}(X)=p&lt;n$,
$$\hat\sigma^2 = \frac{\mathrm{RSS}}{n-p}$$</p>

<p>is an unbiased estimate of $\sigma^2$.</p>

    </div>

    


    
    

    

    


  </div>
</article>

<div class="container">
  <footer class="site-footer">
  
  <p class="powered-by">
    <a href="/privacy/">Privacy Policy</a>
  </p>
  

  <p class="powered-by">
    Zhijian He &copy; 2018 &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

</div>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        CommonHTML: { linebreaks: { automatic: true } },
        tex2jax: { inlineMath: [ ['$', '$'], ['\\(','\\)'] ], displayMath: [ ['$$','$$'], ['\\[', '\\]'] ], processEscapes: false },
        TeX: { noUndefined: { attributes: { mathcolor: 'red', mathbackground: '#FFEEEE', mathsize: '90%' } } },
        messageStyle: 'none'
      });
    </script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha512-+NqPlbbtM1QqiK8ZAo4Yrj2c4lNQoGv8P79DPtKzj++l5jnN39rHA/xsqn8zE9l0uSoxaCdrOgFs6yjyfbBxSg==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha256-VsEqElsCHSGmnmHXGQzvoWjWwoznFSZc6hs7ARLRacQ=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    <script src="/js/hugo-academic.js"></script>
    

    
    

    
    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "Search Results",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    <script src="/js/search.js"></script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

  </body>
</html>

