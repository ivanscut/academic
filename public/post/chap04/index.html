<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 2.4.0">
  <meta name="generator" content="Hugo 0.48" />
  <meta name="author" content="Zhijian He">

  
  
  
  
    
  
  <meta name="description" content="第四章：线性回归(Linear regression)">

  
  <link rel="alternate" hreflang="en-us" href="/post/chap04/">

  


  

  
  
  
  <meta name="theme-color" content="#0095eb">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha256-eSi1q2PG6J7g7ib17yAaWMcrr5GrtohYChqibrV7PBE=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" crossorigin="anonymous">
        
      
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono">
  

  <link rel="stylesheet" href="/styles.css">
  

  
  
  

  
  <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Dr. Zhijian He">
  <link rel="feed" href="/index.xml" type="application/rss+xml" title="Dr. Zhijian He">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/post/chap04/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Dr. Zhijian He">
  <meta property="og:url" content="/post/chap04/">
  <meta property="og:title" content="第四章：线性回归 | Dr. Zhijian He">
  <meta property="og:description" content="第四章：线性回归(Linear regression)">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2018-11-15T00:00:00&#43;00:00">
  
  <meta property="article:modified_time" content="2018-11-15T00:00:00&#43;00:00">
  

  

  

  <title>第四章：线性回归 | Dr. Zhijian He</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Dr. Zhijian He</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/post">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#publications_selected">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/talk">
            
            <span>Talks</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  


  <div class="article-container">
    <h1 itemprop="name">第四章：线性回归</h1>

    

<div class="article-metadata">

  
  
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Zhijian He">
  </span>
  

  <span class="article-date">
    
    <meta content="2018-11-15 00:00:00 &#43;0000 UTC" itemprop="datePublished">
    <time datetime="2018-11-15 00:00:00 &#43;0000 UTC" itemprop="dateModified">
      Nov 15, 2018
    </time>
  </span>
  <span itemscope itemprop="publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Zhijian He">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    16 min read
  </span>
  

  
  

  
  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder"></i>
    
    <a href="/categories/%E8%AF%BE%E4%BB%B6/">课件</a>
    
  </span>
  
  

  
  

  

</div>


    <div class="article-style" itemprop="articleBody">
      <div id="simple-linear-models" class="section level2">
<h2>Simple linear models</h2>
<p>The linear model is given by
<span class="math display">\[y_i=\beta_0+\beta_1x_i+\epsilon_i,\ i=1,\dots,n.\]</span></p>
<ul>
<li><span class="math inline">\(\epsilon_i\)</span> are random (need some assumptions)</li>
<li><span class="math inline">\(x_i\)</span> are <strong>fixed</strong> (<em>independent/predictor</em> variable)</li>
<li><span class="math inline">\(y_i\)</span> are random (<em>dependent/response</em> variable)</li>
<li><span class="math inline">\(\beta_0\)</span> is the <em>intercept</em></li>
<li><span class="math inline">\(\beta_1\)</span> is the <em>slope</em></li>
</ul>
<div id="least-square-estimators" class="section level3">
<h3>Least square estimators</h3>
<p>Choose <span class="math inline">\(\beta_0,\beta_1\)</span> to minimize
<span class="math display">\[Q(\beta_0,\beta_1) = \sum_{i=1}^n(y_i-\beta_0-\beta_1x_i)^2.\]</span></p>
<p>The minimizers <span class="math inline">\(\hat\beta_0,\hat\beta_1\)</span> satisfy
<span class="math display">\[
\begin{cases}
\frac{\partial Q}{\partial \beta_0} = -2\sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)=0\\
\frac{\partial Q}{\partial \beta_1} = -2\sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)x_i=0
\end{cases}
\]</span></p>
<p>This gives
<span class="math display">\[\hat\beta_1 = \frac{\sum_{i=1}^n(y_i-\bar y)x_i}{\sum_{i=1}^n(x_i-\bar x)x_i},\ \hat\beta_0=\bar y-\hat\beta_1\bar x.\]</span></p>
<p>Define <span class="math display">\[\ell_{xx} = \sum_{i=1}^n(x_i-\bar x)^2,\]</span> <span class="math display">\[\ell_{yy} = \sum_{i=1}^n(y_i-\bar y)^2,\]</span> <span class="math display">\[\ell_{xy} = \sum_{i=1}^n(x_i-\bar x)(y_i-\bar y).\]</span>
We thus have
<span class="math display">\[\hat\beta_1 = \frac{\sum_{i=1}^n(y_i-\bar y)(x_i-\bar x)}{\sum_{i=1}^n(x_i-\bar x)(x_i-\bar x)}=\frac{\ell_{xy}}{\ell_{xx}}=\frac{1}{\ell_{xx}}\sum_{i=1}^n(x_i-\bar x)y_i.\]</span></p>
<p><code>Regression function</code>: <span class="math inline">\(\hat y=\hat\beta_0+\hat\beta_1x\)</span>.</p>
</div>
<div id="expected-values" class="section level3">
<h3>Expected values</h3>
<p><code>Assumption A1</code>: <span class="math inline">\(E[\epsilon_i]=0,i=1,\dots,n\)</span>.</p>
<p><code>Theorem 1</code>: Under Assumption A1, <span class="math inline">\(\hat\beta_0,\hat\beta_1\)</span> are unbiased estimators for <span class="math inline">\(\beta_0,\beta_1\)</span>, respectively.</p>
<p><code>Proof</code>:
<span class="math display">\[
\begin{align}
E[\hat \beta_1] &amp;= \frac{1}{\ell_{xx}}\sum_{i=1}^n(x_i-\bar x)E[y_i]\\
&amp;=\frac{1}{\ell_{xx}}\sum_{i=1}^n(x_i-\bar x)(\beta_0+\beta_1x_i)\\
&amp;=\frac{\beta_0}{\ell_{xx}}\sum_{i=1}^n(x_i-\bar x)+\frac{\beta_1}{\ell_{xx}}\sum_{i=1}^n(x_i-\bar x)x_i\\
&amp;=\frac{\beta_1}{\ell_{xx}}\sum_{i=1}^n(x_i-\bar x)(x_i-\bar x)\\
&amp;=\beta_1
\end{align}
\]</span></p>
<p><span class="math display">\[
\begin{align}
E[\hat \beta_0] &amp;= E[\bar y-\hat\beta_1\bar x]=E[\bar y]-\beta_1\bar x=\beta_0+\beta_1\bar x-\beta_1\bar x=\beta_0.
\end{align}
\]</span></p>
</div>
<div id="variances" class="section level3">
<h3>Variances</h3>
<p><code>Assumption A2</code>: <span class="math inline">\(Cov(\epsilon_i,\epsilon_j)=\sigma^21\{i=j\}\)</span>.</p>
<p><code>Theorem 2</code>: Under Assumption A2, we have
<span class="math display">\[Var[\hat\beta_0] = \left(\frac 1n+\frac{\bar x^2}{\ell_{xx}}\right)\sigma^2,\]</span></p>
<p><span class="math display">\[Var[\hat\beta_1] =\frac{\sigma^2}{\ell_{xx}},\]</span></p>
<p><span class="math display">\[Cov(\hat\beta_0,\hat\beta_1) = \frac{-\bar x}{\ell_{xx}}\sigma^2.\]</span></p>
<p><code>Proof</code>: Since <span class="math inline">\(Cov(\epsilon_i,\epsilon_j)=0\)</span> for any <span class="math inline">\(i\neq j\)</span>, <span class="math inline">\(Cov(y_i,y_j)=0\)</span>. We thus have
<span class="math display">\[
\begin{align}
Var[\hat\beta_1] &amp;= \frac{1}{\ell_{xx}^2}\sum_{i=1}^n(x_i-\bar x)^2Var[y_i]\\
&amp;= \frac{\sigma^2}{\ell_{xx}^2}\sum_{i=1}^n(x_i-\bar x)^2=\frac{\sigma^2}{\ell_{xx}}.
\end{align}
\]</span></p>
<p>We next show that <span class="math inline">\(Cov(\bar y,\hat \beta_1)=0\)</span>.
<span class="math display">\[
\begin{align}
Cov(\bar y,\hat \beta_1) &amp;= Cov\left(\frac{1}{n}\sum_{i=1}^n y_i,\frac{1}{\ell_{xx}}\sum_{i=1}^n(x_i-\bar x)y_i\right)\\
&amp;=\frac{1}{n\ell_{xx}}Cov\left(\sum_{i=1}^n y_i,\sum_{i=1}^n(x_i-\bar x)y_i\right)\\
&amp;=\frac{1}{n\ell_{xx}}\sum_{i=1}^nCov(y_i,(x_i-\bar x)y_i)\\
&amp;=\frac{1}{n\ell_{xx}}\sum_{i=1}^n(x_i-\bar x)\sigma^2 = 0.
\end{align}
\]</span></p>
<p><span class="math display">\[Var[\hat\beta_0] = Var[\bar y-\hat\beta_1\bar x]=Var[\bar y]+Var[\hat \beta_1\bar x]=\frac{\sigma^2}{n}+\frac{\bar x^2\sigma^2}{\ell_{xx}}.\]</span></p>
<p><span class="math display">\[Cov(\hat\beta_0,\hat\beta_1) = Cov(\bar y-\hat\beta_1\bar x,\hat\beta_1) = -\bar x Var[\hat\beta_1]=\frac{-\bar x}{\ell_{xx}}\sigma^2.\]</span></p>
<blockquote>
<p>So bigger <span class="math inline">\(n\)</span> is better. Get a bigger sample size if you can. Smaller <span class="math inline">\(\sigma\)</span> is better. The most interesting one is that bigger <span class="math inline">\(\ell_{xx}\)</span> is better. The more spread out the <span class="math inline">\(x_i\)</span> are the better we can
estimate the slope <span class="math inline">\(\beta_1\)</span>. When you’re picking the <span class="math inline">\(x_i\)</span>, if you can spread them out more, then it is more informative.</p>
</blockquote>
</div>
<div id="estimation-of-sigma2" class="section level3">
<h3>Estimation of <span class="math inline">\(\sigma^2\)</span></h3>
<p>For Assumption A2, it is common that the variance <span class="math inline">\(\sigma^2\)</span> is unknown.
The next theorem gives an unbiased estimate of <span class="math inline">\(\sigma^2\)</span>.</p>
<p><code>Definition</code>: The sum of squared errors (SSE) is defined by
<span class="math display">\[S_e^2 = \sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)^2.\]</span></p>
<p><code>Theorem 3</code>: Let
<span class="math display">\[\hat{\sigma}^2 := \frac{Q(\hat \beta_0,\hat\beta_1)}{n-2}=\frac{\sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)^2}{n-2}=\frac{S_e^2}{n-2}.\]</span>
Under Assumptions A1 and A2, we have <span class="math inline">\(E[\hat\sigma^2]=\sigma^2\)</span>.</p>
<p><code>Proof</code>: Let <span class="math inline">\(\hat y_i = \hat\beta_0+\hat\beta_1x_i=\bar y+\hat\beta_1(x_i-\bar x)\)</span>.</p>
<p><span class="math display">\[
\begin{align}
E[Q(\hat \beta_0,\hat\beta_1)] &amp;= \sum_{i=1}^nE[(y_i-\hat y_i)^2]=\sum_{i=1}^nVar[y_i-\hat y_i]+(E[y_i]-E[\hat y_i])^2\\
&amp;=\sum_{i=1}^n[Var[y_i]+Var[\hat y_i]-2Cov(y_i,\hat y_i)].
\end{align}
\]</span></p>
<p><span class="math display">\[
\begin{align}
Var[\hat y_i]&amp;= Var[\hat\beta_0+\hat\beta_1x_i]=Var[\bar y+\hat\beta_1(x_i-\bar x)]\\
&amp;=Var[\bar y]+(x_i-\bar x)^2Var[\hat\beta_1]\\
&amp;=\frac{\sigma^2}{n}+\frac{(x_i-\bar x)^2\sigma^2}{\ell_{xx}}.
\end{align}\]</span></p>
<p><span class="math display">\[
\begin{align}
Cov(y_i,\hat y_i)  &amp;= Cov(\beta_0+\beta_1x_i+\epsilon_i, \bar y+\hat\beta_1(x_i-\bar x))\\
&amp;=Cov(\epsilon_i,\bar y)+(x_i-\bar x)Cov(\epsilon_i,\hat\beta_1)\\
&amp;=\frac{\sigma^2}{n}+\frac{(x_i-\bar x)^2\sigma^2}{\ell_{xx}}.
\end{align}
\]</span></p>
<p>As a result, we have
<span class="math display">\[E[Q(\hat \beta_0,\hat\beta_1)] = \sum_{i=1}^n \left[\sigma^2-\frac{\sigma^2}{n}+\frac{(x_i-\bar x)^2\sigma^2}{\ell_{xx}}\right]=(n-2)\sigma^2.\]</span></p>
</div>
<div id="normal-distributions" class="section level3">
<h3>Normal distributions</h3>
<p><code>Assumption B</code>: <span class="math inline">\(\epsilon_i\stackrel{iid}{\sim}N(0,\sigma^2),i=1,\dots,n\)</span>.</p>
<blockquote>
<p>Assumption B includes Assumptions A1 and A2.</p>
</blockquote>
<p><code>Theorem 4</code>: Under Assumption B, we have</p>
<p>(1). <span class="math inline">\(\hat\beta_0\sim N(\beta_0,(\frac 1n+\frac{\bar x^2}{\ell_{xx}})\sigma^2)\)</span></p>
<p>(2). <span class="math inline">\(\hat\beta_1\sim N(\beta_1,\frac{\sigma^2}{\ell_{xx}})\)</span></p>
<p>(3). <span class="math inline">\(\frac{(n-2)\hat\sigma^2}{\sigma^2}=\frac{S_e^2}{\sigma^2}\sim \chi^2(n-2)\)</span></p>
<p>(4). <span class="math inline">\(\hat\sigma^2\)</span> is independent of <span class="math inline">\((\hat\beta_0,\hat\beta_1)\)</span>.</p>
<p><code>Proof</code>: Under Assumption B, <span class="math inline">\(y_i=\beta_0+\beta_1x_i+\epsilon_i\sim N(\beta_0+\beta_1x_i,\sigma^2)\)</span> independently. Both <span class="math inline">\(\hat\beta_0,\hat\beta_1\)</span> are linear combinations of <span class="math inline">\(y_i\)</span>s. Consequently, they are normally distributed. We have known their expected values and variances from Theorems 1 and 2. The claims (1) and (2) are thus verified. The proofs of claims (3) and (4) are deferred to the general case.</p>
<blockquote>
<p>It is <span class="math inline">\(n-2\)</span> degrees of freedom because we have fit two parameters to the <span class="math inline">\(n\)</span> data points.</p>
</blockquote>
</div>
<div id="confidence-intervals-and-hypothesis-tests" class="section level3">
<h3>Confidence intervals and hypothesis tests</h3>
<p>For known <span class="math inline">\(\sigma\)</span> we can make tests and confidence
intervals using
<span class="math display">\[\frac{\hat\beta_1-\beta_1}{\sigma/\sqrt{\ell_{xx}}}\sim N(0,1).\]</span></p>
<p>The <span class="math inline">\(100(1-\alpha)\%\)</span> confidence interval for <span class="math inline">\(\beta_1\)</span> is given by <span class="math inline">\(\hat\beta_1\pm u_{1-\alpha/2}\sigma/\sqrt{\ell_{xx}}\)</span>. For testing
<span class="math display">\[H_0:\beta_1=\beta_1^*\ vs.\ H_1:\beta_1\neq\beta_1^*,\]</span>
we reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(|\hat\beta_1-\beta_1^*|&gt;u_{1-\alpha/2}\sigma/\sqrt{\ell_{xx}}\)</span> with the most popular hypothesized value being <span class="math inline">\(\beta_1^*=0\)</span> (i.e., the regession function is <strong>significant</strong> or not at significance level <span class="math inline">\(\alpha\)</span>.)</p>
<p>In the more realistic setting of unknown <span class="math inline">\(\sigma\)</span>, so long as <span class="math inline">\(n \ge 3\)</span>, using claims (2-4) gives
<span class="math display">\[\frac{\hat\beta_1-\beta_1}{\hat{\sigma}/\sqrt{\ell_{xx}}}\sim t(n-2).\]</span>
The <span class="math inline">\(100(1-\alpha)\%\)</span> confidence interval for <span class="math inline">\(\beta_1\)</span> is <span class="math inline">\(\hat\beta_1\pm t_{1-\alpha/2}(n-2)\hat{\sigma}/\sqrt{\ell_{xx}}\)</span>. For testing
<span class="math display">\[H_0:\beta_1=\beta_1^*\ vs.\ H_1:\beta_1\neq\beta_1^*,\]</span>
we reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(|\hat\beta_1-\beta_1^*|&gt;t_{1-\alpha/2}(n-2)\hat\sigma/\sqrt{\ell_{xx}}\)</span>.</p>
<p>For drawing inferences about <span class="math inline">\(\beta_0\)</span>, we can use <span class="math display">\[\frac{\hat\beta_0-\beta_0}{\sigma\sqrt{1/n+\bar x^2/\ell_{xx}}}\sim N(0,1),\]</span>
<span class="math display">\[\frac{\hat\beta_0-\beta_0}{\hat\sigma\sqrt{1/n+\bar x^2/\ell_{xx}}}\sim t(n-2).\]</span></p>
<p>The <span class="math inline">\(100(1-\alpha)\%\)</span> confidence interval for <span class="math inline">\(\sigma^2\)</span> is
<span class="math display">\[\left[\frac{(n-2)\hat\sigma^2}{\chi_{1-\alpha/2}^2(n-2)},\frac{(n-2)\hat\sigma^2}{\chi_{\alpha/2}^2(n-2)}\right]=\left[\frac{S_e^2}{\chi_{1-\alpha/2}^2(n-2)},\frac{S_e^2}{\chi_{\alpha/2}^2(n-2)}\right].\]</span></p>
</div>
<div id="case-study-1" class="section level3">
<h3>Case study 1</h3>
<p>A manufacturer of air conditioning units is having assembly problems due to
the failure of a connecting rod (连接杆) to meet finished-weight specifications. Too many
rods are being completely tooled, then rejected as overweight. To reduce that
cost, the company’s quality-control department wants to quantify the relationship
between the weight of the <strong>finished rod</strong>, <span class="math inline">\(y\)</span>, and that of the <strong>rough casting</strong> (毛坯铸件), <span class="math inline">\(x\)</span>. Castings likely to produce rods that are too heavy can then be discarded
before undergoing the final (and costly) tooling process. The data are displayed below.</p>
<table>
<caption>
<span id="tab:unnamed-chunk-1">Table 1: </span>rough weight vs. finished weight
</caption>
<thead>
<tr>
<th style="text-align:right;">
id
</th>
<th style="text-align:right;">
rough_weight
</th>
<th style="text-align:right;">
finished_weight
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2.745
</td>
<td style="text-align:right;">
2.080
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2.700
</td>
<td style="text-align:right;">
2.045
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
2.690
</td>
<td style="text-align:right;">
2.050
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
2.680
</td>
<td style="text-align:right;">
2.005
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
2.675
</td>
<td style="text-align:right;">
2.035
</td>
</tr>
<tr>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
2.670
</td>
<td style="text-align:right;">
2.035
</td>
</tr>
<tr>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
2.665
</td>
<td style="text-align:right;">
2.020
</td>
</tr>
<tr>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
2.660
</td>
<td style="text-align:right;">
2.005
</td>
</tr>
<tr>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
2.655
</td>
<td style="text-align:right;">
2.010
</td>
</tr>
<tr>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
2.655
</td>
<td style="text-align:right;">
2.000
</td>
</tr>
<tr>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
2.650
</td>
<td style="text-align:right;">
2.000
</td>
</tr>
<tr>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
2.650
</td>
<td style="text-align:right;">
2.005
</td>
</tr>
<tr>
<td style="text-align:right;">
13
</td>
<td style="text-align:right;">
2.645
</td>
<td style="text-align:right;">
2.015
</td>
</tr>
<tr>
<td style="text-align:right;">
14
</td>
<td style="text-align:right;">
2.635
</td>
<td style="text-align:right;">
1.990
</td>
</tr>
<tr>
<td style="text-align:right;">
15
</td>
<td style="text-align:right;">
2.630
</td>
<td style="text-align:right;">
1.990
</td>
</tr>
<tr>
<td style="text-align:right;">
16
</td>
<td style="text-align:right;">
2.625
</td>
<td style="text-align:right;">
1.995
</td>
</tr>
<tr>
<td style="text-align:right;">
17
</td>
<td style="text-align:right;">
2.625
</td>
<td style="text-align:right;">
1.985
</td>
</tr>
<tr>
<td style="text-align:right;">
18
</td>
<td style="text-align:right;">
2.620
</td>
<td style="text-align:right;">
1.970
</td>
</tr>
<tr>
<td style="text-align:right;">
19
</td>
<td style="text-align:right;">
2.615
</td>
<td style="text-align:right;">
1.985
</td>
</tr>
<tr>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
2.615
</td>
<td style="text-align:right;">
1.990
</td>
</tr>
<tr>
<td style="text-align:right;">
21
</td>
<td style="text-align:right;">
2.615
</td>
<td style="text-align:right;">
1.995
</td>
</tr>
<tr>
<td style="text-align:right;">
22
</td>
<td style="text-align:right;">
2.610
</td>
<td style="text-align:right;">
1.990
</td>
</tr>
<tr>
<td style="text-align:right;">
23
</td>
<td style="text-align:right;">
2.590
</td>
<td style="text-align:right;">
1.975
</td>
</tr>
<tr>
<td style="text-align:right;">
24
</td>
<td style="text-align:right;">
2.590
</td>
<td style="text-align:right;">
1.995
</td>
</tr>
<tr>
<td style="text-align:right;">
25
</td>
<td style="text-align:right;">
2.565
</td>
<td style="text-align:right;">
1.955
</td>
</tr>
</tbody>
</table>
<p>Consider the linear model
<span class="math display">\[y_i=\beta_0+\beta_1x_i+\epsilon_i,\ \epsilon_i\stackrel{iid}{\sim}N(0,\sigma^2).\]</span></p>
<p>The observed data gives <span class="math inline">\(\bar x = 2.643\)</span>, <span class="math inline">\(\bar y=2.0048\)</span>, <span class="math inline">\(\ell_{xx}=0.0367\)</span>, <span class="math inline">\(\ell_{xy}=0.023565\)</span>, <span class="math inline">\(\hat\sigma = 0.0113\)</span>.
The least square estimates are
<span class="math display">\[\hat\beta_1=\frac{\ell_{xy}}{\ell_{xx}}=\frac{0.023565}{0.0367}=0.642,\ \hat\beta_0=\bar y-\hat\beta_1\bar x=0.308.\]</span></p>
<p>The regession function <span class="math inline">\(\hat y = 0.308+0.642 x\)</span>; see the blue line given below.</p>
<pre class="r"><code>attach(rod)
par(mar=c(4,4,1,0.5))
plot(rough_weight,finished_weight,type=&quot;p&quot;,pch=16,
     xlab = &quot;Rough Weight&quot;,ylab = &quot;Finished Weight&quot;)
lm.rod = lm(finished_weight~rough_weight)
abline(coef(lm.rod),col=&quot;blue&quot;)</code></pre>
<p><img src="/post/chap04_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<pre class="r"><code>summary(lm.rod) #output the results</code></pre>
<pre><code>## 
## Call:
## lm(formula = finished_weight ~ rough_weight)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.023558 -0.008242  0.001074  0.008179  0.024231 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   0.30773    0.15608   1.972   0.0608 .  
## rough_weight  0.64210    0.05905  10.874 1.54e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.01131 on 23 degrees of freedom
## Multiple R-squared:  0.8372, Adjusted R-squared:  0.8301 
## F-statistic: 118.3 on 1 and 23 DF,  p-value: 1.536e-10</code></pre>
</div>
<div id="assessing-the-fit" class="section level3">
<h3>Assessing the Fit</h3>
<p>As an aid in assessing the quality of the fit, we will make extensive use of the residuals,
which are the differences between the observed and fitted values:
<span class="math display">\[\hat \epsilon_i = y_i-\hat\beta_0-\hat\beta_1x_i,\ i=1,\dots,n.\]</span></p>
<p>It is most useful to examine the residuals graphically. Plots of the residuals versus the
<span class="math inline">\(x\)</span> values may reveal systematic misfit or ways in which the data do not conform to
the fitted model. Ideally, the residuals should show no relation to the <span class="math inline">\(x\)</span> values, and the plot should look like a horizontal blur. The residuals for case study 1 are plotted below.</p>
<pre class="r"><code>par(mar=c(4,4,2,1))
plot(lm.rod$fitted.values,lm.rod$residuals,&quot;p&quot;,
     xlab=&quot;Fitted values&quot;,ylab = &quot;Residuals&quot;)</code></pre>
<p><img src="/post/chap04_files/figure-html/unnamed-chunk-3-1.png" width="672" />
Standardized Residuals are graphed below. The key command is <code>rstandard</code>.</p>
<pre class="r"><code>par(mar=c(4,4,2,1))
plot(lm.rod$fitted.values,rstandard(lm.rod),&quot;p&quot;,
     xlab=&quot;Fitted values&quot;,ylab = &quot;Standardized Residuals&quot;)
abline(h=c(-2,2),lty=c(5,5))</code></pre>
<p><img src="/post/chap04_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
<div id="drawing-inferences-about-ey" class="section level3">
<h3>Drawing Inferences about <span class="math inline">\(E[y]\)</span></h3>
<p>For given <span class="math inline">\(x\)</span>, we want to estimate the expected value of <span class="math inline">\(y\)</span>, i.e., <span class="math inline">\(E[y]=\beta_0+\beta_1x.\)</span> A natural unbiased estimate is <span class="math inline">\(\hat y = \hat\beta_0+\hat\beta_1x\)</span>. From the proof of Theorem 3, we have the variance
<span class="math display">\[Var[\hat y] = \left(\frac{1}{n}+\frac{(x-\bar x)^2}{\ell_{xx}}\right)\sigma^2.\]</span>
Under Assumption B, by Theorem 4, we have
<span class="math display">\[\hat y\sim N(\beta_0+\beta_1x,(1/n+(x-\bar x)^2/\ell_{xx})\sigma^2),\]</span>
<span class="math display">\[\frac{\hat y-E[\hat y]}{\hat{\sigma}\sqrt{1/n+(x-\bar x)/\ell_{xx}}}\sim t(n-2)\]</span>
We thus have the following results.</p>
<p><code>Theorem 5</code>: Suppose Assumption B is satisfied. Then we have
<span class="math display">\[\hat y = \hat\beta_0+\hat\beta_1x \sim N(\beta_0+\beta_1x,[1/n+(x-\bar x)^2/\ell_{xx}]\sigma^2).\]</span>
A <span class="math inline">\(100(1−\alpha)\%\)</span> confidence interval for <span class="math inline">\(E[y]=\beta_0+\beta_1x\)</span> is
given by
<span class="math display">\[\hat y\pm t_{1-\alpha/2}(n-2)\hat{\sigma}\sqrt{\frac{1}{n}+\frac{(x-\bar x)^2}{\ell_{xx}}}.\]</span></p>
<blockquote>
<p>Notice from the formula in Theorem 5 that the width of a confidence
interval for <span class="math inline">\(E[y]\)</span> increases as the value of <span class="math inline">\(x\)</span> becomes more extreme. That
is, we are better able to predict the location of the regression line for an <span class="math inline">\(x\)</span>-value
close to <span class="math inline">\(\bar x\)</span> than we are for <span class="math inline">\(x\)</span>-values that are either very small or very large.</p>
</blockquote>
<p>For case study 1, we plot the lower and upper limits for the <span class="math inline">\(95\%\)</span> confidence interval for <span class="math inline">\(E[y]\)</span>.</p>
<pre class="r"><code>x = seq(2.5,2.8,by=0.001)
newdata = data.frame(rough_weight= x)
pred_x = predict(lm.rod,newdata,interval = &quot;confidence&quot;)
par(mar=c(4,4,2,1))
matplot(x,pred_x,type=&quot;l&quot;,lty = c(1,5,5),
        col=c(&quot;blue&quot;,&quot;red&quot;,&quot;red&quot;),lwd=2,
        xlab=&quot;Rough Weight&quot;,ylab=&quot;Finished Weight&quot;)
abline(v=mean(rough_weight),lty=5)
points(rough_weight,finished_weight,pch=16)
legend(2.5,2.1,c(&quot;Fitted&quot;,&quot;Lower limit&quot;,&quot;Upper limit&quot;),
       lty = c(1,5,5),col=c(&quot;blue&quot;,&quot;red&quot;,&quot;red&quot;))</code></pre>
<p><img src="/post/chap04_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
</div>
<div id="drawing-inferences-about-future-observations" class="section level3">
<h3>Drawing Inferences about Future Observations</h3>
<p>We now give a <strong>prediction interval</strong> for the future observation <span class="math inline">\(y\)</span> rather than its expected value <span class="math inline">\(E[y]\)</span>. Note that here <span class="math inline">\(y\)</span> is no longer a fixed parameter, which is assumed to be independent of <span class="math inline">\(y_i\)</span>’s. A prediction interval is a range of numbers that
contains <span class="math inline">\(y\)</span> with a specified probability. Consider <span class="math inline">\(y-\hat y\)</span>. If Assumption A1 is satisfied, then
<span class="math display">\[E[y-\hat y] = E[y]-E[\hat y]= 0.\]</span></p>
<p>If Assumption A2 is satisfied, then
<span class="math display">\[Var[y-\hat y] = Var[y]+Var[\hat y]=\left(1+\frac{1}{n}+\frac{(x-\bar x)^2}{\ell_{xx}}\right)\sigma^2.\]</span></p>
<p>If Assumption B is satisfied, <span class="math inline">\(y-\hat y\)</span> is then normally distributed.</p>
<p><code>Theorem 6</code>: Suppose Assumption B is satisfied. Let <span class="math inline">\(y=\beta_0+\beta_1x+\epsilon\)</span>, where <span class="math inline">\(\epsilon\sim N(0,\sigma^2)\)</span> is independent of <span class="math inline">\(\epsilon_i\)</span>’s. A <span class="math inline">\(100(1−\alpha)\%\)</span> prediction interval for <span class="math inline">\(y\)</span> is
given by
<span class="math display">\[\hat y\pm t_{1-\alpha/2}(n-2)\hat{\sigma}\sqrt{1+\frac{1}{n}+\frac{(x-\bar x)^2}{\ell_{xx}}}.\]</span></p>
<p>For case study 1, we plot the lower and upper limits for the <span class="math inline">\(95\%\)</span> prediction interval for <span class="math inline">\(y\)</span>.</p>
<pre class="r"><code>x = seq(2.5,2.8,by=0.001)
newdata = data.frame(rough_weight= x)
pred_x = predict(lm.rod,newdata,interval = &quot;prediction&quot;)
par(mar=c(4,4,2,1))
matplot(x,pred_x,type=&quot;l&quot;,lty = c(1,5,5),
        col=c(&quot;blue&quot;,&quot;red&quot;,&quot;red&quot;),lwd=2,
        xlab=&quot;Rough Weight&quot;,ylab=&quot;Finished Weight&quot;)
abline(v=mean(rough_weight),lty=5)
points(rough_weight,finished_weight,pch=16)
legend(2.5,2.1,c(&quot;Fitted&quot;,&quot;Lower limit&quot;,&quot;Upper limit&quot;),
       lty = c(1,5,5),col=c(&quot;blue&quot;,&quot;red&quot;,&quot;red&quot;))</code></pre>
<p><img src="/post/chap04_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
<div id="how-to-control-y" class="section level3">
<h3>How to control y?</h3>
<p>Consider case study 1 again. Castings likely to produce rods that are too heavy can then be discarded before undergoing the final (and costly) tooling process. The company’s quality-control department wants to produce the rod <span class="math inline">\(y\)</span> with weights no large than 2.05 with probablity no less than 0.95. How to choose the rough casting?</p>
<p>Now we want <span class="math inline">\(y\le y_0=2.05\)</span> with probability <span class="math inline">\(1-\alpha\)</span>. Similarly to Theorem 6, we can construct one-side confidence interval for <span class="math inline">\(y\)</span>, that is
<span class="math display">\[\bigg(-\infty,\hat y+t_{1-\alpha}(n-2)\hat{\sigma}\sqrt{1+\frac{1}{n}+\frac{(x-\bar x)^2}{\ell_{xx}}}\bigg].\]</span>
This implies <span class="math display">\[\hat\beta_0+\hat\beta_1x+t_{1-\alpha}(n-2)\hat{\sigma}\sqrt{1+\frac{1}{n}+\frac{(x-\bar x)^2}{\ell_{xx}}}\le y_0.\]</span></p>
<p><img src="/post/chap04_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
</div>
</div>
<div id="multiple-linear-regression" class="section level2">
<h2>Multiple linear regression</h2>
<p>With problems more complex than fitting a straight line, it is very useful to approach the linear least squares analysis via linear algebra.
Consider a model of the form
<span class="math display">\[y_i=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\dots+\beta_{p-1}x_{i,p-1}+\epsilon_i,\ i=1,\dots,n.\]</span></p>
<p>Let <span class="math inline">\(Y=(y_1,\dots,y_n)^\top\)</span>, <span class="math inline">\(\beta=(\beta_0,\dots,\beta_{p-1})^\top\)</span>, <span class="math inline">\(\epsilon=(\epsilon_1,\dots,\epsilon_n)^\top\)</span>, and let <span class="math inline">\(X\)</span> be the <span class="math inline">\(n\times p\)</span> matrix
<span class="math display">\[
X=
\left[
\begin{matrix}
1 &amp; x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1,p-1}\\
1 &amp; x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2,p-1}\\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \\
1 &amp; x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{n,p-1}\\
\end{matrix}
\right].
\]</span></p>
<p>The model can be rewritten as <span class="math display">\[Y=X\beta+\epsilon,\]</span></p>
<ul>
<li><p>the matrix <span class="math inline">\(X\)</span> is called the <strong>design matrix</strong>,</p></li>
<li><p>assume that <span class="math inline">\(p&lt;n\)</span>.</p></li>
</ul>
<p>The
least squares problem can then be phrased as follows: Find <span class="math inline">\(\beta\)</span> to minimize</p>
<p><span class="math display">\[Q(\beta)=\sum_{i=1}^n(y_i-\beta_0-\beta_1x_{i1}-\dots-\beta_{p-1}x_{i,p-1})^2:=||Y-X\beta||^2,\]</span></p>
<p>where <span class="math inline">\(||\cdot||\)</span> is the Euclidean norm.</p>
<p>Note that <span class="math display">\[Q=(Y-X\beta)^\top(Y-X\beta) = Y^\top Y-2Y^\top X\beta+\beta^\top X^\top X\beta.\]</span>
If we differentiate <span class="math inline">\(Q\)</span> with respect to each <span class="math inline">\(\beta_i\)</span> and set the derivatives equal to zero, we see that the minimizers <span class="math inline">\(\hat\beta_0,\dots,\hat\beta_{p-1}\)</span> satisfy</p>
<p><span class="math display">\[\frac{\partial Q}{\partial \beta_i}=-2(Y^\top X)_i+2(X^{\top}X)_{i\cdot}\hat\beta=0.\]</span></p>
<p>We thus arrive at the so-called <strong>normal equations</strong>: <span class="math display">\[X^\top X\hat\beta = X^\top Y.\]</span></p>
<p>If the design matrix <span class="math inline">\(X^\top X\)</span> is <strong>nonsingular</strong>, the formal solution is
<span class="math display">\[\hat\beta = (X^\top X)^{-1}X^\top Y.\]</span></p>
<p>The following lemma gives a criterion for the existence and uniqueness of solutions
of the normal equations.</p>
<p><code>Lemma 1</code>: The design matrix <span class="math inline">\(X^\top X\)</span> is nonsingular if and only if <span class="math inline">\(\mathrm{rank}(X)=p\)</span>.</p>
<p><code>Proof</code>: First suppose that <span class="math inline">\(X^\top X\)</span> is singular. There exists a nonzero vector <span class="math inline">\(u\)</span> such that
<span class="math inline">\(X^\top Xu = 0\)</span>. Multiplying the left-hand side of this equation by <span class="math inline">\(u^\top\)</span>, we have <span class="math inline">\(0=u^\top X^\top Xu=(Xu)^\top (Xu)\)</span>
So <span class="math inline">\(Xu=0\)</span>, the columns of <span class="math inline">\(X\)</span> are linearly dependent, and the rank of <span class="math inline">\(X\)</span> is less
than <span class="math inline">\(p\)</span>.</p>
<p>Next, suppose that the rank of <span class="math inline">\(X\)</span> is less than <span class="math inline">\(p\)</span> so that there exists a nonzero
vector <span class="math inline">\(u\)</span> such that <span class="math inline">\(Xu = 0\)</span>. Then <span class="math inline">\(X^\top Xu = 0\)</span>, and hence <span class="math inline">\(X^\top X\)</span> is singular.</p>
<blockquote>
<p>In what follows, we assume that <span class="math inline">\(\mathrm{rank}(X)=p&lt;n\)</span>.</p>
</blockquote>
<div id="expected-values-and-variances" class="section level3">
<h3>Expected values and variances</h3>
<p><code>Assumption A</code>: Assume that <span class="math inline">\(E[\epsilon]=0\)</span> and <span class="math inline">\(Var[\epsilon]=\sigma^2I_p\)</span>.</p>
<p><code>Theorem 7</code>: Suppose that Assumption A is satisfied and <span class="math inline">\(\mathrm{rank}(X)=p&lt;n\)</span>, we have</p>
<p>(1). <span class="math inline">\(E[\hat\beta]=\beta,\)</span></p>
<p>(2). <span class="math inline">\(Var[\hat\beta]=\sigma^2(X^\top X)^{-1}\)</span>,</p>
<p><code>Proof</code>:</p>
<p><span class="math display">\[
\begin{align}
E[\hat\beta]&amp;= E[(X^\top X)^{-1}X^{\top}Y] \\
&amp;= (X^\top X)^{-1}X^{\top}E[Y]\\
&amp;=(X^\top X)^{-1}X^{\top}(X\beta)=\beta.
\end{align}\]</span></p>
<p><span class="math display">\[
\begin{align}
Var[\hat\beta] &amp;= Var[(X^\top X)^{-1}X^{\top}Y]\\
&amp;=(X^\top X)^{-1}X^{\top}Var(Y)X(X^\top X)^{-1}\\
&amp;=\sigma^2(X^\top X)^{-1}.
\end{align}
\]</span></p>
<p>We used the fact that <span class="math inline">\(Var(AY) = AVar(Y)A^\top\)</span> for any fixed matrix <span class="math inline">\(A\)</span>, and <span class="math inline">\(X^\top X\)</span> and therefore <span class="math inline">\((X^\top X)^{-1}\)</span> are symmetric.</p>
</div>
<div id="estimation-of-sigma2-1" class="section level3">
<h3>Estimation of <span class="math inline">\(\sigma^2\)</span></h3>
<p><code>Definition</code>:</p>
<ul>
<li><p><strong>The fitted values</strong>: <span class="math inline">\(\hat Y = X\hat\beta\)</span></p></li>
<li><p><strong>The vector of residuals</strong>: <span class="math inline">\(\hat\epsilon = Y-\hat Y\)</span></p></li>
<li><p><strong>The sum of squared errors (SSE)</strong>: <span class="math inline">\(S_e^2=Q(\hat\beta)=||Y-\hat Y||^2=||\hat\epsilon||^2\)</span></p></li>
</ul>
<p>Note that</p>
<p><span class="math display">\[\hat Y = X\hat\beta=X(X^\top X)^{-1}X^\top Y=:PY\]</span></p>
<ul>
<li><strong>The projection matrix</strong>: <span class="math inline">\(P = X(X^\top X)^{-1}X^\top\)</span></li>
</ul>
<p>The vector of residuals is then <span class="math inline">\(\hat\epsilon=(I_n-P)Y\)</span>. Two useful properties of <span class="math inline">\(P\)</span> are given in the following lemma.</p>
<p><code>Lemma 2</code>: Let <span class="math inline">\(P\)</span> be defined as before. Then
<span class="math display">\[P = P^\top=P^2\]</span></p>
<p><span class="math display">\[I_n-P = (I_n-P)^\top=(I_n-P)^2.\]</span></p>
<p><code>Note</code>: We may think
geometrically of the fitted values, <span class="math inline">\(\hat Y=X\hat\beta=PY\)</span>, as being the projection of <span class="math inline">\(Y\)</span> onto the subspace
spanned by the columns of <span class="math inline">\(X\)</span>.</p>
<p>The sum of squared residuals is then
<span class="math display">\[
\begin{align}
S_e^2 := ||\hat \epsilon||^2 = Y^\top(I_n-P)^\top(I_n-P)Y=Y^\top(I_n-P)Y.
\end{align}
\]</span></p>
<p><code>Definition</code>: Let <span class="math inline">\(A\)</span> be an <span class="math inline">\(n\times n\)</span> matrix. The trace of the matrix <span class="math inline">\(A\)</span> is defined as <span class="math inline">\(tr(A) = \sum_{i=1}^n a_{ii}\)</span>, where <span class="math inline">\(a_{ii}\)</span> are the elements on the main diagonal.</p>
<p><code>Lemma 3</code>: If <span class="math inline">\(A\)</span> is an <span class="math inline">\(m \times n\)</span> matrix and <span class="math inline">\(B\)</span> is an <span class="math inline">\(n \times m\)</span> matrix, then <span class="math display">\[tr(AB)=tr(BA).\]</span> This is the cyclic property of the trace.</p>
<p>Using Lemma 3, we have</p>
<p><span class="math display">\[
\begin{align}
E[S_e^2]&amp;= E[Y^\top(I_n-P)Y]=E[tr(Y^\top(I_n-P)Y)] \\&amp;= E[tr((I_n-P)YY^\top)]=tr((I_n-P)E[YY^\top])\\
&amp;=tr((I_n-P)(Var[Y]+E[Y]E[Y^\top]))\\
&amp;=tr((I_n-P)(\sigma^2 I_n))+tr((I_n-P)X\beta\beta^\top X^\top)\\
&amp;=\sigma^2(n-tr(P))
\end{align}
\]</span>
where we used <span class="math inline">\((I_n-P)X=X-X(X^\top X)^{-1}X^\top X=0\)</span>. Using the cyclic property of the trace again gives</p>
<p><span class="math display">\[
\begin{align}
tr(P)&amp;= tr(X(X^\top X)^{-1}X^\top)\\
&amp;=tr(X^\top X(X^\top X)^{-1})=tr(I_p)=p.
\end{align}
\]</span></p>
<p>We therefore have <span class="math inline">\(E[S_e^2]=(n-p)\sigma^2\)</span>.</p>
<p><code>Theorem 8</code>: Suppose that Assumption A is satisfied and <span class="math inline">\(\mathrm{rank}(X)=p&lt;n\)</span>,
<span class="math display">\[\hat\sigma^2 = \frac{S_e^2}{n-p}\]</span></p>
<p>is an unbiased estimate of <span class="math inline">\(\sigma^2\)</span>.</p>
</div>
<div id="normal-distribution" class="section level3">
<h3>Normal distribution</h3>
<p><code>Assumption B</code>: Assume that <span class="math inline">\(\epsilon\sim N(0,\sigma^2I_n)\)</span>.</p>
<p><code>Theorem 9</code>: Suppose that Assumption B is satisfied and <span class="math inline">\(\mathrm{rank}(X)=p&lt;n\)</span>, we have</p>
<p>(1). <span class="math inline">\(\hat\beta \sim N(\beta, \sigma^2(X^\top X)^{-1})\)</span>,</p>
<p>(2). <span class="math inline">\(\frac{(n-p)\hat\sigma^2}{\sigma^2}=\frac{S_e^2}{\sigma^2}\sim \chi^2(n-p)\)</span>,</p>
<p>(3). <span class="math inline">\(\hat\epsilon\)</span> is independent of <span class="math inline">\(\hat Y\)</span>,</p>
<p>(4). <span class="math inline">\(S_e^2\)</span> (or equivalently <span class="math inline">\(\hat\sigma^2\)</span>) is independent of <span class="math inline">\(\hat\beta\)</span>.</p>
<p><code>Proof</code>: If Assumption B is satisfied, then <span class="math inline">\(Y = X\beta+\epsilon\sim N(X\beta,\sigma^2_n)\)</span>. Recall that <span class="math inline">\(\hat\beta = (X^\top X)^{-1}X^\top Y\)</span> is normally distributed. The mean and covariance are given in Theorem 7 since Assumption A is satisfied.</p>
<p>Let <span class="math inline">\(\xi_1,\dots,\xi_p\)</span> be the orthogonal basis of the subspace <span class="math inline">\(\mathrm{span}(X)\)</span> of <span class="math inline">\(\mathbb{R}^n\)</span> generated by the <span class="math inline">\(p\)</span> columns of the matrix <span class="math inline">\(X\)</span>, and <span class="math inline">\(\xi_{p+1},\dots,\xi_n\)</span> be the orthogonal basis of the orthogonal complement <span class="math inline">\(\mathrm{span}(X)^\perp\)</span>. Since <span class="math inline">\(\hat Y = X\hat\beta\in \mathrm{span}(X)\)</span>, there exists <span class="math inline">\(z_1,\dots,z_p\)</span> such that
<span class="math display">\[\hat Y = \sum_{i=1}^p z_i\xi_i.\]</span></p>
<p>On the other hand, <span class="math inline">\(\hat Y^\top \hat \epsilon = (PY)^\top (I_n-P)Y = Y^\top P^\top (I_n-P)Y\)</span>. By Lemma 2, we have
<span class="math display">\[P^\top (I_n-P)=P-P^2 = 0.\]</span>
As a result, <span class="math inline">\(\hat Y^\top \hat \epsilon = 0\)</span>, implying <span class="math inline">\(\hat \epsilon\in \mathrm{span}(X)^\perp\)</span>. So there exsits <span class="math inline">\(z_{p+1},\dots,z_n\)</span> such that
<span class="math display">\[\hat \epsilon = \sum_{i=p+1}^n z_i\xi_i.\]</span>
Let <span class="math inline">\(U = (\xi_1,\dots,\xi_n)\)</span>, then <span class="math inline">\(U\)</span> is an orthogonal matrix, and let <span class="math inline">\(z=(z_1,\dots,z_n)^\top\)</span>. We thus have
<span class="math display">\[Y = \hat Y+\hat\epsilon =\sum_{i=1}^nz_i\xi_i=Uz.\]</span></p>
<p>Therefore, <span class="math display">\[z=U^{-1}Y=U^\top Y\sim N(U^\top X\beta,U^\top(\sigma^2 I_n)U)=N(U^\top X\beta,\sigma^2 I_n).\]</span></p>
<p>This implies that <span class="math inline">\(z_i\)</span> are independently normally distributed. So <span class="math inline">\(\hat Y\)</span> and <span class="math inline">\(\hat\epsilon\)</span> are independent. We next prove that <span class="math inline">\(E[z_i]=0\)</span> for all <span class="math inline">\(i&gt;p\)</span>. Let <span class="math inline">\(A=(\xi_{p+1},\dots,\xi_{n})\)</span>, then</p>
<p><span class="math display">\[A(z_{p+1},\dots,z_{n})^\top = \hat\epsilon=(I_n-P)Y.\]</span></p>
<p>This gives
<span class="math display">\[E[(z_{p+1},\dots,z_{n})^\top] = E[A^\top (I_n-P)Y]=A^\top (I_n-P)E[Y]=A^\top (I_n-P)X\beta=0.\]</span></p>
<p>Consequently, <span class="math inline">\(z_i\stackrel{iid}{\sim} N(0,\sigma^2),i=p+1,\dots,n\)</span>, implying <span class="math display">\[S_e^2/\sigma^2 = \hat\epsilon^\top\hat\epsilon/\sigma^2 = \sum_{i=p+1}^n (z_i/\sigma)^2\sim \chi^2(n-p).\]</span></p>
<p>Note that <span class="math inline">\(S_e^2\)</span> is a function of <span class="math inline">\(\hat\epsilon_i\)</span> and <span class="math inline">\(\hat \beta = (X^\top X)^{-1}X^\top Y =(X^\top X)^{-1} X^\top \hat Y\)</span> are linear conbinations of <span class="math inline">\(\hat y_i\)</span>. So <span class="math inline">\(S_e^2\)</span> and <span class="math inline">\(\hat\beta\)</span> are independent.</p>
</div>
<div id="confidence-intervals-and-hypothesis-tests-1" class="section level3">
<h3>Confidence intervals and hypothesis tests</h3>
<p>Let <span class="math inline">\(C=(X^\top X)^{-1}\)</span> with entries <span class="math inline">\(c_{ij}\)</span>. Suppose that Assumption B is satisfied and <span class="math inline">\(\mathrm{rank}(X)=p&lt;n\)</span>. If <span class="math inline">\(\sigma^2\)</span> is known, for each <span class="math inline">\(\beta_i\)</span>, the <span class="math inline">\(100(1-\alpha)\%\)</span> CI is</p>
<p><span class="math display">\[\hat\beta_i \pm u_{1-\alpha/2}\sigma\sqrt{c_{ii}}.\]</span></p>
<p>If <span class="math inline">\(\sigma^2\)</span> is unknown, for each <span class="math inline">\(\beta_i\)</span>, the <span class="math inline">\(100(1-\alpha)\%\)</span> CI is</p>
<p><span class="math display">\[\hat\beta_i \pm t_{1-\alpha/2}(n-p)\hat\sigma\sqrt{c_{ii}}.\]</span>
For testing hypothesis <span class="math inline">\(H_0:\beta_i= 0\ vs.\ H_1:\beta_i\neq 0\)</span>, the rejection region is
<span class="math display">\[W = \{|\beta_i|&gt;t_{1-\alpha/2}(n-p)\hat\sigma\sqrt{c_{ii}}\}.\]</span></p>
<p>The <span class="math inline">\(100(1-\alpha)\%\)</span> CI for <span class="math inline">\(\sigma^2\)</span> is <span class="math display">\[\left[\frac{(n-p)\hat\sigma^2}{\chi_{1-\alpha/2}^2(n-p)},\frac{(n-p)\hat\sigma^2}{\chi_{\alpha/2}^2(n-2)}\right]=\left[\frac{S_e^2}{\chi_{1-\alpha/2}^2(n-p)},\frac{S_e^2}{\chi_{\alpha/2}^2(n-p)}\right].\]</span></p>
</div>
<div id="significance-tests" class="section level3">
<h3>Significance tests</h3>
<p>Consider the hypothesis test:</p>
<p><span class="math display">\[H_0:\beta_1=\dots=\beta_{p-1}=0\ vs.\ H_1: \beta_{i^*}\neq 0\text{ for some }i^*\ge 1.\]</span></p>
<p><code>Definition</code>:</p>
<ul>
<li><strong>The total sum of squares (SST)</strong>:</li>
</ul>
<p><span class="math display">\[S_T^2 = \sum_{i=1}^n(y_i-\bar Y)^2\]</span></p>
<ul>
<li><strong>The sum of squares due to regression (SSR)</strong>:</li>
</ul>
<p><span class="math display">\[S_R^2  = \sum_{i=1}^n(\hat y_i-\bar Y)^2\]</span></p>
<ul>
<li><strong>The sum of squared errors (SSE)</strong>:</li>
</ul>
<p><span class="math display">\[S_e^2 = \sum_{i=1}^n(y_i-\hat y_i)^2\]</span></p>
<p>The relationship is
<span class="math display">\[S_T^2=S_R^2+S_e^2.\]</span></p>
<p>It is easy to see that</p>
<p><span class="math display">\[
\begin{align}
S_T^2&amp;=\sum_{i=1}^n (y_i-\bar Y)^2 = \sum_{i=1}^n (y_i-\hat y_i+\hat y_i-\bar Y)^2\\
&amp;=\sum_{i=1}^n [(y_i-\hat y_i)^2+(\hat y_i-\bar Y)^2+2(y_i-\hat y_i)(\hat y_i-\bar Y)]\\
&amp;=S_R^2+S_e^2 +2\sum_{i=1}^n (y_i-\hat y_i)(\hat y_i-\bar Y)
\end{align}
\]</span></p>
<p>Using Lemma 2, we have
<span class="math display">\[
\begin{align}
\sum_{i=1}^n (y_i-\hat y_i)(\hat y_i-\bar Y) &amp;= \hat\epsilon^\top\hat Y-\bar Y\sum_{i=1}^n \hat\epsilon_i\\
&amp;= [(I_n-P)Y]^\top PY-\bar Y(1,1,\dots,1) (I_n-P)Y\\
&amp;=Y^\top (I_n-P)PY - \bar Y[(1,1,\dots,1)-(1,1,\dots,1)P]Y\\
&amp;= 0.
\end{align}
\]</span></p>
<p>This is because <span class="math inline">\(PX = X\)</span> and the first column of <span class="math inline">\(X\)</span> is <span class="math inline">\((1,1,\dots,1)^\top\)</span>. This implies that <span class="math inline">\(P(1,1,\dots,1)^\top = (1,1,\dots,1)^\top\)</span>.</p>
<p><code>Theorem 10</code>: Suppose that Assumption B is satisfied and <span class="math inline">\(\mathrm{rank}(X)=p&lt;n\)</span>, we have</p>
<p>(1). <span class="math inline">\(S_R^2,S_e^2,\bar Y\)</span> are independent, and</p>
<p>(2). if the null <span class="math inline">\(H_0:\beta_1=\dots=\beta_{p-1}=0\)</span> is true, <span class="math inline">\(S_R^2/\sigma^2\sim\chi^2(p-1)\)</span>.</p>
<p><code>Proof</code>: Using the same notations in Theorem 9. Since <span class="math inline">\((1,\dots,1)^\top\in \mathrm{span}(X)\)</span>, we set <span class="math inline">\(\xi_1 = (1/\sqrt{n},1/\sqrt{n},\dots,1/\sqrt{n})^\top\)</span>. Recall that
<span class="math display">\[Y = \sum_{i=1}^n z_i\xi_i = Uz.\]</span>
The average <span class="math inline">\(\bar Y = (1/n,\dots,1/n)Y = (1/n,\dots,1/n)Uz=z_1/\sqrt{n},\)</span> where we used the fact that <span class="math inline">\(U\)</span> is orthogonal matrix and the first colum of <span class="math inline">\(U\)</span> is <span class="math inline">\(\xi_1 = (1/\sqrt{n},1/\sqrt{n},\dots,1/\sqrt{n})^\top\)</span>.</p>
<p>Recall that <span class="math inline">\(\hat Y = \sum_{i=1}^p z_i\xi_i = (z_1/\sqrt{n},z_1/\sqrt{n},\dots,z_1/\sqrt{n})^\top+\sum_{i=2}^p z_i\xi_i\)</span>.
This implies that
<span class="math display">\[(\hat y_1-\bar Y,\dots,\hat y_n-\bar Y)^\top = \sum_{i=2}^p z_i\xi_i.\]</span>
As a result, <span class="math inline">\(S_R^2 = \sum_{i=2}^p z_i^2\)</span>. Recall that <span class="math inline">\(S_e^2=\sum_{i=p+1}^n z_i^2\)</span>. Since <span class="math inline">\(z_i\)</span> are independent, <span class="math inline">\(S_R^2,S_e^2,\bar Y\)</span> are independent.</p>
<p>If <span class="math inline">\(\beta_1=\dots=\beta_{p-1}=0\)</span>, we have</p>
<p><span class="math display">\[E[z] = U^\top X\beta = U^\top (\beta_0,\dots,\beta_0)^\top.\]</span></p>
<p><span class="math display">\[E[z^\top]=\beta_0(1,1,\dots,1) U = \beta_0(\sqrt{n},0,\dots,0).\]</span>
We therefore have <span class="math inline">\(z_i\sim N(0,\sigma^2)\)</span> for <span class="math inline">\(i=2,\dots,n\)</span>. So <span class="math display">\[S_R^2/\sigma^2 = \sum_{i=2}^p (z_i/\sigma)^2\sim \chi^2(p-1).\]</span></p>
<p>We now use generalized likelihood (GLR) test. The likelihood function for <span class="math inline">\(Y\)</span> is given by</p>
<p><span class="math display">\[L(\beta,\sigma^2) = (2\pi \sigma^2)^{-n/2} e^{-\frac{||Y-X\beta||^2}{2\sigma^2}}.\]</span></p>
<p>The maximizers for <span class="math inline">\(L(\beta,\sigma^2)\)</span> over the parameter space <span class="math inline">\(\Theta=\{(\beta,\sigma^2)|\beta\in \mathbb{R}^p,\sigma^2&gt;0\}\)</span> are</p>
<p><span class="math display">\[\hat\beta = (X^\top X)^{-1}X^\top Y,\ \hat\sigma^2 = \frac{||Y-X\hat \beta||}{n}=\frac{S_e^2}{n}.\]</span></p>
<p>The maximizers for <span class="math inline">\(L(\beta,\sigma^2)\)</span> over the sub-space <span class="math inline">\(\Theta_0=\{(\beta,\sigma^2)|\beta_i=0,i\ge 1,\sigma^2&gt;0\}\)</span> are</p>
<p><span class="math display">\[\hat\beta^* = (\bar Y,0,\dots,0)^\top,\ \hat\sigma^{*2} = \frac{||Y-X\hat \beta^*||}{n}=\frac{S_T^2}{n}.\]</span></p>
<p>The likelihood ratio is then given by</p>
<p><span class="math display">\[\lambda = \frac{\sup_{\theta\in\Theta}L(\beta,\sigma^2)}{\sup_{\theta\in\Theta_0}L(\beta,\sigma^2)} = \frac{L(\hat\beta,\hat\sigma^2)}{L(\hat\beta^*,\hat\sigma^{*2})}= \left(\frac{S_T^2}{S_e^2}\right)^{n/2}= \left(1+\frac{S_R^2}{S_e^2}\right)^{n/2}.\]</span></p>
<p>By Theorems 9 and 10, if the null is true we have
<span class="math display">\[F=\frac{S_R^2/(p-1)}{S_e^2/(n-p)}\sim F(p-1,n-p).\]</span>
We take <span class="math inline">\(F\)</span> as the test statistic. The rejection region is <span class="math inline">\(W=\{F&gt;C\}\)</span>, where the critical value <span class="math inline">\(C = F_{1-\alpha}(p-1,n-p)\)</span> so that <span class="math inline">\(\sup_{\theta\in\Theta_0}P_\theta(F&gt;C)=\alpha\)</span>.</p>
<p><code>Definition</code>: The <strong>coefficient of determination</strong> is sometimes used as a crude measure of the strength of a relationship that has been
fit by least squares. This coefficient is defined as</p>
<p><span class="math display">\[R^2 =\frac{S_R^2}{S_T^2}=\frac{\sum_{i=1}^n(\hat y_i-\bar y)^2}{\sum_{i=1}^n(y_i-\bar y)^2}.\]</span>
It can be interpreted as the proportion of the variability of the dependent variable that
can be explained by the independent variables.</p>
<p>It is easy to see that</p>
<p><span class="math display">\[F = \frac{S_T^2 R^2/(p-1)}{S_T^2(1-R^2)/(n-p)}=\frac{ R^2/(p-1)}{(1-R^2)/(n-p)}.\]</span></p>
<p>For the simple linear model <span class="math inline">\(p=2\)</span>, we have</p>
<p><span class="math display">\[S_R^2 = \sum_{i=1}^n(\hat y_i-\bar y)^2 = \hat\beta_1^2\sum_{i=1}^n(x_i-\bar x)^2 = \frac{\ell_{xy}^2}{\ell_{xx}}.\]</span>
This gives
<span class="math display">\[R^2 = \frac{\ell_{xy}^2}{\ell_{xx}\ell_{yy}} = \rho^2,\]</span>
where <span class="math inline">\(\rho = \ell_{xy}/\sqrt{\ell_{xx}\ell_{yy}}\)</span> is the <strong>correlation coefficient</strong> between <span class="math inline">\(x_i\)</span> and <span class="math inline">\(y_i\)</span>.</p>
</div>
</div>
<div id="case-study-2" class="section level2">
<h2>Case study 2</h2>
<p>It is found that the systolic pressure is linked to the weight and the age. We now have the following data.</p>
<pre class="r"><code>blood=data.frame(
weight=c(76.0,91.5,85.5,82.5,79.0,80.5,74.5,79.0,85.0,76.5,82.0,95.0,92.5),
age=c(50,20,20,30,30,50,60,50,40,55,40,40,20),
pressure=c(120,141,124,126,117,125,123,125,132,123,132,155,147))
knitr::kable(blood,format=&quot;html&quot;)</code></pre>
<table>
<thead>
<tr>
<th style="text-align:right;">
weight
</th>
<th style="text-align:right;">
age
</th>
<th style="text-align:right;">
pressure
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
76.0
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
120
</td>
</tr>
<tr>
<td style="text-align:right;">
91.5
</td>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
141
</td>
</tr>
<tr>
<td style="text-align:right;">
85.5
</td>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
124
</td>
</tr>
<tr>
<td style="text-align:right;">
82.5
</td>
<td style="text-align:right;">
30
</td>
<td style="text-align:right;">
126
</td>
</tr>
<tr>
<td style="text-align:right;">
79.0
</td>
<td style="text-align:right;">
30
</td>
<td style="text-align:right;">
117
</td>
</tr>
<tr>
<td style="text-align:right;">
80.5
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
125
</td>
</tr>
<tr>
<td style="text-align:right;">
74.5
</td>
<td style="text-align:right;">
60
</td>
<td style="text-align:right;">
123
</td>
</tr>
<tr>
<td style="text-align:right;">
79.0
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
125
</td>
</tr>
<tr>
<td style="text-align:right;">
85.0
</td>
<td style="text-align:right;">
40
</td>
<td style="text-align:right;">
132
</td>
</tr>
<tr>
<td style="text-align:right;">
76.5
</td>
<td style="text-align:right;">
55
</td>
<td style="text-align:right;">
123
</td>
</tr>
<tr>
<td style="text-align:right;">
82.0
</td>
<td style="text-align:right;">
40
</td>
<td style="text-align:right;">
132
</td>
</tr>
<tr>
<td style="text-align:right;">
95.0
</td>
<td style="text-align:right;">
40
</td>
<td style="text-align:right;">
155
</td>
</tr>
<tr>
<td style="text-align:right;">
92.5
</td>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
147
</td>
</tr>
</tbody>
</table>
<pre class="r"><code>plot(blood)</code></pre>
<p><img src="/post/chap04_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<pre class="r"><code>lm.blood=lm(pressure~weight+age,data=blood)
summary(lm.blood)</code></pre>
<pre><code>## 
## Call:
## lm(formula = pressure ~ weight + age, data = blood)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.0404 -1.0183  0.4640  0.6908  4.3274 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -62.96336   16.99976  -3.704 0.004083 ** 
## weight        2.13656    0.17534  12.185 2.53e-07 ***
## age           0.40022    0.08321   4.810 0.000713 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.854 on 10 degrees of freedom
## Multiple R-squared:  0.9461, Adjusted R-squared:  0.9354 
## F-statistic: 87.84 on 2 and 10 DF,  p-value: 4.531e-07</code></pre>
<p>The regression function is</p>
<p><span class="math display">\[\hat y = -62.96336 + 2.13656 x_1+ 0.40022 x_2.\]</span></p>
<pre class="r"><code>n = length(blood$weight)
X = cbind(intercept=rep(1,n),weight=blood$weight,age=blood$age)
C = solve(t(X)%*%X)
SSE = sum(lm.blood$residuals^2) # sum of squared errors
# SST = var(blood$pressure)*(n-1)
# SSR = SST-SSE
# Fstat = SSR/(3-1)/(SSE/(n-3))
cov = SSE/(n-3)*C
knitr::kable(cov,format=&quot;html&quot;,caption = &quot;Estimated Covariance Matrix&quot;)</code></pre>
<table>
<caption>
<span id="tab:unnamed-chunk-9">Table 2: </span>Estimated Covariance Matrix
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
intercept
</th>
<th style="text-align:right;">
weight
</th>
<th style="text-align:right;">
age
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
intercept
</td>
<td style="text-align:right;">
288.991861
</td>
<td style="text-align:right;">
-2.9499280
</td>
<td style="text-align:right;">
-1.1174334
</td>
</tr>
<tr>
<td style="text-align:left;">
weight
</td>
<td style="text-align:right;">
-2.949928
</td>
<td style="text-align:right;">
0.0307450
</td>
<td style="text-align:right;">
0.0102176
</td>
</tr>
<tr>
<td style="text-align:left;">
age
</td>
<td style="text-align:right;">
-1.117433
</td>
<td style="text-align:right;">
0.0102176
</td>
<td style="text-align:right;">
0.0069243
</td>
</tr>
</tbody>
</table>
<p>Similarly to the simple linear model, we can construct the confidence intervals and prediction intervals for <span class="math inline">\(E[y]\)</span> and <span class="math inline">\(y\)</span>, respectively.</p>
<pre class="r"><code>newdata = data.frame(
        age = rep(31,100),
        weight = seq(70,100,length.out = 100)
)
CI = predict(lm.blood,newdata,interval = &quot;confidence&quot;)
Pred = predict(lm.blood,newdata,interval = &quot;prediction&quot;)
par(mar=c(4,4,2,1))
matplot(newdata$weight,cbind(CI,Pred[,-1]),type=&quot;l&quot;,lty = c(1,5,5,2,2),
        col=c(&quot;blue&quot;,&quot;red&quot;,&quot;red&quot;,&quot;brown&quot;,&quot;brown&quot;),lwd=2,
        xlab=&quot;Weight&quot;,ylab=&quot;Pressure&quot;,main = &quot;Age = 31&quot;)
legend(70,160,c(&quot;Fitted&quot;,&quot;Confidence&quot;,&quot;Prediction&quot;),
       lty = c(1,5,2),col=c(&quot;blue&quot;,&quot;red&quot;,&quot;brown&quot;))</code></pre>
<p><img src="/post/chap04_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<pre class="r"><code>newdata = data.frame(
        weight = rep(85,41),
        age = seq(20,60)
)
CI = predict(lm.blood,newdata,interval = &quot;confidence&quot;)
Pred = predict(lm.blood,newdata,interval = &quot;prediction&quot;)
par(mar=c(4,4,2,1))
matplot(newdata$age,cbind(CI,Pred[,-1]),type=&quot;l&quot;,lty = c(1,5,5,2,2),
        col=c(&quot;blue&quot;,&quot;red&quot;,&quot;red&quot;,&quot;brown&quot;,&quot;brown&quot;),lwd=2,
        xlab=&quot;Age&quot;,ylab=&quot;Pressure&quot;,main = &quot;Weight = 85&quot;)
legend(20,150,c(&quot;Fitted&quot;,&quot;Confidence&quot;,&quot;Prediction&quot;),
       lty = c(1,5,2),col=c(&quot;blue&quot;,&quot;red&quot;,&quot;brown&quot;))</code></pre>
<p><img src="/post/chap04_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
</div>

    </div>

    


<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/%E8%AF%BE%E4%BB%B6/">课件</a>
  
  <a class="badge badge-light" href="/tags/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/">数理统计</a>
  
</div>




    
    
    <div class="article-widget">
      <div class="hr-light"></div>
      <h3>Related</h3>
      <ul>
        
        <li><a href="/post/chap03/">第三章：假设检验</a></li>
        
        <li><a href="/post/chap02/">第二章：估计</a></li>
        
        <li><a href="/post/chap01/">第一章：绪论</a></li>
        
        <li><a href="/post/chap00/">《数理统计》课程简介</a></li>
        
        <li><a href="/post/homework8/">第八次作业</a></li>
        
      </ul>
    </div>
    

    

    


  </div>
</article>

<div class="container">
  <footer class="site-footer">
  
  <p class="powered-by">
    <a href="/privacy/">Privacy Policy</a>
  </p>
  

  <p class="powered-by">
    Zhijian He &copy; 2018 &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

</div>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        CommonHTML: { linebreaks: { automatic: true } },
        tex2jax: { inlineMath: [ ['$', '$'], ['\\(','\\)'] ], displayMath: [ ['$$','$$'], ['\\[', '\\]'] ], processEscapes: false },
        TeX: { noUndefined: { attributes: { mathcolor: 'red', mathbackground: '#FFEEEE', mathsize: '90%' } } },
        messageStyle: 'none'
      });
    </script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha512-+NqPlbbtM1QqiK8ZAo4Yrj2c4lNQoGv8P79DPtKzj++l5jnN39rHA/xsqn8zE9l0uSoxaCdrOgFs6yjyfbBxSg==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha256-VsEqElsCHSGmnmHXGQzvoWjWwoznFSZc6hs7ARLRacQ=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    <script src="/js/hugo-academic.js"></script>
    

    
    

    
    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "Search Results",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    <script src="/js/search.js"></script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

  </body>
</html>

