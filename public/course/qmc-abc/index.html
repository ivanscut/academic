<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 2.4.0">
  <meta name="generator" content="Hugo 0.48" />
  <meta name="author" content="Zhijian He">

  
  
  
  
    
  
  <meta name="description" content="Ingredients for ABCsummary statistic \(S(y):\mathbb{R}^n\to \mathbb{R}^d\)
kernel function \(k(u_1,\dots,u_d)\)
bandwidth \(h&gt;0\)
proposal density \(g(\theta)\)
ABC approximation
\[\pi_{ABC}(\theta|s_{obs})\propto \pi(\theta)\int \pi(s|\theta)K((s-s_{obs})/h) d s\to \pi(\theta|s_{obs})\]
If \(\pi(\theta|s_{obs})\approx \pi(\theta|y)\), then ABC density is a proper approximation of the posterior \(\pi(\theta|y)\).
ABC convergence ratesConsider the estimation of \(\mu=E[a(\theta)|s_{obs}]\). The acceptance-rejection (AR) based ABC estimate is given by\[\hat\mu=\frac{1}{N}\sum_{i=1}^Na(\theta^{(i)}).\]
Under regular conditions, ABC bias is
\[\mathrm{bias}=|E_{ABC}[a(\theta)|s_{obs}]-\mu|=O(h^2),\]
and the acceptance probability is \(R = O(h^{d}),\)">

  
  <link rel="alternate" hreflang="en-us" href="/course/qmc-abc/">

  


  

  
  
  
  <meta name="theme-color" content="#0095eb">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha256-eSi1q2PG6J7g7ib17yAaWMcrr5GrtohYChqibrV7PBE=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" crossorigin="anonymous">
        
      
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono">
  

  <link rel="stylesheet" href="/styles.css">
  

  
  
  

  
  <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Dr. Zhijian He">
  <link rel="feed" href="/index.xml" type="application/rss+xml" title="Dr. Zhijian He">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/course/qmc-abc/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Dr. Zhijian He">
  <meta property="og:url" content="/course/qmc-abc/">
  <meta property="og:title" content="Quasi-Monte Carlo in ABC | Dr. Zhijian He">
  <meta property="og:description" content="Ingredients for ABCsummary statistic \(S(y):\mathbb{R}^n\to \mathbb{R}^d\)
kernel function \(k(u_1,\dots,u_d)\)
bandwidth \(h&gt;0\)
proposal density \(g(\theta)\)
ABC approximation
\[\pi_{ABC}(\theta|s_{obs})\propto \pi(\theta)\int \pi(s|\theta)K((s-s_{obs})/h) d s\to \pi(\theta|s_{obs})\]
If \(\pi(\theta|s_{obs})\approx \pi(\theta|y)\), then ABC density is a proper approximation of the posterior \(\pi(\theta|y)\).
ABC convergence ratesConsider the estimation of \(\mu=E[a(\theta)|s_{obs}]\). The acceptance-rejection (AR) based ABC estimate is given by\[\hat\mu=\frac{1}{N}\sum_{i=1}^Na(\theta^{(i)}).\]
Under regular conditions, ABC bias is
\[\mathrm{bias}=|E_{ABC}[a(\theta)|s_{obs}]-\mu|=O(h^2),\]
and the acceptance probability is \(R = O(h^{d}),\)">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2018-12-18T00:00:00&#43;00:00">
  
  <meta property="article:modified_time" content="2018-12-18T00:00:00&#43;00:00">
  

  

  

  <title>Quasi-Monte Carlo in ABC | Dr. Zhijian He</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Dr. Zhijian He</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#publications_selected">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/talk">
            
            <span>Talks</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/course/">
            
            <span>Courses</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>



<div class="container-fluid docs">
  <div class="row flex-xl-nowrap">
    <div class="col-12 col-md-3 col-xl-2 docs-sidebar">
      





<form class="docs-search d-flex align-items-center">
  <input name="q" type="search" class="form-control" id="search-query" placeholder="Search..." autocomplete="off">
</form>


<nav class="docs-links" id="docs-nav">
  
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/course/">Overview</a>

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/course/bchap01/">Bayesian Statistics</a>
    <ul class="nav docs-sidenav">
      
      <li >
        <a href="/course/bchap01/">Chapter 1</a>
      </li>
      
      <li >
        <a href="/course/bchap02/">Chapter 2</a>
      </li>
      
      <li >
        <a href="/course/bchap03/">Chapter 3</a>
      </li>
      
      <li >
        <a href="/course/bchap04/">Chapter 4</a>
      </li>
      
      <li >
        <a href="/course/bchap05/">Chapter 5</a>
      </li>
      
      <li >
        <a href="/course/bchap10/">Chapter 10</a>
      </li>
      
      <li >
        <a href="/course/abc/">ABC-Review</a>
      </li>
      
      <li class="active">
        <a href="/course/qmc-abc/">QMC-ABC</a>
      </li>
      
    </ul>
    

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/course/chap00/">数理统计</a>
    <ul class="nav docs-sidenav">
      
      <li >
        <a href="/course/chap00/">课程简介</a>
      </li>
      
      <li >
        <a href="/course/chap01/">第一章</a>
      </li>
      
      <li >
        <a href="/course/chap02/">第二章</a>
      </li>
      
      <li >
        <a href="/course/chap03/">第三章</a>
      </li>
      
      <li >
        <a href="/course/chap04/">第四章</a>
      </li>
      
      <li >
        <a href="/course/ex2/">R密度估计</a>
      </li>
      
      <li >
        <a href="/course/ex1/">R画图技巧</a>
      </li>
      
      <li >
        <a href="/course/homework1/">第一次作业</a>
      </li>
      
      <li >
        <a href="/course/homework2/">第二次作业</a>
      </li>
      
      <li >
        <a href="/course/homework3/">第三次作业</a>
      </li>
      
      <li >
        <a href="/course/homework4/">第四次作业</a>
      </li>
      
      <li >
        <a href="/course/homework5/">第五次作业</a>
      </li>
      
      <li >
        <a href="/course/homework6/">第六次作业</a>
      </li>
      
      <li >
        <a href="/course/homework7/">第七次作业</a>
      </li>
      
      <li >
        <a href="/course/homework8/">第八次作业</a>
      </li>
      
      <li >
        <a href="/course/homework9/">第九次作业</a>
      </li>
      
      <li >
        <a href="/course/homework10/">第十次作业</a>
      </li>
      
      <li >
        <a href="/course/homework11/">第十一次作业</a>
      </li>
      
      <li >
        <a href="/course/testa/">期末试卷</a>
      </li>
      
    </ul>
    

  </div>
  
  
</nav>

    </div>

    

    <main class="col-12 col-md-9 col-xl-8 py-md-3 pl-md-5 docs-content" role="main">
      <div id="search-hits">
        
      </div>
      <article class="article" itemscope itemtype="http://schema.org/Article">

        


        <div class="docs-article-container">
          <h1 itemprop="name">Quasi-Monte Carlo in ABC</h1>

          <div class="article-style" itemprop="articleBody">
            <div id="ingredients-for-abc" class="section level2">
<h2>Ingredients for ABC</h2>
<ul>
<li><p>summary statistic <span class="math inline">\(S(y):\mathbb{R}^n\to \mathbb{R}^d\)</span></p></li>
<li><p>kernel function <span class="math inline">\(k(u_1,\dots,u_d)\)</span></p></li>
<li><p>bandwidth <span class="math inline">\(h&gt;0\)</span></p></li>
<li><p>proposal density <span class="math inline">\(g(\theta)\)</span></p></li>
</ul>
<p>ABC approximation</p>
<p><span class="math display">\[\pi_{ABC}(\theta|s_{obs})\propto \pi(\theta)\int \pi(s|\theta)K((s-s_{obs})/h) d s\to \pi(\theta|s_{obs})\]</span></p>
<p>If <span class="math inline">\(\pi(\theta|s_{obs})\approx \pi(\theta|y)\)</span>, then ABC density is a proper approximation of the posterior <span class="math inline">\(\pi(\theta|y)\)</span>.</p>
</div>
<div id="abc-convergence-rates" class="section level2">
<h2>ABC convergence rates</h2>
<p>Consider the estimation of <span class="math inline">\(\mu=E[a(\theta)|s_{obs}]\)</span>. The acceptance-rejection (AR) based ABC estimate is given by
<span class="math display">\[\hat\mu=\frac{1}{N}\sum_{i=1}^Na(\theta^{(i)}).\]</span></p>
<p>Under regular conditions, ABC bias is</p>
<p><span class="math display">\[\mathrm{bias}=|E_{ABC}[a(\theta)|s_{obs}]-\mu|=O(h^2),\]</span></p>
<p>and the acceptance probability is <span class="math inline">\(R = O(h^{d}),\)</span></p>
<p>and Monte Carlo variance is</p>
<p><span class="math display">\[\mathrm{variance}=\frac{\sigma^2_{ABC}}{N}=O\left(\frac{1}{C h^d}\right),\]</span></p>
<p>where <span class="math inline">\(C\)</span> is the complexity. Overall, the MSE is</p>
<p><span class="math display">\[\mathrm{MSE} = \mathrm{bias}^2+\mathrm{variance} = O(h^4)+O\left(\frac{1}{Ch^d}\right).\]</span></p>
<p>For a given <span class="math inline">\(C&gt;0\)</span>,</p>
<ul>
<li><p>the optimal <span class="math inline">\(h^*=O(C^{-1/(4+d)})\)</span></p></li>
<li><p>the optimal <span class="math inline">\(\mathrm{MSE}^*=O(C^{-4/(d+4)})\)</span></p></li>
</ul>
<p>This reveals that ABC suffers from the <strong>curse of dimensionality</strong>.</p>
</div>
<div id="improving-the-sampling-efficiency" class="section level2">
<h2>Improving the sampling efficiency</h2>
<p>A possible way to improve ABC efficiency is to accelerate the Monte Carlo. Monte Carlo error is</p>
<p><span class="math display">\[\text{MC error}=\frac{\sigma}{\sqrt{N}}\]</span></p>
<p>Some variance reduction techniques are proposed to reduce <span class="math inline">\(\sigma\)</span>, such as</p>
<ul>
<li>importance sampling</li>
<li>antithetic variates</li>
<li>control variates</li>
<li>hybrid strategies</li>
</ul>
<p>On the other hand, quasi-Monte Carlo is used to <strong>improve the rate of convergence</strong> (<span class="math inline">\(1/\sqrt{N}\)</span>) rather than the constant <span class="math inline">\(\sigma\)</span>.</p>
</div>
<div id="quasi-monte-carlo-a-review" class="section level2">
<h2>Quasi-Monte Carlo: A review</h2>
<p>As a start-up setting, let’s consider an intergal over the unit cube <span class="math inline">\([0,1]^d\)</span>:</p>
<p><span class="math display">\[\mu=\int_{[0,1]^d} f(u_1,\dots,u_d)du_1\cdots du_d.\]</span></p>
<p>MC estimate is the average of <span class="math inline">\(N\)</span> iid samples:</p>
<p><span class="math display">\[\hat\mu_N = \frac 1 N\sum_{i=1}^N f(u^{(i)}),\ u^{(i)}\stackrel{iid}{\sim} U[0,1]^d.\]</span></p>
<p>QMC estimate has the same form but uses deterministic sequences</p>
<p><span class="math display">\[\hat\mu_N = \frac 1 N\sum_{i=1}^N f(u^{(i)}),\ u^{(i)}\in[0,1]^d.\]</span></p>
<p>The sequences are clearly constructed with better uniformness, which are known as <strong>low discrepancy sequences (LDSs)</strong>:</p>
<ul>
<li>Halton (1960)</li>
<li>Sobol’ (1967)</li>
<li>Faure (1982)</li>
<li>Niderreiter (1992)</li>
<li>Lattice rules</li>
</ul>
<p><strong>Koksma-Hlawka inequality</strong> gives</p>
<p><span class="math display">\[
|\hat\mu_N-\mu|\le V_{\mathrm{HK}}(f)D^*(u^{(1)},...,u^{(N)})
\]</span></p>
<ul>
<li><p><span class="math inline">\(V_{\mathrm{HK}}(f)\)</span> is the variation in the sense of Hardy and Krause</p></li>
<li><p><span class="math inline">\(D^*\)</span> is the star discrepancy of the points</p></li>
</ul>
<p>If <span class="math inline">\(V_{\mathrm{HK}}(f)&lt;\infty\)</span> and <span class="math inline">\(\{u^{(1)},...,u^{(N)}\}\)</span> is a LDS, then</p>
<p><span class="math display">\[\text{QMC error}=O(N^{-1}(\log N)^d)\]</span></p>
<p>QMC can achieve higher-order rate of convergence, but needs higher smoothness conditions; see Dick (Ann. Stat., 2011).</p>
<p>Some mathematical softwares such as <code>Matlab</code>, <code>R</code> include some common generators of LDSs.</p>
<pre class="r"><code>#  install.packages(&quot;randtoolbox&quot;)
set.seed(7)
library(&quot;randtoolbox&quot;)
par(mfrow=c(2,2))
qmc = sobol(1024,2)
plot(qmc[1:256,],pch=19,xlab=&quot;x&quot;,ylab=&quot;y&quot;,main=&quot;Sobol&#39; points: N = 256&quot;)
plot(qmc,pch=19,xlab=&quot;x&quot;,ylab=&quot;y&quot;,main=&quot;Sobol&#39; points: N = 1024&quot;)
mc = matrix(runif(2048),ncol = 2)
plot(mc[1:256,],pch=19,xlab=&quot;x&quot;,ylab=&quot;y&quot;,main=&quot;MC points: N = 256&quot;)
plot(mc,pch=19,xlab=&quot;x&quot;,ylab=&quot;y&quot;,main=&quot;MC points: N = 1024&quot;)</code></pre>
<p><img src="/course/qmc-abc_files/figure-html/unnamed-chunk-1-1.png" width="960" /></p>
</div>
<div id="example-1" class="section level2">
<h2>Example 1</h2>
<p>Consider the integral:</p>
<p><span class="math display">\[\mu = \int_{[0,1]^d} \sum_{i=1}^d x_i^2 dx =\frac{d}{3}.\]</span></p>
<pre class="r"><code>myfun = function(x){
  #rowSums(x^2)
  apply(x-0.5,1,prod)
}

m = 16
N = 2^m
d = 8
#trueval = d/3
trueval = 0
qmc = as.matrix(sobol(n=N,dim=d),ncol=d)
fsum = cumsum(myfun(qmc))
ns = 1:N
fmean = fsum[ns]/ns
par(mar=c(4,4,2,1),mfrow=c(2,1))
tt = paste0(&quot;QMC: d = &quot;,d)
plot(ns,fmean,xlab=&quot;N&quot;,ylab=&quot;Mean&quot;,typ=&quot;l&quot;,main=tt)
abline(h=trueval,lty=5,col=&quot;red&quot;)
ns = 2^(0:m)
fmean = fsum[ns]/ns
err = abs(fmean-trueval)
plot(ns,err,xlab=&quot;N&quot;,ylab=&quot;Error&quot;,typ=&quot;b&quot;,log=&quot;xy&quot;,main=tt)
r = 1
lines(ns[c(3,m)], c(err[3],err[3]*(ns[3]/ns[m])^r),col=&quot;red&quot;,lty=5)
legend(500,err[2],legend = c(&quot;QMC errors&quot;,paste0(&quot;N^{&quot;,-r,&quot;}&quot;)),lty = c(1,5),
       col=c(&quot;black&quot;,&quot;red&quot;),pch=c(1,NA),cex=1.2)</code></pre>
<p><img src="/course/qmc-abc_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
</div>
<div id="randomized-qmc" class="section level2">
<h2>Randomized QMC</h2>
<p>In practice, we use randomized QMC (RQMC), which yields an unbiased estimate.</p>
<ul>
<li><p>random-shift, see Cranley and Patterson (1976)</p></li>
<li><p>scrambled nets, see Owen (1995,1997,1998)</p></li>
<li><p>Survey in L’Ecuyer and Lemieux (2005)</p></li>
</ul>
<pre class="r"><code>set.seed(7)
library(&quot;randtoolbox&quot;)
par(mfrow=c(2,2))
qmc = sobol(1024,2)
rqmc = sobol(1024,2,scrambling=1)
plot(qmc[1:256,],pch=19,xlab=&quot;x&quot;,ylab=&quot;y&quot;,main=&quot;Sobol&#39; points: N = 256&quot;)
plot(qmc,pch=19,xlab=&quot;x&quot;,ylab=&quot;y&quot;,main=&quot;Sobol&#39; points: N = 1024&quot;)
mc = matrix(runif(2048),ncol = 2)
plot(rqmc[1:256,],pch=19,xlab=&quot;x&quot;,ylab=&quot;y&quot;,main=&quot;RQMC points: N = 256&quot;)
plot(rqmc,pch=19,xlab=&quot;x&quot;,ylab=&quot;y&quot;,main=&quot;RQMC points: N = 1024&quot;)</code></pre>
<p><img src="/course/qmc-abc_files/figure-html/unnamed-chunk-3-1.png" width="960" /></p>
<pre class="r"><code>myfun = function(x){
  d = ncol(x)
  #(rowSums(x)&gt;d/2)-0.5
  rowSums(qnorm(x))
  #apply(x-0.5,1,prod)
  
}

m = 16
N = 2^m
d = 8
trueval = 0
R = 100
ns = 2^(0:m)
fmean = matrix(0,m+1,R)
tmp = sobol(N,d)##initialization
for(i in 1:R)
{
  rqmc = as.matrix(sobol(N,d,scrambling=1,init=FALSE),ncol=d)
  fsum = cumsum(myfun(rqmc))
  fmean[,i] = fsum[ns]/ns
}

par(mar=c(4,4,2,1),mfrow=c(2,1))
tt = paste0(&quot;RQMC: d = &quot;,d)
plot(ns,rowMeans(fmean),xlab=&quot;N&quot;,ylab=&quot;Mean&quot;,typ=&quot;b&quot;,main=tt)
abline(h=trueval,lty=5,col=&quot;red&quot;)

rmse = apply(fmean,1,sd)

plot(ns,rmse,xlab=&quot;N&quot;,ylab=&quot;rmse&quot;,typ=&quot;b&quot;,log=&quot;xy&quot;,main=tt)
r = 1
lines(ns[c(3,m)], c(rmse[3],rmse[3]*(ns[3]/ns[m])^r),col=&quot;red&quot;,lty=5)
legend(500,rmse[2],legend = c(&quot;RQMC errors&quot;,paste0(&quot;N^{&quot;,-r,&quot;}&quot;)),lty = c(1,5),
       col=c(&quot;black&quot;,&quot;red&quot;),pch=c(1,NA),cex=1.2)</code></pre>
<p><img src="/course/qmc-abc_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
<div id="using-qmc-in-abc" class="section level2">
<h2>Using QMC in ABC</h2>
<p>The univariate g-and-k distribution is a flexible unimodal distribution that
is able to describe data with significant amounts of skewness and kurtosis. Its density function has no closed form, but
is alternatively defined through its quantile function as:</p>
<p><span class="math display">\[Q(q|A,B,g,k)=A+B\left[1+c\frac{1-\exp\{-gz(q)\}}{1+\exp\{-gz(q)\}}\right](1+z(q)^2)^kz(q)\]</span></p>
<ul>
<li><p><span class="math inline">\(c=0.8,\ B&gt;0, k&gt;-1/2\)</span>, <span class="math inline">\(z(q)=\Phi^{-1}(q)\)</span></p></li>
<li><p>if <span class="math inline">\(g=k=0\)</span>, it is the normal density</p></li>
<li><p><span class="math inline">\(y_{obs}\)</span> of length <span class="math inline">\(1000\)</span> is generated from the <span class="math inline">\(g\)</span>-and-<span class="math inline">\(k\)</span> distribution with parameter <span class="math inline">\(\theta_0=(3,1,2,0.5)\)</span></p></li>
<li><p>prior density</p></li>
</ul>
<p><span class="math display">\[\pi(\theta) = \pi(A)\pi(B)\pi(g)\pi(k) = N(1,5)\times N(0.25,2) \times U(0,10) \times U(0,1)\]</span></p>
<p>Summary statistic (Drovandi and Pettitt, 2011): <span class="math inline">\(S(y) = (S_A,S_B,S_g,S_k)\)</span></p>
<ul>
<li><span class="math inline">\(S_A=E_4\)</span></li>
<li><span class="math inline">\(S_B=E_6-E_2\)</span></li>
<li><span class="math inline">\(S_g=(E_6+E_2-2E_4)/S_B\)</span></li>
<li><span class="math inline">\(S_k = (E_7-E_5+E_3-E_1)/S_B\)</span></li>
<li><span class="math inline">\(E_1\le E_2 \le \cdots \le E_8\)</span> are the octiles of <span class="math inline">\(y\)</span></li>
</ul>
<pre class="r"><code>set.seed(100)
theta0 = c(3,1,2,0.5)

gkmodel &lt;- function(theta,n,z=NA){
  if(is.na(z)[1]){
    z = rnorm(n)
  }
  y = theta[1] + theta[2]*(1+0.8*(1-exp(-theta[3]*z))/
        (1+exp(-theta[3]*z)))*(1+z^2)^theta[4]*z
  return(matrix(y,n,1))
}
n = 1e3
yobs = gkmodel(theta0,n)
## prior density
gkprior &lt;- function(n,u=NA){
  if(is.na(u)[1]){
    u = matrix(runif(n*4),n,4)
  }
  A = qnorm(u[,1])*sqrt(5)+1
  B = qnorm(u[,2])*sqrt(2)+.25
  g = u[,3]*10
  k = u[,4]
  return(cbind(A,B,g,k))
}
gksummary &lt;- function(y){
  sorty = sort(y)
  n = length(y)
  q = sorty[ceiling(n/8*(1:8))]
  s = c(q[4],
        q[6]-q[2],
        (q[6]+q[2]-2*q[4])/(q[6]-q[2]),
        (q[7]-q[5]+q[3]-q[1])/(q[6]-q[2]))
  return(s)
}
ytmp1 = matrix(0,2000,4)
for(i in 1:2000){
  ytmp1[i,] = gksummary(gkmodel(gkprior(1),n))
}
Sigma = var(ytmp1)
invsig = solve(Sigma)
N = 1e5
ytmp = rep(0,N)
theta = matrix(0,N,4)
stmp = rep(0,N)
sobs = gksummary(yobs)
#qmc = sobol(N+1,n+4)
#qmc = qmc[-1,]##initialization
for(i in 1:N){
  theta[i,] = gkprior(1) # matrix(qmc[i,1:4],1,4)
  ypro = gkmodel(theta[i,],n) # qnorm(matrix(qmc[i,-(1:4)],1,n))
  spro = gksummary(ypro)
  diff = matrix(spro-sobs,1,4)
  ytmp[i] = sqrt(sum((ypro-yobs)^2))
  stmp[i] = sqrt(diff%*%invsig%*%t(diff))
}
ysort = sort(ytmp)
ssort = sort(stmp)
effN = N*5e-3
hy = ysort[effN]
hs = ssort[effN]
## draw pairwise scatterplots
theta1 = theta[ytmp&lt;=hy,]
theta2 = theta[stmp&lt;=hs,]
theta = rbind(theta1,theta2,theta0)
colnames(theta) = c(&quot;A&quot;,&quot;B&quot;,&quot;g&quot;,&quot;k&quot;)
pairs(theta,pch=c(rep(20,effN*2),24),
      col=c(rep(&quot;grey&quot;,effN),rep(&quot;black&quot;,effN),&quot;red&quot;),cex=1)</code></pre>
</div>

          </div>

          

        </div>

        <div class="body-footer">
          Last updated on Dec 18, 2018
        </div>

      </article>

      <footer class="site-footer">
  
  <p class="powered-by">
    <a href="/privacy/">Privacy Policy</a>
  </p>
  

  <p class="powered-by">
    Zhijian He &copy; 2018 &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
  </p>
</footer>


    </main>
  </div>
</div>

    

    
    
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        CommonHTML: { linebreaks: { automatic: true } },
        tex2jax: { inlineMath: [ ['$', '$'], ['\\(','\\)'] ], displayMath: [ ['$$','$$'], ['\\[', '\\]'] ], processEscapes: false },
        TeX: { noUndefined: { attributes: { mathcolor: 'red', mathbackground: '#FFEEEE', mathsize: '90%' } } },
        messageStyle: 'none'
      });
    </script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha512-+NqPlbbtM1QqiK8ZAo4Yrj2c4lNQoGv8P79DPtKzj++l5jnN39rHA/xsqn8zE9l0uSoxaCdrOgFs6yjyfbBxSg==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha256-VsEqElsCHSGmnmHXGQzvoWjWwoznFSZc6hs7ARLRacQ=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    <script src="/js/hugo-academic.js"></script>
    

    
    

    
    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "Search Results",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.1.1/anchor.min.js" integrity="sha256-pB/deHc9CGfFpJRjC43imB29Rse8tak+5eXqntO94ck=" crossorigin="anonymous"></script>
    <script>
      anchors.add();
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    <script src="/js/search.js"></script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

  </body>
</html>


