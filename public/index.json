[{"authors":null,"categories":["作业"],"content":"课本P61第23题\n课本P62第27题\n课本P62第28题\n分析R软件的dslabs包中的身高数据heights, 利用R软件完成以下问题。相关的R语言操作见 https://hezhijian.netlify.com/post/ex2/\n假设整个总体服从正态分布，求期望和方差的95%置信区间。\n\r为了判断“正态总体”的假设的合理性，画图比较核估计密度与正态分布密度的差异？\n\r假设男生总体与女生总体均服从正态分布（方差相同）且独立，求这两个总体平均水平的差的95%置信区间。可否认为男生总体的平均身高大于女生总体的平均身高？你的理由是什么？\n\r为了考察第3问中“男女总体的方差相同”的假设是否合理，不妨求这两个总体的方差比的95%置信区间。并观察该置信区间是否包含1？\n\r\r","date":1540166400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540166400,"objectID":"839ac97b79678078a1f5a42c57139c7e","permalink":"/post/homework5/","publishdate":"2018-10-22T00:00:00Z","relpermalink":"/post/homework5/","section":"post","summary":"截止日期：2018-10-28 23:00","tags":["作业","数理统计"],"title":"第五次作业","type":"post"},{"authors":null,"categories":["R"],"content":"案例：身高数据\r数据来源于R的包dslabs，第一次使用时需要安装该包，命令为install.packages(\u0026quot;dslabs\u0026quot;)\n参考资料： Some datasets for teaching data science\n直方图\r直方图的R命令为：hist(...), 查看帮助?hist看具体参数含义\n\r核估计\r核估计的R命令为：density(...), 查看帮助?density看具体参数含义。注意该命令只是给出估计值的数据，不能直接画图，如果要画图则需要调用画图函数，如plot(...).\n以下代码展示所有身高数据的直方图与和核估计。\nif(!require(dslabs))\rinstall.packages(\u0026quot;dslabs\u0026quot;)\rattach(heights) #此命令用于使用该包里面的身高数据heights\rpar(mar=c(2,2,1,1)) #调整图形边距\r#直方图\rhist(height,breaks=10,ylim=c(0,.115),col = \u0026quot;lightblue\u0026quot;, border = \u0026quot;pink\u0026quot;,freq=FALSE,main=\u0026quot;Histogram vs. Kernel density\u0026quot;)\r#添加核估计数据\rlines(density(height,from = 50,to=85),col=\u0026quot;red\u0026quot;,lwd=2)\r下面代码比较男生和女生数据的核估计\nfemale_height = height[sex==\u0026quot;Female\u0026quot;]#提取女生数据\rmale_height = height[sex==\u0026quot;Male\u0026quot;]#提取男生数据\rpar(mar=c(2,2,1,1))\r#画男生数据\rplot(density(male_height,from = 50,to=85),col=\u0026quot;red\u0026quot;,lwd=2,ylim=c(0,.14),main=\u0026quot;Male vs. Female\u0026quot;)\r#添加女生数据\rlines(density(female_height,from = 50,to=85),col=\u0026quot;blue\u0026quot;,lwd=2)\r#画出图例说明\rlegend(74,0.12,legend = c(\u0026quot;Male\u0026quot;,\u0026quot;Female\u0026quot;),lty = c(1,1),col=c(\u0026quot;red\u0026quot;,\u0026quot;blue\u0026quot;),lwd=c(2,2))\r\r结论：身高数据可以近似看成正态分布，而且男生、女生两个总体的均值有差异，男生身高平均水平大于女生身高的平均水平。\n\r\r\r","date":1539907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539907200,"objectID":"1cd09b01a254f98a7df74a59103667d1","permalink":"/post/ex2/","publishdate":"2018-10-19T00:00:00Z","relpermalink":"/post/ex2/","section":"post","summary":"密度估计：直方图与核估计","tags":["上机练习","数理统计"],"title":"密度估计：直方图与核估计","type":"post"},{"authors":null,"categories":["作业"],"content":"设\\(X_1,\\dots,X_n\\)为来自参数为\\(\\lambda\\)的Poisson分布的样本. 在下列选项中选出用于估计参数\\(\\lambda\\)的无偏估计量。（多选题）\nA. \\(\\bar X\\)\nB. \\(S_n^{*2}=\\frac{1}{n-1}\\sum_{i=1}^{n}(X_i-\\bar X)^2\\)\nC. \\(\\frac 1 {n-1}\\sum_{i=1}^{n-1}X_i\\)\nD. \\(S_n^2=\\frac{1}{n}\\sum_{i=1}^n(X_i-\\bar X)^2\\)\nE. \\(\\frac{1}2 \\bar X + \\frac 12 S_n^{*2}\\)\n设\\(X_1,\\dots,X_n\\)为来自参数为\\(\\lambda\\)的Poisson分布的样本, 已知\\(\\bar X\\)是未知参数\\(\\lambda\\)的完全统计量。在下列选项中选出用于估计参数\\(\\lambda\\)的最有效的估计量。\nA. \\(\\bar X\\)\nB. \\(S_n^{*2}=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar X)^2\\)\nC. \\(\\frac 1 {n-1}\\sum_{i=1}^{n-1}X_i\\)\nD. \\(\\frac{1}2 \\bar X + \\frac 12 S_n^{*2}\\)\n设\\(X,\\dots,X_n\\)为来自参数为\\(\\lambda\\)的Poisson分布的样本，求\\(\\lambda^2\\)的无偏估计。已知\\(\\bar X\\)是参数\\(\\lambda\\)的完全统计量，能否找到\\(\\lambda^2\\)的最小方差无偏估计量？\n设\\(X_1,\\dots,X_n\\)为\\(N(\\mu,\\sigma^2)\\)分布的样本，参数\\(\\mu,\\sigma^2\\)未知。证明样本方差\\(S_n^2\\)与修正样本方差\\(S_n^{*2}\\)均为\\(\\sigma^2\\)的弱相合估计量。\n设\\(X_1,\\dots,X_n\\)为\\(N(\\mu,\\sigma^2)\\)分布的样本，参数\\(\\mu,\\sigma^2\\)未知。样本方差\\(S_n^2\\)与修正样本方差\\(S_n^{*2}\\)作为\\(\\sigma^2\\)的两种估计量，哪个更有效？由B-L-S定理知，\\(S_n^{*2}\\)是最小方差无偏估计量，这是否与你所得的结论矛盾？由此你能得到什么启发？\n设\\(X_1,\\dots,X_n\\)为总体\\(N(\\mu,\\sigma^2)\\), 其中\\(\\mu\\)已知，\\(\\sigma^2\\)未知。证明\\(\\sigma^2\\)的估计量\r\\[T(X_1,\\dots,X_n)=\\frac 1n\\sum_{i=1}^n(X_i-\\mu)^2\\]\r的方差达到C-R不等式的下界。\n","date":1539302400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539302400,"objectID":"2229665408f433affac80cacae21a9b6","permalink":"/post/homework4/","publishdate":"2018-10-12T00:00:00Z","relpermalink":"/post/homework4/","section":"post","summary":"截止日期：2018-10-21 23:00","tags":["作业","数理统计"],"title":"第四次作业","type":"post"},{"authors":null,"categories":["slides"],"content":"Introduction to hierarchial models\rMany statistical applications involve multiple parameters (say, \\(\\theta_1,\\dots,\\theta_J\\)) that can be regarded as related or connected in some way by the structure of the problem.\n\rfor the group \\(j\\in 1{:}J\\), we have the observed data \\(y_{ij}\\), \\(i=1,\\dots,n_j\\) from the population distribution with unknown parameter \\(\\theta_j\\)\n\rwe use a prior distribution in which the \\(\\theta_j\\)’s are viewed as a sample from a common population distribution, say \\(p(\\theta|\\phi)\\), where \\(\\phi\\) is known as hyperparameters. Assume that \\(\\theta_j\\) are iid, i.e.,\r\\[p(\\theta|\\phi)=\\prod_{j=1}^Jp(\\theta_j|\\phi)\\]\n\r\r\rHierarchical model for Rats experiment\rThe experiment is used to estimate the probability \\(\\theta\\) of tumor in a population of female laboratory rats of type ‘F344’ that receive a zero dose of the drug. The data show that 4 out of 14 rats developed a kind of tumor.\n\rassume a binomial model for the number of tumors\rselect a prior from the conjugate family, i.e., \\(\\theta\\sim Beta(\\alpha,\\beta)\\)\rthe posterior is therefore \\(Beta(\\alpha+1,\\beta+10)\\)\r\rThe question is how to determine the hyperparameters \\(\\phi=(\\alpha,\\beta)\\)\n\rhistorical data are available on previous experiments on similar groups of rats: in the jth historical experiments, let the number of rats with tumors be \\(y_j\\) and the total number of rats be \\(n_j\\), the parameters for the populations are \\(\\theta_j\\), \\(j=1,\\dots,70\\).\rfor current experiment, let \\(y_{71},n_{71},\\theta_{71}\\) be the associated notations.\r\r\rHistorical data for the 70 historical experiments\r## [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 2\r## [24] 2 2 2 2 2 2 2 2 1 5 2 5 3 2 7 7 3 3 2 9 10 4 4\r## [47] 4 4 4 4 4 10 4 4 4 5 11 12 5 5 6 5 6 6 6 6 16 15 15\r## [70] 9 4\r## [1] 20 20 20 20 20 20 20 19 19 19 19 18 18 17 20 20 20 20 19 19 18 18 25\r## [24] 24 23 20 20 20 20 20 20 10 49 19 46 27 17 49 47 20 20 13 48 50 20 20\r## [47] 20 20 20 20 20 48 19 19 19 22 46 49 20 20 23 19 22 20 20 20 52 46 47\r## [70] 24 14\r\rViewed as separate models using uniform priors\r\rViewed as a pooled model using uniform prior\r\rUsing the historical data to estimate the hyperparameters\r\rthe sample mean and standard deviation of the 70 values \\(y_i/n_i\\) are 0.136 and 0.103\n\rlet \\(E[\\theta]=\\frac{\\alpha}{\\alpha+\\beta}=0.136\\) and \\(Var[\\theta]=\\frac{E[\\theta](1-E[\\theta])}{\\alpha+\\beta+1}=0.103\\)\n\r\\(\\hat{\\alpha}=1.4,\\ \\hat{\\beta}=8.6\\)\n\rfor the current exeriment, the posterior for \\(\\theta\\) is \\(Beta(5.4,18.6)\\), posterior mean is \\(0.223\\), standard deviation is 0.083.\n\r\rThere are several logical and practical problems with the approach of directly estimating a prior distribution from existing data:\n\rthe data will be used twice for inference about the first 70 experiments – overestimate our precision\n\rthe point estimate for \\(\\alpha,\\beta\\) seems arbitrary that necessarily ignores some posterior uncertainty\n\rthis is not the logic of Bayesian inference\n\r\r\rThe full Bayesian treatment of the hierarchical model\rSuppose the hyperparameters \\(\\phi\\) has its own prior distribution \\(p(\\phi)\\), which is called hyperprior distribution. The appropriate Bayesian posterior distribution is of the vector \\((\\phi,\\theta)\\).\n\rthe joint prior distribution is\r\\[p(\\phi,\\theta)=p(\\phi)p(\\theta|\\phi)\\]\n\rthe joint posterior distribution is\r\\[p(\\phi,\\theta|y)\\propto p(\\phi,\\theta)p(y|\\phi,\\theta)=p(\\phi)p(\\theta|\\phi)p(y|\\theta)\\]\n\r\rPreviously, we assumed \\(\\phi\\) was known, which is unrealistic; now we include the uncertainty in \\(\\phi\\) in the model.\n\rFully Bayesian analysis of conjugate hierarchical models\rConsider the setting in which \\(p(\\theta|\\phi)\\) is conjugate to the likelihood \\(p(y|\\theta)\\). For this case, it is easy to determine analytically \\[p(\\theta|\\phi,y)\\propto p(\\theta|\\phi)p(y|\\theta)\\]\n\rthe joint posterior density:\r\\[p(\\phi,\\theta|y)\\propto p(\\phi)p(\\theta|\\phi)p(y|\\theta)\\]\rthe marginal posterior density \\(p(\\phi|y)\\) can be computed via\r\\[p(\\phi|y)=\\int p(\\phi,\\theta|y)d \\theta\\]\r\r\\[\\text{or }p(\\phi|y)=\\frac{p(\\phi,\\theta|y)}{p(\\theta|\\phi,y)}\\]\n\rApplication to the model for rat tumors\rThe binomial model:\r\\[y_j\\sim Bin(n_j,\\theta_j),\\ j=1,\\dots,J=71\\]\nThe parameters \\(\\theta_j\\) are assumed to be independent samples from a beta distribution:\r\\[\\theta_j\\sim Beta(\\alpha,\\beta)\\]\nThe joint posterior density is\r\\[p(\\theta,\\alpha,\\beta|y)\\propto p(\\alpha,\\beta)p(\\theta|\\alpha,\\beta)p(y|\\theta)\\]\r\\[\\propto p(\\alpha,\\beta)\\prod_{j=1}^J\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\theta_j^{\\alpha-1}(1-\\theta_j)^{\\beta-1}\\prod_{j=1}^J\\theta_j^{y_j}(1-\\theta_j)^{n_j-y_j}\\]\n\\[p(\\theta|\\alpha,\\beta,y)=\\prod_{j=1}^J\\frac{\\Gamma(\\alpha+\\beta+n_j)}{\\Gamma(\\alpha+y_j)\\Gamma(\\beta+n_j-y_j)}\\theta_j^{\\alpha+y_i-1}(1-\\theta_j)^{\\beta+n_j-y_j-1}\\]\n\rApplication to the model for rat tumors\rThe marginal posterior density:\r\\[p(\\alpha,\\beta|y)\\propto p(\\alpha,\\beta)\\prod_{j=1}^J\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\frac{\\Gamma(\\alpha+y_j)\\Gamma(\\beta+n_j-y_j)}{\\Gamma(\\alpha+\\beta+n_j)}\\]\nChoosing a noninformative hyperprior distribution:\r\\[p(\\alpha,\\beta)\\propto (\\alpha+\\beta)^{-5/2}\\]\nThis implies that \\((\\alpha/(\\alpha+\\beta),(\\alpha+\\beta)^{-1/2})\\) is uniformly distributed.\n\rthe prior mean is \\(\\alpha/(\\alpha+\\beta)\\)\rthe prior variance is approximately \\((\\alpha+\\beta)^{-1}\\)\r\r\rPlot of the marginal posterior density\r\rCompare the separate model and hierarchical model\r\rHierarchical model based on normal distribution\rConsider \\(J\\) independent experiments, with experiment \\(j\\) estimating \\(\\theta_j\\) form \\(n_j\\) independent distributed data points \\(y_{ij}\\), each with known error variance \\(\\sigma^2\\), that is\r\\[y_{ij}|\\theta_j\\stackrel{iid}{\\sim} N(\\theta_j,\\sigma^2), \\text{ for }i=1,\\dots,n_j;\\ j=1,\\dots,J\\]\n\rdenote the sample mean of each group \\(j\\) as\r\\[\\bar{y}_{\\cdot j}=\\frac 1{n_j}\\sum_{i=1}^{n_j}y_{ij}\\]\n\rlet \\(\\sigma^2_j=\\sigma^2/n_j\\)\n\rthe likelihood for each \\(\\theta_j\\):\r\\[\\bar{y}_{\\cdot j}|\\theta_j\\sim N(\\theta_j,\\sigma_j^2)\\]\n\rfor the convenience of conjugacy, assume the paramerters \\(\\theta_j\\) are drawn from a normal distribution with hyperparameters \\(\\mu,\\tau\\):\r\\[p(\\theta_1,\\dots,\\theta_J|\\mu,\\tau)=\\prod_{j=1}^J N(\\theta_j|\\mu,\\tau^2)\\]\n\rassign noninformative uniform hyperprior density to \\(\\mu\\) given \\(\\tau\\):\r\\[p(\\mu,\\tau)=p(\\mu|\\tau)p(\\tau)\\propto p(\\tau)\\]\rprior distribution for \\(\\tau\\): \\(p(\\tau)\\propto 1\\)\rthe joint posterior density is\r\\[p(\\theta,\\mu,\\tau|y)\\propto p(\\mu,\\tau)p(\\theta|\\mu,\\tau)p(y|\\theta)\\]\n\r\r\\[p(\\theta,\\mu,\\tau|y)\\propto p(\\mu,\\tau)\\prod_{j=1}^J N(\\theta_j|\\mu,\\tau^2)\\prod_{j=1}^JN(\\bar{y}_{\\cdot j}|\\theta_j,\\sigma_j^2)\\]\n\rthe conditional posterior distirbution:\r\\[\\theta_j|\\mu,\\tau,y\\sim N(\\hat{\\theta}_j,V_j)\\]\r\rwhere\r\\[\\hat{\\theta}_j=\\frac{\\frac 1{\\sigma^2}\\bar{y}_{\\cdot j}+\\frac 1{\\tau^2}\\mu}{\\frac 1{\\sigma^2}+\\frac 1{\\tau^2}},\\ V_j=\\frac{1}{\\frac 1{\\sigma^2}+\\frac 1{\\tau^2}}\\]\n\rthe marginal posterior density can be computed in a simple way\r\\[p(\\mu,\\tau|y)\\propto p(\\mu,\\tau)p(y|\\mu,\\tau)\\]\n\r\\(\\bar{y}_{\\cdot j}|\\mu,\\tau\\sim N(\\mu,\\sigma_j^2+\\tau^2)\\)\n\r\r\\[p(\\mu,\\tau|y)\\propto p(\\mu,\\tau)\\prod_{j=1}^JN(\\bar{y}_{\\cdot j}|\\mu,\\sigma_j^2+\\tau^2)\\]\n\rposterior distribution of \\(\\mu\\) given \\(\\tau\\)\r\\[\\mu|\\tau,y\\sim N(\\hat{\\mu},V_{\\mu})\\]\r\rwhere\r\\[\\hat{\\mu}=\\frac{\\sum_{j=1}^J \\frac 1{\\sigma_j^2+\\tau^2}\\bar{y}_{\\cdot j}}{\\sum_{j=1}^J \\frac 1{\\sigma_j^2+\\tau^2}},\\ V_{\\mu}^{-1}=\\sum_{j=1}^J \\frac 1{\\sigma_j^2+\\tau^2}\\]\n\rposterior distribution of \\(\\tau\\):\r\\[p(\\tau|y)=\\frac{p(\\mu,\\tau|y)}{p(\\mu|\\tau,y)}\\propto \\frac{p(\\tau)\\prod_{j=1}^JN(\\bar{y}_{\\cdot j}|\\mu,\\sigma_j^2+\\tau^2)}{N(\\mu|\\hat{\\mu},V_{\\mu})}\\]\r\r\\[p(\\tau|y)\\propto p(\\tau)V_{\\mu}^{1/2}\\prod_{j=1}^J(\\sigma_j^2+\\tau^2)^{-1/2}\\exp\\left(-\\frac{(\\bar{y}_{\\cdot j}-\\hat{\\mu})^2}{2(\\sigma_j^2+\\tau^2)}\\right)\\]\n\rExample: parallel experiments in eight schools\rA study was performanced for the Educational Testing Service to analyze the effects of special coaching programs on test scores. Seperate randomized experiments were performed to estimate the effects of coaching programs for the SAT-V (Verbal).\n\r\r\r\rSchool\rEstiamted treatment effect \\(y_j\\)\rStandard error of effect estimate \\(\\sigma_j\\)\r\r\r\rA\r28\r15\r\rB\r8\r10\r\rC\r-3\r16\r\rD\r7\r11\r\rE\r-1\r9\r\rF\r1\r11\r\rG\r18\r10\r\rH\r12\r18\r\r\r\r\rComparisons\r\rPlot the posterior summaries\r\r","date":1538870400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538870400,"objectID":"38baa4fc7e283d33ef1355f82a095655","permalink":"/post/bayes_chap05/","publishdate":"2018-10-07T00:00:00Z","relpermalink":"/post/bayes_chap05/","section":"post","summary":"Chapter 5: Hierarchial models","tags":["slides","BDA"],"title":"Chapter 5: Hierarchial models","type":"post"},{"authors":null,"categories":["slides"],"content":"Large-sample theory\rAssumptions and notations:\n\rtrue distribution: \\(y_i\\stackrel {iid}{\\sim} f(\\cdot)\\)\r\\(\\theta\\in\\Theta\\)\rprior distribution: \\(p(\\theta)\\)\rmodel distribution: \\(p(y_i|\\theta)\\)\rKullback-Leibler divergence: a measure of ‘discrepancy’ between the model and the true distribution\r\\[KL(\\theta)= E\\left[\\log\\left(\\frac{f(y_i)}{p(y_i|\\theta)}\\right)\\right]=\\int \\log\\left(\\frac{f(y_i)}{p(y_i|\\theta)}\\right)f(y_i)dy_i\\]\n\r\\(\\theta_0\\): the unique minimizer of \\(KL(\\theta)\\)\rif \\(f(y_i) = p(y_i|\\theta)\\) then \\(\\theta=\\theta_0\\)\n\r\r\rConvergence of the posterior distribution\rDiscrete parmeter space: If the parameter space \\(\\Theta\\) is finite and \\(P(\\theta=\\theta_0)\u0026gt;0\\), then\r\\[P(\\theta=\\theta_0|y)\\to 1\\text{ as }n\\to \\infty,\\]\rwhere \\(\\theta_0=\\arg_{\\theta\\in\\Theta} KL(\\theta)\\).\nContinuous parmeter space: If \\(\\theta\\) is defined on a compace set \\(\\Theta\\) and \\(A\\) is a neighborhood of \\(\\theta_0\\) with \\(P(\\theta\\in A)\u0026gt;0\\), then\r\\[P(\\theta\\in A|y)\\to 1\\text{ as }n\\to \\infty,\\]\rwhere \\(\\theta_0=\\arg_{\\theta\\in\\Theta} KL(\\theta)\\).\nSee the proofs in Appendix B.\n\rNormal approximations to the posterior distribution\r\r\\(\\hat \\theta\\): the posterior mode\rTaylor series expansion of \\(\\log p(\\theta|y)\\) gives\r\\[\\log(\\theta|y) = \\log p(\\hat \\theta|y)-\\frac 12 (\\theta-\\hat\\theta)^\\top I(\\hat \\theta) (\\theta-\\hat\\theta) + \\cdots \\]\n\rwhere \\(I(\\theta)\\) is the observed information\r\\[I(\\theta)=-\\frac{d^2}{d\\theta^2}\\log p(\\theta|y)\\]\n\rNormal approximation: \\(p(\\theta|y)\\approx N(\\hat\\theta,[I(\\hat\\theta)]^{-1})\\)\n\rFisher information:\r\\[J(\\theta)=-E_f\\left[\\frac{d^2}{d\\theta^2}\\log p(y_j|\\theta)\\right]\\]\n\r\r\rConvergence of the posterior distribution to normality\rTheorem: Under some regularity conditions (notably that \\(\\theta\\) not be on the boundary of \\(\\Theta\\)), as \\(n\\to \\infty\\), the posterior distribution of \\(\\theta\\) approaches normality with mean \\(\\theta_0\\) and variance \\([nJ(\\theta_0)]^{-1}\\), where \\(\\theta_0=\\arg_{\\theta\\in\\Theta} KL(\\theta)\\) and \\(J\\) is the Fisher information.\nOberved that:\n\r\\(\\hat\\theta\\to \\theta_0\\) as \\(n\\to \\infty\\)\n\r\\(I(\\hat\\theta)=-\\frac{d^2}{d\\theta^2}\\log p(\\hat\\theta)-\\sum_{i=1}^n\\frac{d^2}{d\\theta^2}\\log p(y_i|\\hat\\theta)\\approx nJ(\\theta_0)\\)\n\r\\(J(\\theta_0)=\\frac{d^2}{d\\theta^2} KL(\\theta_0)\u0026gt;0\\)\n\r\r\rCounterexamples to the theorems\r\runderidentified models: \\(p(y|\\theta)\\) is equal for a range of values of \\(\\theta\\)\n\rnonindentified parameters: for example, consider the model,\r\\[\\left(\r\\begin{matrix}\ru\\\\\rv\r\\end{matrix}\r\\right)\\sim N \\left( \\left(\\begin{matrix}\r0\\\\\r0\r\\end{matrix}\r\\right),\\left(\\begin{matrix}\r1\u0026amp;\\rho\\\\\r\\rho \u0026amp; 1\r\\end{matrix}\r\\right)\\right)\\]\ronly one of \\(u,v\\) is observed from each pair \\((u,v)\\)\n\rnumber of parameters increasing with sample sizes: new latent parameters with each data point\n\r\r\rPoint estimation, consistency, and efficiency\rpoint estimations:\n\rposterior mode \\(\\hat\\theta(y)=\\arg \\max_{\\theta\\in\\Theta} p(\\theta|y)\\)\rposterior mean \\(\\hat\\theta(y)=E[\\theta|y]=\\int \\theta p(\\theta|y)d \\theta\\)\rposterior median \\(\\hat\\theta(y)=F^{-1}_{\\theta|y}(0.5)\\)\r\rconsistency: \\(\\hat\\theta(y)\\to \\theta_0\\) as \\(n\\to \\infty\\)\nasymptotic unbiasedness: \\(E[\\hat\\theta|\\theta_0]\\to\\theta_0\\) as \\(n\\to \\infty\\)\nefficiency:\r\\[\\text{eff}(\\hat\\theta)=\\frac{\\inf_T E[(T(y)-\\theta_0)^2|\\theta_0]}{E[(\\hat\\theta-\\theta_0)^2|\\theta_0]}\\le 1\\]\nasymptotically efficient: \\(\\text{eff}(\\hat\\theta)\\to 1\\) as \\(n\\to \\infty\\)\n\r","date":1538784000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538784000,"objectID":"890de9f9642233e16b5e316dd5e04b27","permalink":"/post/bayes_chap04/","publishdate":"2018-10-06T00:00:00Z","relpermalink":"/post/bayes_chap04/","section":"post","summary":"Chapter 4: Asymptotics and connections to non-Bayesian approaches","tags":["slides","BDA"],"title":"Chapter 4: Asymptotics and connections to non-Bayesian approaches","type":"post"},{"authors":null,"categories":["slides"],"content":"Nuisance parameters\r\rthere are more than one unknown or unobservable parameters\rconclusions will often be drawn about one, or only a few parameters at a time\rthere is no interest in making inferences about many of the unknown parameters – nuisance parameters\n\rsuppose \\(\\theta=(\\theta_1,\\theta_2)\\)\rinterest centers only on \\(\\theta_1\\); \\(\\theta_2\\) is a ‘nuisance’ parameter.\rthe joint posterior density:\r\\[p(\\theta_1,\\theta_2|y)\\propto p(y|\\theta_1,\\theta_2)p(\\theta_1,\\theta_2)\\]\n\rthe marginal posterior density:\r\\[p(\\theta_1|y)=\\int p(\\theta_1,\\theta_2|y)d\\theta_2=\\int p(\\theta_1|\\theta_2,y)p(\\theta_2|y)d\\theta_2\\]\n\r\r\rNormal data with a noninformative prior distribution\rLikelihood function:\r\\[p(y|\\mu,\\sigma^2)=\\prod_{i=1}^n \\frac 1{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(y_i-\\mu)^2}{2\\sigma^2}}\\]\nNoninformative prior distribution:\r\\[p(\\mu,\\sigma^2)\\propto (\\sigma^2)^{-1}\\]\nPosterior distribution:\r\\[p(\\mu,\\sigma^2|y)\\propto \\sigma^{-n-2}e^{-\\frac{\\sum_{i=1}^n(y_i-\\mu)^2}{2\\sigma^2}}=\\sigma^{-n-2}e^{-\\frac{(n-1)s^2+n(\\bar y-\\mu)^2}{2\\sigma^2}}\\]\n\r\\(s^2=\\frac 1{n-1}\\sum_{i=1}^n(y_i-\\bar y)^2\\) is the sample variance\r\r\rNormal data with a noninformative prior distribution\rConditional posterior distribution:\r\\[p(\\mu|\\sigma^2,y)\\sim N(\\bar y,\\sigma^2/n)\\]\nMarginal posterior distribution \\(p(\\sigma^2|y)\\):\r\\[p(\\sigma^2|y)\\propto \\int \\sigma^{-n-2}e^{-\\frac{(n-1)s^2+n(\\bar y-\\mu)^2}{2\\sigma^2}} d\\mu=(\\sigma^2)^{-\\frac{n+1}2}e^{-\\frac{(n-1)s^2}{2\\sigma^2}}\\]\n\\[\\sigma^2|y\\sim \\text{Inv-}\\chi^2(n-1,s^2)\\]\n\rNormal data with a noninformative prior distribution\rMarginal posterior distribution \\(p(\\mu|y)\\):\n\\[p(\\mu|y)\\propto \\int_0^\\infty \\sigma^{-n-2}e^{-\\frac{(n-1)s^2+n(\\bar y-\\mu)^2}{2\\sigma^2}} d\\sigma^2\\propto \\left[1+\\frac{n(\\mu-\\bar y)^2}{(n-1)s^2}\\right]^{-\\frac n2}\\]\n\\[\\mu|y\\sim t_{n-1}(\\bar y,s^2/n),\\ \\frac{\\mu-\\bar y}{s/\\sqrt{n}}\\Big|y\\sim t_{n-1}\\]\nPosterior predictive distribution for a future observation\n\\[\\tilde y|y \\sim t_{n-1}(\\bar y,(1+1/n)s^2)\\]\n\rExample: Estimating the speed of light\rSimon Newcomb set up an experiment in 1882 to measure the speed of light. Newcom measured the amount of time rquired for light to travel a distance of 7442 meters (66 measurements, from Stigler (1977), the data are recorded as deviations from 24800 nanoseconds).\n\r\\(n=66,\\ \\bar y = 26.2,\\ s = 10.8\\)\r\\((\\mu-26.2)/(10.8/\\sqrt{66})|y\\sim t_{65}\\)\r\\(95\\%\\) central posterior interval for \\(\\mu\\) is \\(26.2\\pm 10.8t_{65,0.975}/\\sqrt{66}=[23.6,28.8]\\)\rthe speed of light is 299792458 m/s, so the true value for \\(\\mu\\) is \\(23.8\\) nanoseconds\r\r\rExample: Estimating the speed of light\r\rNormal data with a conjugate prior distribution\rPrior distribution:\r\\[\\mu|\\sigma^2\\sim N(\\mu_0,\\sigma^2/\\kappa_0),\\]\n\\[\\sigma^2\\sim \\text{Inv-}\\chi^2(\\nu_0,\\sigma^2).\\]\n\\[p(\\mu,\\sigma^2)\\propto \\sigma^{-1}(\\sigma^2)^{-(\\nu_0/2+1)}\\exp\\left(-\\frac 1{2\\sigma^2}[\\nu_0\\sigma^2+\\kappa_0(\\mu_0-\\mu)^2]\\right)\\]\n\rdenoted by \\(\\text{N-Inv-}\\chi^2(\\mu_0,\\sigma^2_0/\\kappa_0;\\nu_0,\\sigma_0^2)\\)\r\r\rNormal data with a conjugate prior distribution\rPosterior distribution:\n\\[\\mu,\\sigma^2|y\\sim \\text{N-Inv-}\\chi^2(\\mu_n,\\sigma^2_n/\\kappa_n;\\nu_n,\\sigma_n^2)\\]\n\\[\r\\begin{cases}\r\\mu_n \u0026amp;= \\frac{\\kappa_0}{\\kappa_0+n}\\mu_0+\\frac{n}{\\kappa_0+n}\\bar y\\\\\r\\kappa_n \u0026amp;= \\kappa_0+n\\\\\r\\nu_n\u0026amp;=\\nu_0+n\\\\\r\\nu_n\\sigma_n^2 \u0026amp;= \\nu_0\\sigma_0^2+(n-1)s^2+\\frac{\\kappa_0n}{\\kappa_0+n}(\\bar y-\\mu_0)^2\r\\end{cases}\r\\]\n\r\\(\\mu|\\sigma^2,y\\sim N(\\mu_n,\\sigma^2/\\kappa_n)\\)\r\\(\\sigma^2|y\\sim \\text{Inv-}\\chi^2(\\nu_n,\\sigma_n^2)\\)\r\\(\\mu|y\\sim t_{\\nu_n}(\\mu_n,\\sigma_n^2/\\kappa_n)\\)\r\r\rMultinormal model for categorical data\rThe multinomial sampling distribution is used to describe data for which each observation is one of \\(k\\) possible outcomes. If \\(y\\) is the vector of counts of the number of observations of each outcome, then\r\\[p(y|\\theta)\\propto \\prod_{j=1}^k\\theta_j^{y_j},\\]\rwhere \\(\\sum_{j=1}^k\\theta_j=1\\).\nConjugate prior:\r\\[p(\\theta|\\alpha)\\propto \\prod_{j=1}^k\\theta_j^{\\alpha_j-1}\\]\n\rDirichlet distribution\r\rPosterior distribution:\r\\[p(\\alpha|\\theta)\\propto \\prod_{j=1}^k\\theta_j^{y_j+\\alpha_j-1}\\]\n\rMultivariate normal model with known variance\rLikelihood function:\r\\[p(y_1,\\dots,y_n|\\mu,\\Sigma)\\propto |\\Sigma|^{-n/2}\\exp\\left(-\\frac 12\\sum_{i=1}^n(y_i-\\mu)^\\top\\Sigma^{-1}(y_i-\\mu)\\right)\\]\nConjuate prior: \\(\\mu\\sim N(\\mu_0,\\Lambda_0)\\)\nPosterior distribution: \\(\\mu|y\\sim N(\\mu_n,\\Lambda_n)\\)\n\r\\(\\mu_n=(\\Lambda_n^{-1}+n\\Sigma^{-1})^{-1}(\\Lambda_0^{-1}\\mu_0+n\\Sigma^{-1}\\bar y)\\)\r\\(\\Lambda_n^{-1} = \\Lambda_n^{-1}+n\\Sigma^{-1}\\)\r\r\rMultivariate normal model with unknown mean and variance\rPrior distribution: the normal-inverse-Wishart \\((\\mu_0,\\Lambda_0/\\kappa_0;\\nu_0,\\Lambda_0)\\)\n\\[\\Sigma\\sim \\text{Inv-Wishart}_{\\nu_0}(\\Lambda_0^{-1})\\]\r\\[\\mu|\\Sigma\\sim N(\\mu_0,\\Sigma/\\kappa_0)\\]\r\\[p(\\mu,\\Sigma)\\propto |\\Sigma|^{-\\frac{\\nu_0+d}{2}-1}\\exp\\left(-\\frac{1}{2}tr(\\Lambda_0\\Sigma^{-1})-\\frac {\\kappa_0}2(\\mu-\\mu_0)^\\top\\Sigma^{-1}(\\mu-\\mu_0)\\right)\\]\nPosterior distribution: the normal-inverse-Wishart \\((\\mu_n,\\Lambda_n/\\kappa_n;\\nu_0,\\Lambda_n)\\)\n\\[\r\\begin{cases}\r\\mu_n \u0026amp;= \\frac{\\kappa_0}{\\kappa_0+n}\\mu_0+\\frac{n}{\\kappa_0+n}\\bar y\\\\\r\\kappa_n \u0026amp;= \\kappa_0+n\\\\\r\\nu_n\u0026amp;=\\nu_0+n\\\\\r\\Lambda_n \u0026amp;= \\Lambda_0+\\sum_{i=1}^n(y_i-\\bar y)(y_i-\\bar y)^\\top+\\frac{\\kappa_0n}{\\kappa_0+n}(\\bar y-\\mu_0)(\\bar y-\\mu_0)^\\top\r\\end{cases}\r\\]\n\r","date":1538697600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538697600,"objectID":"228c7989f89f2450a86ee0a2102adaa5","permalink":"/post/bayes_chap03/","publishdate":"2018-10-05T00:00:00Z","relpermalink":"/post/bayes_chap03/","section":"post","summary":"Chapter 3: Introduction to multiparameter models","tags":["slides","BDA"],"title":"Chapter 3: Introduction to multiparameter models","type":"post"},{"authors":null,"categories":["作业"],"content":"（课本p.59, 第2题）设\\(X\\)的分布密度函数为\r\\[f(x)=\\frac{1}{2\\sigma} e^{-|x|/\\sigma}\\ (\\sigma\u0026gt;0),\\]\n\\(X_1,\\dots,X_n\\)是\\(X\\)的样本，求\\(\\sigma\\)的最大似然估计。\n解: 似然函数为\n\\[L(\\sigma)=\\prod_{i=1}^n f(x_i)=\\prod_{i=1}^n \\left(\\frac{1}{2\\sigma} e^{-|x_i|/\\sigma}\\right)=(2\\sigma)^{-n}e^{-\\sum_{i=1}^n|x_i|/\\sigma}\\]\n对数似然函数为：\r\\[\\ln L(\\sigma) = -n \\ln (2\\sigma)-\\left(\\sum_{i=1}^n|x_i|\\right)/\\sigma.\\]\n对数似然方程为：\r\\[\\frac{d \\ln L(\\sigma)}{d\\sigma}=-\\frac{n}{\\sigma}+\\frac{\\sum_{i=1}^n|x_i|}{\\sigma^2}=0.\\]\n其根是\\(\\sigma^* = \\frac{\\sum_{i=1}^n|x_i|}{n}\\). 又\r\\[\\frac{d^2 \\ln L(\\sigma)}{d\\sigma^2}\\Bigg|_{\\sigma=\\sigma^*}=\\frac{n}{\\sigma^{*2}}-\\frac{2\\sum_{i=1}^n|x_i|}{\\sigma^{*3}}=-\\frac{n}{\\sigma^{*2}}\u0026lt;0.\\]\n所以，\\(\\ln L(\\sigma)\\)在\\(\\sigma=\\sigma^*\\)处取得最大值，故\\(\\sigma\\)的最大似然估计为\\(\\hat{\\mu}= \\frac{\\sum_{i=1}^n|X_i|}{n}\\).\n\r注意：最终的估计量要用大写字母\\(X_i\\)表示，这样才是估计量。用小写字母\\(x_i\\)表示的是估计值，是具体的数值，而不是估计量。\n\r（课本p.59, 第3题）设\\(X_1,\\dots,X_n\\)是来自\\([\\theta,\\theta+1]\\)上均匀分布的样本，其中\\(\\theta\\in\\mathbb{R}\\), 证明\\(\\theta\\)的最大似然估计不止一个，并求出所有的最大似然估计。\n证明：似然函数为\n\\[L(\\theta)=\\prod_{i=1}^n f(x_i)=\\prod_{i=1}^n 1\\{\\theta\\le x_i\\le \\theta+1\\}=1\\{x_{(n)}-1\\le\\theta\\le x_{(1)}\\}\\]\n观察得知，当\\(\\theta\\in [x_{(n)}-1,x_{(1)}]\\)时，似然函数取得最大值\\(1\\). 所以，\\(\\theta\\)的最大似然估计不止一个，所有的最大似然估计为集合\\([X_{(n)}-1,X_{(1)}]\\).\n（课本p.59, 第4题）设随机变量\\(X\\)以均等机会按\\(N(0,1)\\)分布取值和按\\(N(\\mu,\\sigma^2)\\)分布取值，其中\\(\\mu\\in \\mathbb{R},\\sigma^2\u0026gt;0\\). 这时\\(X\\)的分布密度函数为这两个分布的密度的平均，即\r\\[f(x;\\mu,\\sigma^2) = \\frac 12\\frac{1}{\\sqrt{2\\pi}}e^{-x^2/2}+\\frac 12\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-(x-\\mu)^2/(2\\sigma^2)},\\]\n设\\(X_1,\\dots,X_n\\)为此混合分布的简单随机样本，证明\\(\\mu,\\sigma^2\\)不存在最大似然估计。能否通过矩法估计\\(\\mu,\\sigma^2\\)？\n证明：似然函数为\r\\[L(\\mu,\\sigma^2)=\\prod_{i=1}^n \\left(\\frac 12\\frac{1}{\\sqrt{2\\pi}}e^{-x_i^2/2}+\\frac 12\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-(x_i-\\mu)^2/(2\\sigma^2)}\\right)\\]\r\r取\\(\\mu=x_1\\), 则有\r\\[L(x_1,\\sigma^2)\\ge \\frac{1}{2\\sqrt{2\\pi}\\sigma}\\prod_{i=2}^n\\left(\\frac 12\\frac{1}{\\sqrt{2\\pi}}e^{-x_i^2/2}\\right)\\]\n因为\\(\\sigma\\to 0\\)时，上式右端趋于无穷，所以似然函数\\(L(\\mu,\\sigma^2)\\)在\\(\\mathbb{R}\\times[0,\\infty)\\)上无界，故最大似然估计不存在。\n解：\r\r\\[E[X]=\\int_{-\\infty}^\\infty xf(x;\\mu,\\sigma^2) dx= \\frac \\mu2\\]\n\\[E[X^2] = \\int_{-\\infty}^\\infty x^2f(x;\\mu,\\sigma^2)dx = \\frac{1+\\sigma^2+\\mu^2}{2} \\]\n所以，\r\\[\\begin{cases}\r\\mu \u0026amp;= 2E[X]\\\\\r\\sigma^2 \u0026amp;= 2E[X^2]-4(E[X])^2-1 = 2Var[X]-2(E[X])^2-1\r\\end{cases}\r\\]\n矩估计为：\r\\[\\begin{cases}\r\\hat{\\mu} \u0026amp;= 2\\bar X\\\\\r\\hat{\\sigma^2} \u0026amp;= 2S_n^2-2\\bar X^2-1\r\\end{cases}\r\\text{ 或者 }\\begin{cases}\r\\hat{\\mu} \u0026amp;= \\frac{2}{n}\\sum_{i=1}^n X_i\\\\\r\\hat{\\sigma^2} \u0026amp;= \\frac 2n\\sum_{i=1}^n X_i^2-\\frac{4}{n^2}(\\sum_{i=1}^nX_i)^2-1\r\\end{cases}\r\\]\n（附加题I，选做）考虑上题的模型。设\\(Y\\)为一随机变量，\\(Y=1\\)表示\\(X\\)来自\\(N(0,1)\\)分布，\\(Y=0\\)表示\\(X\\)来自\\(N(\\mu,\\sigma^2)\\)分布，即\\(Y\\sim b(1,0.5)\\). 假设我们可以观测\\(Y_i\\)的值，基于样本\\((X_i,Y_i),i=1,\\dots,n\\)，是否可以求出\\(\\mu,\\sigma^2\\)的最大似然估计？事实上，\\(Y_i\\)的值不可观测（通常称为潜变量），此时你有没有更好的办法估计\\(\\mu,\\sigma^2\\)？\n解：当\\(Y_i\\)可观测时，似然函数为\r\\[L(\\mu,\\sigma^2)=\\prod_{i=1}^n \\left(\\frac{y_i}{\\sqrt{2\\pi}}e^{-x_i^2/2}+\\frac{1-y_i}{\\sqrt{2\\pi}\\sigma}e^{-(x_i-\\mu)^2/(2\\sigma^2)}\\right)\\]\n令\\(I = \\{i=1,\\dots,n|y_i=0\\}\\), 则\r\\[L(\\mu,\\sigma^2)=\\prod_{i\\notin I}\\frac{1}{\\sqrt{2\\pi}}e^{-x_i^2/2}\\prod_{i\\in I}\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-(x_i-\\mu)^2/(2\\sigma^2)}\\]\r因为\\(\\prod_{i\\notin I}\\frac{1}{\\sqrt{2\\pi}}e^{-x_i^2/2}\\)与参数\\(\\mu,\\sigma^2\\)无关, 则只需求出\r\\[\\tilde{L}(\\mu,\\sigma^2):=\\prod_{i\\in I}\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-(x_i-\\mu)^2/(2\\sigma^2)}\\]\r的最大值点即可。这就等价于求样本为\\(\\{X_i,i\\in I\\}\\)时，正态总体的最大似然估计，所以最大似然估计为\n\\[\\hat{\\mu}=\\frac{1}{|I|}\\sum_{i\\in I} X_i,\\ \\hat{\\sigma^2} = \\frac{1}{|I|} \\sum_{i\\in I}(X_i-\\hat{\\mu})^2 \\]\r考虑到\\(I\\)中元素的个数\\(|I|\\)可能为0。当\\(I= \\varnothing\\)时，似然函数不含未知参数，此时估计量可以为任意常数。故最终的估计量可以写成\n\\[\\hat{\\mu}=\r\\begin{cases}\r\\frac{1}{n-\\sum_{i=1}^n Y_i}\\sum_{i=1}^n X_i(1-Y_i), \u0026amp; \\sum_{i=1}^n Y_i\u0026lt;n\\\\\rc_1, \u0026amp;\\sum_{i=1}^n Y_i=n\r\\end{cases}\r\\]\n\\[\r\\hat{\\sigma^2} = \\begin{cases}\r\\frac{1}{n-\\sum_{i=1}^n Y_i} \\sum_{i=1}^n(X_i-\\hat{\\mu})^2(1-Y_i), \u0026amp; \\sum_{i=1}^n Y_i\u0026lt;n\\\\\rc_2, \u0026amp;\\sum_{i=1}^n Y_i=n\r\\end{cases}\r\\]\n其中\\(c_1\\in \\mathbb{R}\\), \\(c_2\u0026gt;0\\)为常数。\n当\\(Y_i\\)不可观测时，我们可以利用EM算法求出最大似然估计值。参考：\nAndrew Ng’s lecture notes 1\nAndrew Ng’s lecture notes 2\n（附加题II，选做）若考虑更一般的混合分布：\n\\[f(x;\\lambda,\\mu_1,\\sigma_1^2,\\mu_2,\\sigma_2^2)=\\frac{\\lambda}{\\sqrt{2\\pi}\\sigma_1}e^{-(x-\\mu_1)^2/(2\\sigma_1^2)}+\\frac{1-\\lambda}{\\sqrt{2\\pi}\\sigma_2}e^{-(x-\\mu_2)^2/(2\\sigma_2^2)}\\]\r其中\\(\\lambda\\in[0,1],\\mu_1,\\mu_2\\in \\mathbb{R},\\sigma_1^2,\\sigma_2^2\u0026gt;0\\), 你能求出未知参数\\(\\lambda,\\mu_1,\\sigma_1^2,\\mu_2,\\sigma_2^2\\)的矩估计吗？\n解：参考文献: Estimation in Mixtures of Two Normal Distributions\n（课本p.59, 第9题）设\\(X_1,\\dots,X_n\\)是来自分布密度为\r\\[f(x;\\theta)=\\frac{\\Gamma(\\theta+1)}{\\Gamma(\\theta)\\Gamma(1)}x^{\\theta-1}1\\{0\\le x\\le 1\\}\\]\n的总体的样本，其中\\(\\theta\u0026gt;0\\), 试用矩法估计\\(\\theta\\).\n解：\n\\[E[X]=\\int_0^1 x\\theta x^{\\theta-1}d x=\\frac{\\theta}{\\theta+1}\\]\n\\[\\theta = \\frac{E[X]}{1-E[X]}\\]\n所以，矩估计为\r\\[\\hat\\theta =\\frac{\\bar X}{1-\\bar X}=\\frac{\\sum_{i=1}X_i}{n-\\sum_{i=1}^nX_i}\\]\n（课本p.60, 第10题）设\\(X_1,\\dots,X_n\\)是来自分布密度为\r\\[f(x;c,\\theta)=\\frac{1}{2\\theta}1\\{c-\\theta\\le x\\le c+\\theta\\}\\]\n的总体的样本，其中\\(\\theta\u0026gt;0,c\\in\\mathbb{R}\\), 试用矩法估计\\(c,\\theta\\).\n解：已知总体\\(X\\sim U[c-\\theta,c+\\theta]\\), 所以\n\\[E[X]=\\frac{(c+\\theta)+(c-\\theta)}{2}=c\\]\n\\[Var[X] = \\frac{(2\\theta)^2}{12}=\\frac{\\theta^2}{3}\\]\n所以矩估计为：\r\\[\\hat{c}=\\bar X=\\frac 1n\\sum_{i=1}^nX_i,\\ \\hat{\\theta} = \\sqrt{3S_n^2}=\\sqrt{\\frac{3}{n}\\sum_{i=1}^n(X_i-\\bar X)^2}.\\]\n","date":1537833600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537833600,"objectID":"01e111844b236611ca21b8ef35fb00ee","permalink":"/post/homework3/","publishdate":"2018-09-25T00:00:00Z","relpermalink":"/post/homework3/","section":"post","summary":"答案已上传！","tags":["作业","数理统计"],"title":"第三次作业","type":"post"},{"authors":null,"categories":["课件"],"content":"目录\r1. 点估计\r\r1.1 矩估计法\r1.2 极大似然估计法\r1.3 估计的优良性准则\r\r\r2. 区间估计\r\r2.1 单个正态总体的区间估计\r2.2 两个独立正态总体的区间估计\r2.3 非正态总体的区间估计\r\r\r3. 分布估计\r\r3.1 直方图法\r3.2 核估计法\r\r\r\r参数估计\r在实际问题中，对于一个总体\\(X\\)往往是仅知其分布的类型\\(f(x, \\theta)\\)，而参数\\(\\theta=(\\theta_1,\\dots,\\theta_m)\\in \\Theta \\subset \\mathbb{R}^m\\)是未知的。对任给的实值函数\\[g:\\ \\mathbb{R}^m\\to \\mathbb{R},\\]\r如何根据\\(X\\)的样本\\(x_1,\\dots,x_n\\)估计\\(g( \\theta)\\)的值呢？这就是统计推断中的“参数估计”问题。\n点估计：寻找一个统计量\\(\\hat{ \\theta} = T(X_1,\\dots,X_n)\\)作为$ $的点估计\n区间估计：寻找两个统计量\\(\\hat{ \\theta}_1 = T_1(X_1,\\dots,X_n)\\), \\(\\hat{ \\theta}_2 = T_2(X_1,\\dots,X_n)\\)，所构成的区间\\([\\hat{ \\theta}_1,\\hat{ \\theta}_2]\\)作为$ $的区间估计\n\r1.1 矩估计法\r矩估计的想法来源于大数定理。如果总体\\(X\\)存在\\(k\\)阶矩，对任意\\(\\epsilon\u0026gt;0\\),\r\\[\r\\lim_{n\\to \\infty} P(|\\frac 1 n\\sum_{i=1}^n X_i^k-E[X^k]|\\ge \\epsilon )=0.\r\\]\n这说明，当样本容量\\(n\\)较大时，样本\\(k\\)阶矩与总体\\(k\\)阶矩差别很小。矩法估计就是用样本\\(k\\)阶矩代替总体的\\(k\\)阶矩。通常用\\(\\hat{\\theta}_M\\)表示。一般步骤如下：\n\r列出估计式\\(E[X^k]=g_k(\\theta_1,\\dots,\\theta_m),\\ k=1,\\dots,m.\\)\n\r求解关于估计量的方程组\\(\\theta_k = \\theta_k(E[X^1],\\dots,E[X^m])\\)\n\r用\\(M_k=\\frac 1 n\\sum_{i=1}^n X_i^k\\)替代\\(E[X^k]\\)得到矩估计\\(\\hat\\theta_k = \\theta_k(M_1,\\dots,M_m)\\)\n\r\r\r例1\r例：求总体\\(X\\)的期望\\(\\mu=E[X]\\)与方差\\(\\sigma^2=Var[X]\\)的矩估计。\n解: (1)列出估计式\n\\[\r\\begin{cases}\rE[X] \u0026amp;= \\mu\\\\\rE[X^2] \u0026amp;= \\mu^2+\\sigma^2\r\\end{cases}\r\\]\n(2)求解关于估计量的方程组\r\\[\r\\begin{cases}\r\\mu \u0026amp;= E[X]\\\\\r\\sigma^2 \u0026amp;= E[X^2]-(E[X])^2\r\\end{cases}\r\\]\n所以，\\(\\hat{\\mu}_M = \\bar X\\), \\(\\hat{\\sigma}^2_M = \\frac{1}{n}\\sum_{i=1}^n X_i^2-(\\bar X)^2 = S_n^2.\\)\n注：不难证明，总体的各阶中心矩的矩估计就是样本各阶中心矩。\n\r例2\r例：设总体\\(X\\sim U[a,b]\\), 求\\(a,b\\)的矩估计。\n解: 易知，\\(E[X]=(a+b)/2,\\ Var[X]= (b-a)^2/12\\).\n所以，\r\\[\r\\begin{cases}\ra \u0026amp;= E[X]-\\sqrt{3Var[X]}\\\\\rb \u0026amp;= E[X]+\\sqrt{3Var[X]}\r\\end{cases}\r\\]\n\\[\r\\begin{cases}\r\\hat a_M \u0026amp;= \\bar{X}-\\sqrt{3}S_n\\\\\r\\hat b_M \u0026amp;= \\bar{X}+\\sqrt{3}S_n\r\\end{cases}\r\\]\n\r例3\r例：设总体\\(X\\)的分布密度为\r\\[\rf(x)=\\frac{\\theta}{2}e^{-\\theta|x|},\\ x\\in\\mathbb{R}, \\theta\u0026gt;0.\r\\]\r求\\(\\theta\\)的矩估计。\n解:\r\\[\rE[X]= 0,\\ E[X^2]=\\int_{-\\infty}^{\\infty}x^2\\frac{\\theta}{2}e^{-\\theta|x|}d x=\\theta\\int_{0}^{\\infty}x^2e^{-\\theta x}d x=\\frac{2}{\\theta^2}\r\\]\n\\[\\hat{\\theta}_M=\\sqrt{\\frac{2n}{\\sum_{i=1}^n X_i^2}}.\\]\n除外，还可以由\\(E[|X|]=1/\\theta\\)得到另一种矩估计。\n\r1.2 最大似然估计法\r最大似然估计法最早由高斯(C.F.Gauss)提出，后来被 Fisher完善。最大似然估计这一名称也是Fisher给的。这是一个目前仍得到广泛应用的方法。它是建立在最大似然原理基础上的一个统计方法。\n\r最大似然原理：最先出现的是概率最大的\n\r例：设有外形完全相同的两个箱子，甲箱中有99个白球和1个黑球，乙箱中有99个黑球和1个白球，今随机地抽取一箱并从中随机抽取一球，结果取得白球，问这球是从哪个箱子中取出？\n\r最大似然估计法\r似然函数：\r\\[L(x_1,\\dots,x_n;\\theta)=L(\\theta)=\\prod_{i=1}^{n}f(x_i;\\theta)\r\\]\r给定样本观测值\\((x_1,\\dots,x_n)\\), 记\\(L(x_1,\\dots,x_n;\\theta)\\)的最大值点为\\(\\theta=T(x_1,\\dots,x_n)\\). 则\\(\\theta\\)的最大似然估计量(MLE, maximum likelihood estimator)为\r\\[\\hat{\\theta}_L=T(X_1,\\dots,X_n).\\]\n\r最大似然估计的一般步骤\r第一步：写出似然函数\\(L(x_1,\\dots,x_n;\\theta)\\)\n第二步：若似然函数\\(L\\)是\\(\\theta\\)的可微函数，则最大值必然满足似然方程\r\\[\\frac{d L}{d \\theta}=0\\]\r解出\\(\\theta\\), 并验证其是否是极大值：\\[\\frac{d^2 L}{d \\theta^2}\u0026lt;0.\\]\n注1：为方便求导，一般求对数似然函数\\(\\ln L(x_1,\\dots,x_n;\\theta)\\)求极大值点\n注2：若有多个参数\\(\\theta_1,\\dots,\\theta_m\\)，对每个变量求偏导，联立\\(m\\)个方程求解\n\r例1：0-1离散型\r例：设总体\\(X\\sim B(1,p)\\), 从中抽取样本\\(X_1,\\dots,X_n\\)的观测值为\\(x_1,\\dots,x_n\\). 求参数\\(p\\)的最大似然估计。\n解: 似然函数为\r\\[ L(x_1,\\dots,x_n;p)=\\prod_{i=1}^{n}p^{x_i}(1-p)^{1-x_i}=p^{\\sum_{i=1}^nx_i}(1-p)^{n-\\sum_{i=1}^nx_i}\\]\n令\\(y=\\sum_{i=1}^nx_i\\), 对数似然函数为：\r\\[\\ln L = y \\ln p + (n-y)\\ln (1-p).\\]\r对数似然方程为：\r\\(\\frac{d \\ln L}{d p} = y/p - (n-y)/(1-p)=0.\\)\r解得\\(p= y/n=\\frac{1}{n}\\sum_{i=1}^nx_i\\). 因为\\(\\frac{d^2\\ln L}{d p^2}\u0026lt;0\\), 所以\\(p= y/n\\)是极大值。\\(\\hat{p}_L = \\bar X.\\)\n\r例2：正态分布\r例：设总体\\(X\\sim N(\\mu,\\sigma^2)\\), 从中抽取样本\\(X_1,\\dots,X_n\\)的观测值为\\(x_1,\\dots,x_n\\). 求参数\\(\\mu,\\sigma^2\\)的最大似然估计。\n解: 似然函数为\n\\[ L(x_1,\\dots,x_n;\\mu,\\sigma^2)=\\prod_{i=1}^{n}f(x_i)=\\prod_{i=1}^{n}\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-(x_i-\\mu)^2/(2\\sigma^2)}\\]\r令\\(\\theta_1=\\mu,\\theta_2=\\sigma^2\\), 对数似然函数为：\r\\[\\ln L = (n/2)\\ln (2\\pi)-(n/2)\\ln\\theta_2-\\frac{\\sum_{i=1}^n(x_i-\\theta_1)^2}{2\\theta_2}\\]\n\r例2：正态分布\r对数似然方程组为：\r\\[\r\\begin{cases}\r\\frac{\\partial \\ln L}{\\partial \\theta_1} \u0026amp;=\\frac{\\sum_{i=1}^n(x_i-\\theta_1)}{\\theta_2}=0\\\\\r\\frac{\\partial \\ln L}{\\partial \\theta_2} \u0026amp;=-\\frac{n}{2\\theta_2}+\\frac{\\sum_{i=1}^n(x_i-\\theta_1)^2}{2\\theta_2^2}=0\r\\end{cases}\r\\]\r解得\\(\\hat{\\mu}_L=\\bar X,\\ \\hat{\\sigma}^2_L = S_n^2\\). (可以验证二阶导函数非正定，即取得极大值。)\n\r例3\r例：设总体\\(X\\sim U[a,b]\\), 从中抽取样本\\(X_1,\\dots,X_n\\)的观测值为\\(x_1,\\dots,x_n\\). 求参数\\(a,b\\)的最大似然估计。\n解: 似然函数为\r\\[ L(x_1,\\dots,x_n;a,b)=\\frac{1}{(b-a)^n}\\prod_{i=1}^{n} 1\\{a\\le x_i\\le b\\}\\]\r注意到\\(L\\)关于\\(a,b\\)不可微。容易观察到，当\\(a=\\min_{i=1,\\dots,n}\\{x_i\\},\\ b=\\max_{i=1,\\dots,n}\\{x_i\\}\\)时\\(L\\)取得最大值。故\\[\\hat{a}_L = X_{(1)},\\ \\hat{b}_L = X_{(n)}.\\]\n\r关于最大似然估计的一些说明\r最大似然估计的不变性：如果\\(\\hat{\\theta}\\)是\\(\\theta\\)的最大似然估计，则对任一函数\\(g(\\theta)\\), 其最大似然估计为\\(g(\\hat{\\theta})\\).\n当分布中有多余的参数或者数据为截尾或缺失时，似然函数的求极大值比较困难。针对这种问题，文献\nDempster, A.P.; Laird, N.M.; Rubin, D.B. (1977). Maximum Likelihood from Incomplete Data via the EM Algorithm. Journal of the Royal Statistical Society, Series B. 39 (1): 1–38. (cited by 54539, 2018/8/18)\n提出了一种有效的Expectation–Maximization (EM)算法。\n\r矩估计与最大似然估计的对比\r矩估计法（也称数字特征法）\r\r直观意义比较明显，但要求总体\\(k\\)阶矩存在。\r缺点是不唯一，此时尽量使用样本低阶矩。\r观测值受异常值影响较大，不够稳健，实际中避免使用样本高阶矩。\r\r\r极大似然估计法\r\r具有一些理论上的优点（不变性、渐近正态性）\r缺点是如果似然函数不可微，没有一般的求解法则。\r\r\r\r矩估计与最大似然估计的对比\r\r1.3 估计的优良性标准\r\r估计量的无偏性\n\r估计量的有效性\n\r估计量的大样本性质：相合性与渐近正态性\n\r\r\r无偏性\r定义：设总体\\(X\\sim F(x;\\theta),\\theta\\in \\Theta\\), \\(T(X_1,\\dots,X_n)\\)为\\(g(\\theta)\\)的估计量。\n\r无偏估计量：\r\r\\[E[T(X_1,\\dots,X_n)]=g(\\theta), \\forall \\theta\\in \\Theta\\]\n\r渐近无偏估计量：\r\r\\[\\lim_{n\\to \\infty}E[T(X_1,\\dots,X_n)]=g(\\theta), \\forall \\theta\\in \\Theta\\]\n无偏性意味着：虽然估计量\\(T\\)由于随机可能偏离真值\\(g(\\theta)\\), 但取其平均值（期望）却等于\\(g(\\theta)\\). 即没有系统偏差。\n\r例\r\r样本均值是总体的均值的无偏估计，即\\(E[\\bar X]=E[X]\\)\n\r样本方差是总体方差的渐近无偏估计，即\\(\\lim_{n\\to \\infty}E[S_n^{2}]=Var[X]\\)\n\r修正样本方差是总体方差的无偏估计，即\\(E[S_n^{*2}]=Var[X]\\)\n\r\r\r均方误差\r定义：设\\(T(X_1,\\dots,X_n)\\)为\\(g(\\theta)\\)的估计量，其均方误差 (mean squared error, MSE)为\r\\[M_{\\theta}(T):=E_\\theta[(T(X_1,\\dots,X_n)-g(\\theta))^2].\\]\r均方根误差 (root mean squared error, RMSE)为\r\\[R_{\\theta}(T):=\\sqrt{E_\\theta[(T(X_1,\\dots,X_n)-g(\\theta))^2]}.\\]\n注意到：\n\\[M_{\\theta}(T)=(E[T]-g(\\theta))^2+Var(T)=偏差^2+方差\\]\n注：如果\\(T\\)是\\(g(\\theta)\\)的无偏估计，则\\(M_{\\theta}(T)=Var(T)\\)\n\r比较两个估计量的优劣\r定义：若\\(T_1(X_1,\\dots,X_n)\\)和\\(T_2(X_1,\\dots,X_n)\\)都为\\(g(\\theta)\\)的估计量，\n\r如果\\(M_{\\theta}(T_1)\\le M_{\\theta}(T_2),\\forall \\theta\\in \\Theta\\), 则称\\(T_1\\)不次于\\(T_2\\)。\r在此基础上，如果存在一个\\(\\theta_0\\in\\Theta\\)使得\\(M_{\\theta_0}(T_1)\u0026lt; M_{\\theta_0}(T_2)\\), 则称\\(T_1\\)比\\(T_2\\)有效。\r\r例：设总体\\(X\\)的期望\\(\\mu\\)方差为\\(\\sigma^2\\), \\(X_1,\\dots,X_n\\)为其样本(\\(n\u0026gt;1\\))，证明下列估计量\\(\\hat{\\mu} = \\sum_{i=1} C_iX_i\\)为\\(\\mu\\)的无偏估计的充要条件是\\(\\sum_{i=1}^nC_i = 1.\\) 在满足该条件前提下，\\(C_i\\)取何值时，\\(\\hat{\\mu}\\)的最有效。\n解：\\(E[\\hat{\\mu}]=\\mu\\Leftrightarrow \\sum_{i=1}^nC_i = 1\\)\r\\[Var[\\hat{\\mu}]=\\sigma^2\\sum_{i=1}^nC_i^2\\ge \\sigma^2\\frac{(C_1+\\dots+C_n)^2}{n}=\\frac{\\sigma^2}{n}.\\]\n而且唯一的最小值在\\(C_i=1/n,i=1,\\dots,n\\)处取得。\n\r一致最小方差无偏估计\r定义：如果\\(T_0(X_1,\\dots,X_n)\\)为\\(g(\\theta)\\)的无偏估计，如果对于\\(g(\\theta)\\)的任意无偏估计量\\(T(X_1,\\dots,X_n)\\)都有\r\\[Var[T_0]\\le Var[T],\\ \\forall\\theta\\in\\Theta\\]\r则称\\(T_0\\)为\\(g(\\theta)\\)的一致最小方差无偏估计量 (uniformly minimum-variance unbiased estimator, UMVUE)。\n思考\r\r无偏估计量是不是一定存在？（反例见课本例2.5, p27）\r如果存在多个无偏估计量，如何找到UMVUE？（Blackwell, Rao, Lehmann, Scheffe等统计学家获得了一系列寻求UMVUE的理论和方法）\r\r\r\rBlack-Lehmann-Scheffe定理\r定义：设\\(T(X_1,\\dots,X_n)\\)为统计量。如果对任何(Borel可测)函数\\(u(\\cdot)\\), 只要\\(E[u(T)]=0\\)(对一切\\(\\theta\\))就可以推出\\(P(u(T)=0)=1\\), 这次称统计量\\(T\\)为完全的统计量。\nB-L-S定理：如果\\(T(X_1,\\dots,X_n)\\)为完全的充分统计量，\\(\\psi(T)\\)为\\(g(\\theta)\\)的无偏估计，则\\(\\psi(T)\\)为\\(g(\\theta)\\)的最小方差无偏估计。\n\r说明：可以证明，如果参数\\(\\theta\\)的集合有内点，则指数型分布族的充分统计量\r\\(\\left(\\sum_{i=1}^nT_1(x_i),\\dots,\\sum_{i=1}^nT_k(x_i)\\right)\\)\r是完全的。\n\r\rCramer-Rao不等式\r定理：设\\(X\\)的密度为\\(f(x;\\theta)\\), 参数\\(\\theta\\in (a,b)\\). \\(X_1,\\dots,X_n\\)为\\(X\\)的样本，\\(\\psi(X_1,\\dots,X_n)\\)是\\(g(\\theta)\\)的一个无偏估计，且满足下列正则性条件：\n\r\\(X\\)的支撑与\\(\\theta\\)无关；\r\\(g\u0026#39;(\\theta)\\)和\\(\\frac{df(x;\\theta)}{d\\theta}\\)都存在且对一切\\(\\theta\\)有\r\r\\[\\int_{-\\infty}^\\infty \\frac{df(x;\\theta)}{d\\theta} d x = 0\\]\n\\[\\int_{-\\infty}^\\infty\\frac d{d\\theta} L(\\vec x;\\theta) d \\vec x=0\\]\n\\[\\frac d{d\\theta}\\int_{-\\infty}^\\infty \\psi(\\vec x) L(\\vec x;\\theta) d \\vec x=\\int_{-\\infty}^\\infty \\psi(\\vec x) \\frac d{d\\theta}L(\\vec x;\\theta) d \\vec x\\]\n\rCramer-Rao不等式\r\r\\(I(\\theta):=E[(\\frac {d\\ln f(X;\\theta)}{d\\theta})^2]\u0026gt;0\\)\r\r则有\r\\[Var_\\theta[\\psi(X_1,\\dots,X_n)]\\ge \\frac{[g\u0026#39;(\\theta)]^2}{nI(\\theta)}.\\]\n说明:\n\r证明见p. 28\r\\(I(\\theta)\\)叫做Fisher信息量\r离散情形有类似的结论\rC-R不等式的下界不一定达到，见例2.9, p30.\r\r\r例\r设\\(X\\sim N(\\mu,\\sigma^2)\\), 其中\\(\\mu\\)未知，\\(\\sigma\\)已知。\rFisher信息量为\r\\[I(\\mu) = E[(\\frac {d\\ln f(X;\\mu)}{d\\mu})^2]=\\frac 1{\\sigma^4}E[(X-\\mu)^2]=\\frac 1{\\sigma^2}\\]\n\\[Var[\\bar X] = \\frac{\\sigma^2}{n}=\\frac{1}{nI(\\mu)}\\]\n可以证明：若\\(\\mu\\)已知，则\\(\\sigma^2\\)的估计量\\(\\frac 1n\\sum_{i=1}^n(X_i-\\mu)^2\\)的方差达到了C-R不等式的下界。\n\r统计量的大样本性质\r统计量的相合性\n\r统计量的渐近正态性\n\r\r\r统计量的相合性\r（弱）相合估计：称\\(T_n(X_1,\\dots,X_n)\\)是\\(g(\\theta)\\)的相合估计，如果对任何\r\\(\\epsilon\u0026gt;0\\), 有\\[\\lim_{n\\to\\infty}P(|T_n-g(\\theta)|\\ge \\epsilon)=0.\\] 等价于依概率收敛\\(T_n\\stackrel p\\to g(\\theta)\\).\n强相合估计：称\\(T_n(X_1,\\dots,X_n)\\)是\\(g(\\theta)\\)的强相合估计，如果\\[P(\\lim_{n\\to\\infty}T_n=g(\\theta))=1.\\] 等价于概率1收敛\\(T_n\\stackrel {w.p.1}\\to g(\\theta)\\).\n说明\n\r由强大数定理知，矩估计一般是强估计的\r最大似然估计在十分广泛的条件下也是有强相合性（见p31)\r\r\r例\r设总体\\(X\\)的期望\\(\\mu\\)方差为\\(\\sigma^2\\), \\(X_1,\\dots,X_n\\)为其样本，证明\n\r样本均值\\(\\bar X\\)是\\(\\mu\\)的相合估计量；\r样本\\(k\\)阶原点矩\\(M_k\\)是总体\\(k\\)阶原点矩\\(E[X^k]\\)的相合估计量；\r样本方差\\(S_n^2\\)和修正样本方差\\(S_n^{2*}\\)都是\\(\\sigma^2\\)的相合估计量。\r\r证明：由辛钦大数定律知，\\(\\bar X\\stackrel p\\to \\mu\\), \\(M_k\\stackrel p\\to E[X^k]\\).\n\\[S_n^2 = \\frac{1}{n}\\sum_{i=1}^nX_i^2-\\bar X^2\\stackrel p\\to E[X^2]-E[X]^2=\\sigma^2\\]\r同理，\\(S_n^{2*}=\\frac{n-1}{n}S_n^2\\stackrel p\\to \\sigma^2\\)\n注：这里用到依概率收敛的性质：假设\\(X_n\\stackrel p\\to X,\\ Y_n\\stackrel p\\to Y\\). 则\\(X_n+Y_n\\stackrel p\\to X+Y\\). 如果\\(g\\)连续，则\\(g(X_n)\\stackrel p\\to g(X)\\), \\(g(X_n,Y_n)\\stackrel p\\to g(X,Y)\\).\n\r相合估计的充分条件\r定理：设总体\\(X\\sim F(x;\\theta),\\ \\theta\\in \\Theta\\), 统计量\\(T(X_1,\\dots,X_n)\\)为\\(g(\\theta)\\)的估计量。如果\r\\[\\lim_{n\\to \\infty}E[T(X_1,\\dots,X_n)] = g(\\theta),\\ \\lim_{n\\to \\infty}Var[T(X_1,\\dots,X_n)] =0,\\]\r则\\(T(X_1,\\dots,X_n)\\)为\\(g(\\theta)\\)的相合估计量。\n证明：令\\(T_n=T(X_1,\\dots,X_n)\\).\r注意到\\[\\{|T_n-g(\\theta)|\\ge \\epsilon\\}\\subset \\{|T_n-E[T_n]|\\ge \\epsilon/2\\}\\cup \\{|E[T_n]-g(\\theta)|\\ge \\epsilon/2\\}.\\]\r对任意\\(\\epsilon\u0026gt;0\\), 存在\\(N\\), 当\\(n\\ge N\\)时，\\(|E[T_n]-g(\\theta)|\u0026lt; \\epsilon/2\\).\r此时\\[\\{|T_n-g(\\theta)|\\ge \\epsilon \\}\\subset \\{|T_n-E[T_n]|\\ge \\epsilon/2\\}\\]\r所以，\r\\[P(|T_n-g(\\theta)|\\ge \\epsilon)\\le P(|T_n-E[T_n]|\\ge \\epsilon/2)\\le \\frac{4 VaR[T_n]}{\\epsilon^2}\\to 0.\\]\n\r统计量的渐近正态性\r定义：设\\(T(X_1,\\dots,X_n)\\)为\\(g(\\theta)\\)的估计量。如果存在一个趋于零的正数列\\(\\sigma_n(\\theta)\\), 使得\\([T-g(\\theta)]/\\sigma_n(\\theta)\\)的分布收敛到标准正态分布，则称\\(T(X_1,\\dots,X_n)\\)为\\(g(\\theta)\\)的渐近正态估计，或称\\(T\\)具备渐近正态性，即\\[T\\stackrel{\\cdot}{\\sim} N(\\theta, \\sigma_n(\\theta)^2).\\]\n\r最大似然估计的渐近正态性\r定理：设\\(X\\)的密度为\\(f(x;\\theta)\\), 其参数空间\\(\\Theta\\)是非退化区间，且满足下列正则性条件：\n\r对一切\\(\\theta\\in\\Theta\\), \\(\\frac{\\partial \\ln f}{\\partial\\theta}, \\frac{\\partial^2 \\ln f}{\\partial\\theta^2}, \\frac{\\partial^3 \\ln f}{\\partial\\theta^3}\\) 都存在\n\r对一切\\(\\theta\\in\\Theta\\), 有\\(|\\frac{\\partial \\ln f}{\\partial\\theta}|\u0026lt;F_1(x),\\ |\\frac{\\partial^2 \\ln f}{\\partial\\theta^2}|\u0026lt;F_2(x),\\ |\\frac{\\partial^3 \\ln f}{\\partial\\theta^3}|\u0026lt;H(x),\\) 其中\\(F_1(x),F_2(x)\\)在实数轴上可积，且\\(\\int_{-\\infty}^\\infty H(x)f(x;\\theta)dx\u0026lt;M\\), \\(M\\)与\\(\\theta\\)无关。\n\r对一切\\(\\theta\\in\\Theta\\), 有\\(0\u0026lt;I(\\theta)=E[(\\frac{\\partial \\ln f}{\\partial\\theta})^2]\u0026lt;\\infty\\).\n\r\r则在参数真值\\(\\theta\\)为\\(\\Theta\\)内点的情况下，其似然方程有一个解\\(\\hat{\\theta}_L\\)存在，且\\[\\hat{\\theta}_L\\stackrel{p}{\\to}\\theta,\\ \\hat{\\theta}_L\\stackrel{\\cdot}{\\sim} N(\\theta,[nI(\\theta)]^{-1}).\\]\n证明参考：陈希孺. 概率论与数理统计. 中国科技大学出版社, 1992\r\r\r2 区间估计\r\r区间估计的定义\n\r单个正态总体的区间估计\n\r两个独立正态总体的区间估计\n\r非正态总体的区间估计\n\r\r\r区间估计的定义\r定义：设总体\\(X\\sim F(x;\\theta),\\ \\theta\\in \\Theta\\). 如果统计量\\(T_1(X_1,\\dots,X_n),T_2(X_1,\\dots,X_n)\\)使得对给定的\\(\\alpha\\in(0,1)\\)有\r\\[P(T_1\\le g(\\theta)\\le T_2)=1-\\alpha,\\ \\forall \\theta\\in\\Theta,\\]\r则称随机区间\\([T_1,T_2]\\)为参数\\(g(\\theta)\\)的置信度（置信概率）为\\(1-\\alpha\\)的置信区间，\\(T_1,T_2\\)分别称为置信下界和置信上界。\n说明\r在一些情况下，定义中的“等式”无解，此时考虑的置信区间\\([T_1,T_2]\\)应满足\r\\[P(T_1\\le g(\\theta)\\le T_2)\\ge 1-\\alpha,\\ \\forall \\theta\\in\\Theta.\\]\n\r\r置信区间示意图\r\r区间估计基本步骤——枢轴量法\r目标：找到\\(g(\\theta)\\)的区间估计，置信度为\\(1-\\alpha\\).\nStep 1: 选择恰当的枢轴量\\(G(X_1,\\dots,X_n;g(\\theta))\\)\n\r\\(G\\)不含有其他未知参数\r\\(G\\)的分布确定，即不含未知参数\\(\\theta\\)\r一般地，\\(G\\)是关于参数\\(g(\\theta)\\)的单调函数\r\rStep 2: 求\\(a,b\\)使得\\(P(a\\le G\\le b)=1-\\alpha\\)\nStep 3: 转化不等式\\(a\\le G\\le b\\)为如下形式：\r\\[T_1\\le g(\\theta)\\le T_2.\\]\n\r指数分布\r目标：若总体为指数分布\\(Exp(\\lambda)\\)，求未知参数\\(\\lambda\\)的置信区间。\nStep 1: 选择枢轴量\r\\[G(X_1,\\dots,X_n;\\lambda) = 2\\lambda n\\bar X\\sim Ga(n,1/2)=\\chi^2(2n)\\]\nStep 2: 求\\(a,b\\)使得\\(P(a\\le G\\le b)=1-\\alpha\\)，即\r\\[P(a\\le 2\\lambda n\\bar X\\le b)=1-\\alpha\\]\nStep 3: \\(\\lambda\\)的置信区间为\\([a/(2n\\bar X),b/(2n\\bar X)]\\).\n如何选择\\(a,b\\)?\r\r平分法：\\(a=\\chi^2_{\\alpha/2}(2n), b=\\chi^2_{1-\\alpha/2}(2n)\\)\n\r最优方案？参考书p35\n\r\r\r\r平分法示意图\r\r2.1 单个正态总体的区间估计\r设总体\\(X\\sim N(\\mu,\\sigma^2)\\), 如何找出未知参数\\(\\mu\\)和\\(\\sigma^2\\)的置信区间？\n\r已知\\(\\sigma^2\\), 找出\\(\\mu\\)的置信区间\n\r未知\\(\\sigma^2\\), 找出\\(\\mu\\)的置信区间\n\r已知\\(\\mu\\), 找出\\(\\sigma^2\\)的置信区间\n\r未知\\(\\mu\\), 找出\\(\\sigma^2\\)的置信区间\n\r\r\r已知方差，求期望的置信区间\r由抽样定理知，\\(\\bar{X}\\sim N(\\mu,\\sigma^2/n)\\). 因此\r\\(U = \\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}\\sim N(0,1)\\)\r\\[P\\left(a\\le \\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}\\le b\\right) = 1-\\alpha\\]\r\\(\\mu\\)的置信度为\\(1-\\alpha\\)的置信区间为\\[\\left[\\bar{X}+a\\frac{\\sigma}{\\sqrt{n}},\\ \\bar{X}+b\\frac{\\sigma}{\\sqrt{n}}\\right]\\]\n最优的选择：\\(b=-a=u_{1-\\alpha/2}\\)\n\r方差未知，求期望的置信区间\r由抽样定理知，\r\\[T = \\frac{\\bar{X}-\\mu}{S_n/\\sqrt{n-1}}= \\frac{\\bar{X}-\\mu}{S_n^*/\\sqrt{n}}\\sim t(n-1)\\]\r\\[P\\left(a\\le \\frac{\\bar{X}-\\mu}{S_n^*/\\sqrt{n}}\\le b\\right) = 1-\\alpha\\]\r\\(\\mu\\)的置信度为\\(1-\\alpha\\)的置信区间为：\r\\[\\left[\\bar{X}-a\\frac{S_n^*}{\\sqrt{n}},\\ \\bar{X}+b\\frac{S_n^*}{\\sqrt{n}}\\right]=\\left[\\bar{X}-a\\frac{S_n}{\\sqrt{n-1}},\\ \\bar{X}+b\\frac{S_n}{\\sqrt{n-1}}\\right]\\]\n最优的选择：\\(b=-a=t_{1-\\alpha/2}\\)\n\r例\r例：假设OPPO手机充电五分钟通话时间\\(X\\sim N(\\mu,\\sigma^2)\\). 随机抽取6部手机测试通话时间（单位：小时）为\r\\[1.6,\\ 2.1,\\ 1.9,\\ 1.8,\\ 2.2,\\ 2.1,\\]\n\r已知\\(\\sigma^2=0.06\\), 求\\(\\sigma^2\\)的置信度为\\(95\\%\\)的置信区间。\n\r\\(\\sigma^2\\)未知, 求\\(\\sigma^2\\)的置信度为\\(95\\%\\)的置信区间。\n\r\r解：\n查表知，\\(u_{1-\\alpha/2}=u_{0.975}=1.96,\\ t_{1-\\alpha/2}=t_{0.975}=2.5706\\). 且\\(\\bar X = 1.95,\\ S_n=0.206\\)\n\\(\\left[1.95-1.96\\frac{\\sqrt{0.06}}{\\sqrt{6}},\\ 1.95+1.96\\frac{\\sqrt{0.06}}{\\sqrt{6}}\\right]=[1.754,\\ 2.146]\\)\n\r\\(\\left[1.95-2.5706\\frac{0.206}{\\sqrt{6-1}},\\ 1.95+2.5706\\frac{0.206}{\\sqrt{6-1}}\\right]=[1.713,\\ 2.187]\\)\n\r\r\r一些思考\r\r分析这两种的结果会发现，由同一组样本观察值，按同样的置信概率，对\\(\\mu\\)计算出的置信区间因为\\(\\sigma\\)的是否已知会不一样。这因为：当\\(\\sigma\\)为已知时，我们掌握的信息多一些，在其他条件相同的情况下，对\\(\\mu\\)的估计精度要高一些，即表现为\\(\\mu\\)的置信区间长度要小些。反之，当\\(\\sigma\\)为未知时，对\\(\\mu\\)的估计精度要低一些，即表现为\\(\\mu\\)的置信区间长度在大一些。这是因为当\\(n\\)比较小时，\\(t_{1-\\alpha/2}(n-1)\u0026gt;u_{1-\\alpha/2}\\).\n\r还可以发现，当样本量\\(n\\)不断增大时，两种情况下的置信区间会慢慢接近。\r也就意味着大样本信息可以弥补\\(\\sigma\\)的缺失带来的偏差（大数定律）。\n\r\r\r已知期望，求方差的置信区间\r构造统计量\r\\[T =\\sum_{i=1}^n\\frac{(X_i-\\mu)^2}{\\sigma^2}\\sim \\chi^2(n)\\]\r\\[P\\left(\\chi^2_{\\alpha/2}(n)\\le\\sum_{i=1}^n\\frac{(X_i-\\mu)^2}{\\sigma^2}\\le \\chi^2_{1-\\alpha/2}(n)\\right) = 1-\\alpha\\]\r\\(\\sigma^2\\)的置信度为\\(1-\\alpha\\)的置信区间为：\r\\[\\left[\\frac{\\sum_{i=1}^n(X_i-\\mu)^2}{\\chi^2_{1-\\alpha/2}(n)},\\ \\frac{\\sum_{i=1}^n(X_i-\\mu)^2}{\\chi^2_{\\alpha/2}(n)}\\right]\\]\n\r期望未知，求方差的置信区间\r构造统计量\r\\[T =\\frac{nS_n^2}{\\sigma^2}=\\sum_{i=1}^n\\frac{(X_i-\\bar X)^2}{\\sigma^2}\\sim \\chi^2(n-1)\\]\r\\[P\\left(\\chi^2_{\\alpha/2}(n-1)\\le\\sum_{i=1}^n\\frac{(X_i-\\bar X)^2}{\\sigma^2}\\le \\chi^2_{1-\\alpha/2}(n-1)\\right) = 1-\\alpha\\]\r\\(\\sigma^2\\)的置信度为\\(1-\\alpha\\)的置信区间为：\r\\[\\left[\\frac{\\sum_{i=1}^n(X_i-\\bar X)^2}{\\chi^2_{1-\\alpha/2}(n-1)},\\ \\frac{\\sum_{i=1}^n(X_i-\\bar X)^2}{\\chi^2_{\\alpha/2}(n-1)}\\right]=\\left[\\frac{nS_n^2}{\\chi^2_{1-\\alpha/2}(n-1)},\\ \\frac{nS_n^2}{\\chi^2_{\\alpha/2}(n-1)}\\right]\\]\n\r2.2 两个独立正态总体的区间估计\r设两个独立总体\\(X\\sim N(\\mu_1,\\sigma_1^2)\\), \\(Y\\sim N(\\mu_2,\\sigma^2)\\), 如何找出未知参数\\(\\mu\\)和\\(\\sigma^2\\)的置信区间？其中\\(X\\)的样本为\\(X_1,\\dots,X_m\\), 样本方差为\\(S_{1m}^2\\); \\(Y\\)的样本为\\(Y_1,\\dots,Y_n\\), 样本方差为\\(S_{2n}^2\\)\n\r已知\\(\\sigma_1^2,\\sigma_2^2\\), 找出\\(\\mu_1-\\mu_2\\)的置信区间\n\r以知\\(\\sigma_1^2=\\sigma_2^2=\\sigma^2\\), 找出\\(\\mu_1-\\mu_2\\)的置信区间\n\r已知\\(\\mu_1,\\mu_2\\), 找出\\(\\sigma_1^2/\\sigma_2^2\\)的置信区间\n\r未知\\(\\mu_1,\\mu_2\\), 找出\\(\\sigma_1^2/\\sigma_2^2\\)的置信区间\n\r\r应用场景\r\r比较男生、女生两个群体的身高/体重/成绩平均水平的差异\r\r\r\r已知方差，求均值差的置信区间\r选择枢轴量：\r\\[U=\\frac{(\\bar X-\\bar Y)-(\\mu_1-\\mu_2)}{\\sqrt{\\sigma_1^2/m+\\sigma_2^2/n}}\\sim N(0,1).\\]\n\\(\\mu_1-\\mu_2\\)的置信度为\\(1-\\alpha\\)的置信区间为：\r\\[\\left[(\\bar{X}-\\bar{Y})-u_{1-\\alpha/2}\\sqrt{\\frac{\\sigma_1^2}m+\\frac{\\sigma_2^2}n},\\ (\\bar{X}-\\bar{Y})+u_{1-\\alpha/2}\\sqrt{\\frac{\\sigma_1^2}m+\\frac{\\sigma_2^2}n}\\right]\\]\n\r已知方差相同，求均值差的置信区间\r选择枢轴量：\r\\[T=\\frac{(\\bar X-\\bar Y)-(\\mu_1-\\mu_2)}{S_w\\sqrt{1/m+1/n}}\\sim t(m+n-2).\\]\n其中\\(S_w =\\sqrt{(mS_{1m}^2+nS_{2n}^2)/(m+n-2)}.\\)\n令\\(t_{1-\\alpha/2}(m+n-2)=t_{1-\\alpha/2}\\)，\n\\(\\mu_1-\\mu_2\\)的置信度为\\(1-\\alpha\\)的置信区间为：\r\\[\\left[(\\bar{X}-\\bar{Y})-t_{1-\\alpha/2}S_w\\sqrt{\\frac 1m+\\frac 1n},\\ (\\bar{X}-\\bar{Y})+t_{1-\\alpha/2}S_w\\sqrt{\\frac 1m+\\frac 1n}\\right]\\]\n\r例\r例：假设OPPO手机充电五分钟通话时间\\(X\\sim N(\\mu_1,\\sigma_1^2)\\), VIVO手机充电五分钟通话时间\\(Y\\sim N(\\mu_2,\\sigma_2^2)\\). 随机抽取6部手机测试通话时间（单位：小时）为\n\\[\\text{OPPO}:\\ 1.6,\\ 2.1,\\ 1.9,\\ 1.8,\\ 2.2,\\ 2.1\\]\n\\[\\text{VIVO}:\\ 1.8,\\ 2.2,\\ 1.5,\\ 1.4,\\ 2.0,\\ 1.7\\]\n求\\(\\mu_1-\\mu_2\\)的置信度为\\(95\\%\\)的置信区间:\n\r已知\\(\\sigma_1^2 = 0.06,\\ \\sigma_2^2 = 0.08\\).\r已知\\(\\sigma_1^2 =\\sigma_2^2\\).\r\r解：\\(m=n=6\\), \\(\\bar{X}=1.95,\\ \\bar{Y}=1.77\\), \\(S_{1m}^2=0.042, S_{2n}^2=0.064, S_w = 0.252.\\) 查表知，\\(u_{0.975}=1.96\\),\r\\(t_{0.975}(10)=2.23\\).\n\r第一种情况为\\([-0.12,\\ 0.48]\\)\r第二种情况为\\([-0.14,\\ 0.50]\\)\r\r\r已知均值，求方差比的置信区间\r\\[T_1 =\\sum_{i=1}^m\\frac{(X_i-\\mu_1)^2}{\\sigma_1^2}\\sim \\chi^2(m),\\ T_2 =\\sum_{i=1}^n\\frac{(Y_i-\\mu_2)^2}{\\sigma_2^2}\\sim \\chi^2(n)\\]\n\\[\\frac{T_1/m}{T_2/n}=\\frac{\\frac 1 m\\sum_{i=1}^m(X_i-\\mu_1)^2}{\\frac 1 n\\sum_{i=1}^n(Y_i-\\mu_2)^2}\\frac{\\sigma_2^2}{\\sigma_1^2}\\sim F(m,n)\\]\n\\(\\sigma_1^2/\\sigma_2^2\\)的置信度为\\(1-\\alpha\\)的置信区间为：\r\\[\\left[\\frac{1}{F_{1-\\alpha/2}(m,n)}\\frac{\\frac 1 m\\sum_{i=1}^m(X_i-\\mu_1)^2}{\\frac 1 n\\sum_{i=1}^n(Y_i-\\mu_2)^2},\\ \\frac{1}{F_{\\alpha/2}(m,n)}\\frac{\\frac 1 m\\sum_{i=1}^m(X_i-\\mu_1)^2}{\\frac 1 n\\sum_{i=1}^n(Y_i-\\mu_2)^2} \\right]\\]\n\r均值未知，求方差比的置信区间\r\\[T_1=\\frac{(m-1)S_{1m}^{*2}}{\\sigma_1^2}=\\sum_{i=1}^m\\frac{(X_i-\\bar X)^2}{\\sigma_1^2}\\sim \\chi^2(m-1)\\] \\[T_2=\\frac{(n-1)S_{2n}^{*2}}{\\sigma_2^2}=\\sum_{i=1}^n\\frac{(Y_i-\\bar Y)^2}{\\sigma_2^2}\\sim \\chi^2(n-1)\\]\r\\[\\frac{T_1/(m-1)}{T_2/(n-1)}=\\frac{S_{1m}^{*2}}{S_{2n}^{*2}}\\frac{\\sigma_2^2}{\\sigma_1^2}\\sim F(m-1,n-1)\\]\r\\(\\sigma_1^2/\\sigma_2^2\\)的置信度为\\(1-\\alpha\\)的置信区间为：\r\\[\\left[\\frac{1}{F_{1-\\alpha/2}(m-1,n-1)}\\frac{S_{1m}^{*2}}{S_{2n}^{*2}},\\ \\frac{1}{F_{\\alpha/2}(m-1,n-1)}\\frac{S_{1m}^{*2}}{S_{2n}^{*2}} \\right]\\]\n\r一些说明\r\r枢轴量法的难点在于寻找枢轴量，没有统一的方法。正态总体下的应用应当熟练掌握。\n\r另外一种求置信区间方法叫统计量方法，不作要求，感兴趣参考教材pp42-46.\n\r“最优的置信区间”是否存在？目前尚缺乏对置信区间的优良性讨论。\n\r\r\r2.3 非正态总体参数的区间估计\r令\\(\\mu=E[X],\\sigma^2=Var[X]\\)分别为总体\\(X\\)的期望和方差。\r由中心极限定理，\r\\[\\frac{\\bar X-\\mu}{\\sigma/\\sqrt{n}}\\stackrel{\\cdot}\\sim N(0,1).\\]\r当\\(\\sigma\\)已知时，总体期望\\(\\mu\\)的置信度为\\(1-\\alpha\\)的区间估计可以近似为\r\\[\\left[\\bar X-u_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}, \\bar X+u_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\right].\\]\r如果\\(\\sigma\\)未知，可以用样本标准差\\(S_n\\)（或者修正样本差\\(S_n^*\\)）替代\\(\\sigma\\)，\r\\[\\left[\\bar X-u_{1-\\alpha/2}\\frac{S_n}{\\sqrt{n}}, \\bar X+u_{1-\\alpha/2}\\frac{S_n}{\\sqrt{n}}\\right].\\]\n\r3 分布的估计\r\r分布函数的估计\r密度函数的直方图估计\r密度函数的核估计\r\r\r分布函数的估计\r经验分布函数：\n\\[\rF_n(x) =\\frac{1}{n}\\sum_{i=1}^n 1\\{x_i\\le x\\} = \\begin{cases}\r0,\u0026amp;\\ x\u0026lt;x_{(1)}\\\\\r1/n,\u0026amp;\\ x_{(1)}\\le x\u0026lt;x_{(2)}\\\\\r2/n,\u0026amp;\\ x_{(2)}\\le x\u0026lt;x_{(3)}\\\\\r\u0026amp;\\vdots\\\\\rk/n,\u0026amp;\\ x_{(k)}\\le x\u0026lt;x_{(k+1)}\\\\\r\u0026amp;\\vdots\\\\\r1,\u0026amp;\\ x\u0026gt;x_{(n)}\\\\\r\\end{cases}\r\\]\n\r3.1 密度函数的估计——直方图法\r只考虑一维连续型总体\\(X\\sim f(x)\\)。设\\(X_1,\\dots,X_n\\)为样本，\\(R_n(a,b)\\)表示落在区间\\((a,b]\\)中的个数。由中值定理得，存在\\(x_0\\in(a,b]\\)使得\r\\[f(x_0)=\\frac 1{b-a}\\int_a^b f(x)dx\\approx \\frac {R_n(a,b)}{n(b-a)}\\]\n设\\(-\\infty\u0026lt;t_0\u0026lt;t_1\u0026lt;\\dots\u0026lt;t_m\u0026lt;\\infty\\)，\\(t_{i+1}-t_i=h\u0026gt;0\\). 直方图法的密度估计为：\n\\[\rf_n(x)=\r\\begin{cases}\r\\frac{R_n(t_i,t_{i+1})}{nh},\\ x\\in(t_i,t_{i+1}],i=0,\\dots,m-1\\\\\r0, x\\le t_0,x\u0026gt;t_m\r\\end{cases}\r\\]\n实际上选取\\(t_0\\)为比\\(X_{(1)}\\)略小的数，选取\\(t_m\\)为比\\(X_{(n)}\\)略大的数。经验法则：\\(m\\approx 1+3.322\\log_{10} n.\\)\n\r案例：身高数据\r\r直方图法的相合性\r定理：设\\(f(\\cdot)\\)在点\\(x\\)连续且\\(\\lim_n h_n=0,\\lim_n nh_n=\\infty\\), 则对任何\\(\\epsilon\u0026gt;0\\)有\r\\[\\lim_{n\\to\\infty} P(|f_n(x)-f(x)|\\ge \\epsilon)=0.\\]\n定理：设\\(f(\\cdot)\\)在\\(\\mathbb{R}\\)上一致连续，\\(\\int_{-\\infty}^\\infty |x|^\\delta d x\u0026lt;\\infty\\)(对某个\\(\\delta\u0026gt;0\\)), 且\\(\\lim_n h_n=0,h_n\\ge (\\ln n)^2/n\\), 则\r\\[P(\\lim_{n\\to\\infty} \\sup_x|f_n(x)-f(x)|=0)=1.\\]\n证明见书pp54-55.\n\r3.2 核估计法\r中心差分：\r\\[f(x)\\approx \\frac{F(x+h)-F(x-h)}{2h}\\approx \\frac{F_n(x+h)-F_n(x-h)}{2h}\\]\n\\[\\hat{f}_n(x) = \\frac{1}{2hn}\\sum_{i=1}^n 1\\{x-h\u0026lt;X_i\\le x+h\\}=\\frac{1}{2hn}\\sum_{i=1}^n K_0\\left(\\frac{x-X_i}{h}\\right)\\]\n其中\r\\[K_0(x)= \\frac 12 1\\{-1\\le x\u0026lt;1\\}\\]\n核函数：\\(K(x)\\)是\\(\\mathbb{R}\\)上的非负函数且满足\\(\\int_{-\\infty}^\\infty K(x)=1\\).\n核估计：\\(\\hat{f}_n(x) = \\frac{1}{2hn}\\sum_{i=1}^n K\\left(\\frac{x-X_i}{h}\\right)\\)\n\r常用的核函数\r均匀核函数：\r\\[K_0(x)= \\frac 12 1\\{-1\\le x\\le1\\}\\]\n\\[K_1(x)= 1\\{-1/2\\le x\\le1/2\\}\\]\n正态核函数：\r\\[K_2(x)=\\frac{1}{\\sqrt{2\\pi}}e^{-x^2/2}\\]\n\r核估计的相合性\r定理：设核函数\\(K(x)\\)满足条件\r\\[\\int_{-\\infty}^\\infty (K(x))^2 dx\u0026lt;\\infty,\\ \\lim_{|x|\\to \\infty} |x|K(x)=0,\\]\n又密度函数\\(f\\)在点\\(x\\)处连续，且\\(h_n\\to 0\\), \\(nh_n\\to\\infty\\), 则对一切\\(\\epsilon\u0026gt;0\\), 有\r\\[\\lim_{n\\to\\infty} P(|\\hat{f}_n(x)-f(x)|\\ge \\epsilon) = 0.\\]\n证明见pp56-58.\n\r案例：身高数据\r\r案例：身高数据\r\r一些说明\r收敛速度的比较\r在满足一些正则性的条件（如，\\(h_n\\to 0\\), \\(nh_n\\to\\infty\\)）下，可以证明\n\r直方图法的均方误差为\\(O(n^{-2/3})\\)\r核估计的均方误差为\\(O(n^{-4/5})\\)\r\r\r核估计的带宽(bandwidth) \\(h_n\\)如何选取?\r\r如果选择正态核函数，经验法则：\\(h_n\\approx 1.06S_nn^{-1/5}\\)\r\r\r延伸阅读\r\rhttps://en.wikipedia.org/wiki/Kernel_density_estimation\n\rKernel smoothing techniques used in finance\n\rused in Approximate Bayesian Computation (ABC)\n\r\r\r\r一些参考文献\rLiu, Guangwu; Hong, L. Jeff. Kernel estimation of the greeks for options with discontinuous payoffs. Operations Research. Vol. 59, No. 1, pp. 96-108, 2011.\n\rHong, L. Jeff; Juneja, Sandeep; Liu, Guangwu. Kernel smoothing for nested estimation with application to portfolio risk measurement. Operations Research. Vol. 65, No. 3, pp. 657-673, 2017.\n\rAmal B. Abdellah, Pierre L’Ecuyer, Art B. Owen, Florian Puchhammer. Density estimation by Randomized Quasi-Monte Carlo. arXiv:1807.06133, 2018\n\r\r\r","date":1537228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537228800,"objectID":"80850aa8cbdec1e73f6e615503ead319","permalink":"/post/chap02/","publishdate":"2018-09-18T00:00:00Z","relpermalink":"/post/chap02/","section":"post","summary":"第二章：估计","tags":["课件","数理统计"],"title":"第二章：估计","type":"post"},{"authors":null,"categories":["作业"],"content":"设随机变量\\(X\\sim N(0,1)\\), 对给定的\\(\\alpha\\in(0,1)\\), 数\\(u_{\\alpha}\\) 满足\\(P(X\u0026gt;u_\\alpha)=\\alpha\\). 若\\(P(|X|\u0026lt;x)=\\alpha\\), 则\\(x\\)等于（ ）。答案：C\nA. \\(u_{\\alpha/2}\\)\nB. \\(u_{1-\\alpha/2}\\)\nC. \\(u_{(1-\\alpha)/2}\\)\nD. \\(u_{1-\\alpha}\\)\n设\\(X_1,\\dots,X_n\\)为总体\\(N(1,2^2)\\)的样本，下面正确的是（ ）。答案：D\nA. \\(\\frac{\\bar X-1}{2/\\sqrt{n}}\\sim t(n)\\)\nB. \\(\\frac{1}{4}\\sum_{i=1}^n(X_i-1)^2\\sim F(n,1)\\)\nC. \\(\\frac{\\bar X-1}{\\sqrt{2}/\\sqrt{n}}\\sim N(0,1)\\)\nD. \\(\\frac{1}{4}\\sum_{i=1}^n(X_i-1)^2\\sim \\chi^2(n)\\)\n设\\(X_1,\\dots,X_{15}\\)为总体\\(N(0,2^2)\\)的样本，则统计量\r\\[Y=\\frac{X_1^2+\\dots+X_{10}^2}{2(X_{11}^2+\\dots+X_{15}^2)}\\]\r的分布为（ ）。答案：A\nA. \\(F(10,5)\\)\nB. \\(F(11,4)\\)\nC. \\(\\chi^2(10)\\)\nD. 以上都不是\n设\\(X_1,\\dots,X_n\\)是来自双参数指数分布\r\\[p(x;\\mu,\\theta)=\\frac 1\\theta \\exp\\{-(x-\\mu)/\\theta\\}, x\u0026gt;\\mu,\\theta\u0026gt;0\\]\r的一个样本，证明\\((\\bar X,X_{(1)})\\)是该分布的充分统计量。\n证明：样本的联合密度函数为\r\\[f(x_1,\\dots,x_n) =\\prod_{i=1}^n\\frac 1\\theta e^{-(x_i-\\mu)/\\theta}1\\{x_i\u0026gt;\\mu\\}=\\theta^{-n}e^{-(n\\bar X-n\\mu)/\\theta}1\\{x_{(1)}\u0026gt;\\mu\\}\\]\n由此得知，样本的联合密度函数可以表示为形如\\(h(\\bar X,X_{(1)};\\mu,\\theta)\\)的函数。由因子分解定理可得\\((\\bar X,X_{(1)})\\)为该分布的充分统计量。\n设\\(X_1,\\dots,X_n\\)是来自密度函数\r\\[p_\\theta(x)=\\theta/x^2,\\ 0\u0026lt;\\theta\u0026lt;x\u0026lt;\\infty\\]\r的一个样本，求参数\\(\\theta\\)的充分统计量。\n解：样本的联合密度函数为\r\\[f(x_1,\\dots,x_n) = \\prod_{i=1}^n \\theta/x_i^2 1\\{x_i\u0026gt;\\theta\\}=\\left(\\prod_{i=1}^nx_i^{-2}\\right)\\theta^n 1\\{x_{(1)}\u0026gt;\\theta\\}\\]\n由因子分解定理可得\\(X_{(1)}\\)为参数\\(\\theta\\)的充分统计量。\n\r注意：当然充分统计量不是唯一，比如一部分同学认为\\((X_{(1)},\\prod_{i=1}^nx_i^{-2})\\)是充分统计量。这也是正确的，不过这个充分统计量不是最优的。\n\r在R中使用命令boxplot分析数据OrchardSprays, 上传相应的箱线图并基于实验背景分析结果。\n\r关于数据OrchardSprays的背景介绍查看帮助文档：? OrchardSprays\rR语言的代码供参考（请把“你的名字”替换成你真实的名字)：boxplot(decrease ~ treatment, data = OrchardSprays, main=“你的名字”)\r\r实验背景：在蔗糖溶液中用石灰硫磺合剂填充蜂巢的每个蜂房,一共使用了七个不同浓度的石灰硫磺合剂，浓度分别为\\(0.01\\times 5^{-i+1},i=1,\\dots,7\\), 以及一个不含石灰硫磺合剂的溶液。\r通过将100只蜜蜂放在密室中两小时，测量不同浓度下蜂巢中蔗糖溶液体积的减少量。\n实验目的：研究不同浓度的石灰硫磺合剂对蜜蜂的驱赶效果\nR代码：\nboxplot(decrease ~ treatment, data = OrchardSprays,xlab=\u0026quot;石灰硫磺合剂浓度\u0026quot;,ylab=\u0026quot;溶液减少量\u0026quot;,main=\u0026quot;不同浓度的驱蜂效果比较\u0026quot;)\r数据分析：由于蜜蜂是喜欢蔗糖溶液的，所以在正常情况下将100只蜜蜂放至密室两小时，蜂巢中蔗糖溶液体积是会变少的。实验中的“蔗糖溶液”的作用类比果园中的“花粉”。果园种植要除去害虫，但使用含石灰硫磺合剂的喷雾剂在消灭害虫的同时也会驱赶蜜蜂。本实验的目的是研究不同浓度的石灰硫磺合剂对蜜蜂的驱赶效果。因此，实验中溶液的减少量（即纵坐标）越小说明蜜蜂越排斥该溶液，这样驱赶效果越明显，也就意味着不利于花粉的传播。\n石灰硫磺合剂的浓度对驱赶蜜蜂是有显著影响的，而且浓度越高驱赶效果越明显，对应数据的中位数呈线性相关。\n\r当浓度比较低时(F、G组), 数据波动较大，且与不含石灰硫磺合剂的溶液(H组)中位数相差不大，这表明浓度比较低时驱蜂效果不显著。\n\r从实际的角度出发，我们需要有一定浓度的石灰硫磺合剂才能取得较好的除去害虫的效果，但过高的浓度必然会导致驱赶蜜蜂（这不利于花粉传播）。如何权衡这两者？这就要对这两方面进行建模：一是建立石灰硫磺合剂浓度对去除害虫效果的关系；二是建立石灰硫磺合剂浓度对驱赶蜜蜂效果的关系。对于第二方面，我们可以利用这个数据进行深入建模，比如假设\\(y\\)表示溶液减少量，\\(x\\)表示浓度，可以通过引入恰当的模型刻画\\(x,y\\)的联系。而对于第一方面，根据目前的数据无法判断。基于这点，我们目前不能得到“哪种浓度的石灰硫磺合剂最好”的结论。或许我们可以排除两种极端情况：A（不利于花粉传播）和H（不利于去除害虫）。\n\r\r","date":1536796800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536796800,"objectID":"0de6d2498a6c4c2113c00d4e682d7494","permalink":"/post/homework2/","publishdate":"2018-09-13T00:00:00Z","relpermalink":"/post/homework2/","section":"post","summary":"答案已上传！","tags":["作业","数理统计"],"title":"第二次作业","type":"post"},{"authors":null,"categories":["作业"],"content":"韦布尔分布(Weibull distribution)族\r\\[p(x)=\\frac k\\lambda\\left(\\frac{x}{\\lambda}\\right)^{k-1}e^{-(x/\\lambda)^k}1\\{x\\ge 0\\},k\u0026gt;0,\\lambda\u0026gt;0\\]\r是不是指数型分布族？答案：B\nA. 是\nB. 不是\n\r注意：这里的未知参数有两个，分别是\\(k,\\lambda\\)\n\r从均值为\\(\\mu\\), 方差为\\(\\sigma^2\\)的总体中随机抽取样本量为\\(n\\)的样本\\(x_1,\\dots,x_n\\), 其中\\(\\mu,\\sigma^2\\)均未知，指出下列样本函数中哪些为统计量（ ）。答案：ADE\nA. \\(T_1=x_1+x_2\\)\nB. \\(T_2=x_1+x_2-2\\mu\\)\nC. \\(T_3=(x_1-\\mu)/\\sigma\\)\nD. \\(T_4=(\\bar x-10)/5\\)\nE. \\(T_5=\\frac 1 n\\sum_{i=1}^n(x_i-S_n)^2\\)\n设\\(\\bar x_n,s_n^2\\)表示样本\\(x_1,\\dots,x_n\\)的样本均值与样本方差。已知\\[n=15,\\bar x_{n}=168, s_n=11.43, x_{n+1}=170.\\] 求\\(\\bar x_{n+1},s_{n+1}^2\\)，以及修正样本方差\\(s_{n+1}^{*2}\\).\n\r答案来自：张华@16统计\n\r\r注意：有同学把已知条件\\(s_n=11.43\\)看成\\(s_n^2=11.43\\)\n\r设\\(X_1\\sim Ga(\\alpha_1,\\lambda)\\), \\(X_2\\sim Ga(\\alpha_2,\\lambda)\\), 且\\(X_1\\)与\\(X_2\\)独立。证明\n\\(Y_1=X_1+X_2\\sim Ga(\\alpha_1+\\alpha_2,\\lambda)\\)\r\\(Y_2=X_1/(X_1+X_2)\\sim Beta(\\alpha_1,\\alpha_2)\\)\r\\(Y_1\\)与\\(Y_2\\)独立\r\r\r答案来自：王博@16统计\n\r\r注意：大部分同学每一问都分别给出证明；实际上只需在求第三问时算出他们的联合密度函数即可，容易观察出联合密度函数是“可分离”的。\n\r设\\(X_1,\\dots,X_n\\)是来自某连续总体的一个样本，总体的分布函数\\(F(x)\\)是连续严增函数，证明：统计量\\(T=-2\\sum_{i=1}^n \\ln F(X_i)\\sim \\chi^2(2n)\\).\n\r答案来自：刘霏@16信管\n\r从正态总体\\(N(52,6.3^2)\\)中随机抽取容量为36的样本。\n求样本均值\\(\\bar X\\)的分布；\r求\\(\\bar X\\)落在区间\\((50.8,53.8)\\)内的概率；\r若要以\\(99\\%\\)的概率保证\\(|\\bar X-52|\u0026lt;2\\), 试问样本量至少应取多少？\r\r\r答案来自：甘桃菁@16信计\n\r","date":1536624000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536624000,"objectID":"f98ea091f467a910ba50d8659c168b76","permalink":"/post/homework1/","publishdate":"2018-09-11T00:00:00Z","relpermalink":"/post/homework1/","section":"post","summary":"答案已上传！","tags":["作业","数理统计"],"title":"第一次作业","type":"post"},{"authors":null,"categories":["R"],"content":"预备工作\r\rR软件下载: https://www.r-project.org/\n\rRStudio编辑器下载: https://www.rstudio.com/\n\r\r\r例1：密度函数画图\r画图的主要命令1为plot(x,y,...)，里面各种参数的含义可查看帮助文档help(plot)或者? plot. 下面以画伽马分布的密度为例。\n所有的图像都是离散点拼接起来，首先要确定横坐标，可用seq(from = a, to = b, length=n)生成\\([a,b]\\)间的\\(n\\)个等分点。\n\r接着根据横坐标的值计算相应的密度函数值，常用分布的密度函数在R种有现成的函数（使用命令? distribution查看常用的分布），直接调用即可。比如伽马分布的密度为dgamma(x,shape=alpha,rate = lambda), 详情查看帮助文档? dgamma\r下面为三组参数下的画图代码（可复制到一个空白的R文件中保存运行）\n\r\rx = seq(0.001,10,length = 10000) #生成横坐标值\rlambda = 0.5\ralpha = 1\ry = dgamma(x,shape=alpha,rate = lambda) #计算相应的密度值\rpar(mai=c(0.9,0.9,0.3,0.1),cex=1.1) #调整图像边缘空白处大小，初学者可不用设置\rplot(x,y,type=\u0026quot;l\u0026quot;,ylab = \u0026quot;f(x)\u0026quot;,col=\u0026quot;blue\u0026quot;,cex.lab=1.2)\r#画第二组参数的图像\ralpha = 2\ry = dgamma(x,shape=alpha,rate = lambda)\rlines(x,y,col=\u0026quot;red\u0026quot;) #此次通过lines命令画第二组参数的图，若用plot命令则输出一副新的图像，而不是在上一幅图基础上叠加\r#画第三组参数的图像\ralpha = 3\ry = dgamma(x,shape=alpha,rate = lambda)\rlines(x,y,col=\u0026quot;green\u0026quot;)\r#画出相应的标注，即图中的小矩形\rexpr1 = expression(alpha==1) #此命令用于希腊字母的转化\rexpr2 = expression(alpha==2)\rexpr3 = expression(alpha==3)\rlegend(6,0.5,legend=c(expr1,expr2,expr3),col=c(\u0026quot;blue\u0026quot;,\u0026quot;red\u0026quot;,\u0026quot;green\u0026quot;),lty = c(1,1,1))\r\r例2：经验分布函数画图\r画经验分布函数主要用到命令ecdf(x). 下面的例子为标准正态样本的经验分布图。R中提供了生成常见分布的样本的命令（使用命令? distribution查看常用的分布）。如生成正态分布\\(N(a,b^2)\\)的\\(n\\)个样本代码为rnorm(n, mean = a, sd = b).\nx = rnorm(100) #生成100个标准正态的样本\rFn1 = ecdf(x[1:10]) #计算前10个样本对应的经验分布函数\rFn2 = ecdf(x[1:100]) #计算前100个样本对应的经验分布函数\r#计算标准正态分布函数\rt = seq(-3,3,by=0.01) #横坐标\ry = pnorm(t) #相应的分布函数值\r#mfrow表示生成两行一列的图，后面的两个参数用于调整页边距，初学者可不用设置\r#最终输出一幅图，包含两幅子图\rpar(mfrow=c(2,1),mgp=c(1.5,0.8,0),mar=.1+c(3,3,2,1)) # 第一幅子图\rplot(Fn1,verticals=TRUE,do.points=FALSE,main=\u0026quot;n=10\u0026quot;,xlim=c(-3,3)) #画经验分布函数\rlines(t,y,col=\u0026quot;red\u0026quot;) #画真实的正态分布函数图像\r# 第二幅子图\rplot(Fn2,verticals=TRUE,do.points=FALSE,main=\u0026quot;n=100\u0026quot;,xlim=c(-3,3)) #画经验分布函数\rlines(t,y,col=\u0026quot;red\u0026quot;) #画真实的正态分布函数图像\r\r课后练习\r\r安装相应的软件\r参考上面两个例子，学会使用plot(...)画相关的图形，了解该命令里面参数的作用。\r参考例1，画不同参数下贝塔分布的密度函数；关键的命令查看帮助? beta.\r参考例2，比较其他分布（查看R中的常用分布?distribution）的经验分布函数与真实的分布函数，并观察他们的差距是否随着样本量的增加而减小。\r\r\r一些建议\r\r充分利用帮助文档。我们不可能记住所有命令的使用方式，使用帮助文档是一种高效的学习途径，此外帮助文档末尾还提供一些参考例子，有助于理解命令的使用方式。\r充分利用网上资源。编程过程中如果遇到问题，可以通过度娘等方式搜索寻找答案，现在的很多技术博客提供很多有价值的资源。\r要学会偷懒。在编写一种算法之前，首先要去了解R软件中有没有现成的命令。如果有现成的，则只需学会如何运用即可。通过不断地积累，工作效率会大大提高。\r\r\r\r更高级的画图方式见ggplot2, 初学者可先忽略↩\n\r\r\r","date":1536537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536537600,"objectID":"4a59260e69ffcbc3b455fbf9c21edaf7","permalink":"/post/ex1-1/","publishdate":"2018-09-10T00:00:00Z","relpermalink":"/post/ex1-1/","section":"post","summary":"R的基本画图技巧","tags":["上机练习","数理统计"],"title":"R的基本画图技巧","type":"post"},{"authors":null,"categories":["课件"],"content":"目录\r1.1 数理统计是一门什么样的学科？\r\r1.2 统计学的发展简史\r\r1.3 基本概念\r\r1.4 抽样分布\r\r1.5 充分统计量\r\r\r1.1 数理统计是一门什么样的学科？\r它使用概率论和其它数学方法，研究怎样收集（通过试验和观察）带有随机误差的数据，并在设定的模型（称为统计模型）之下，对这种数据进行分析（称为统计分析），以对所研究的问题作出推断（称为统计推断）。\r由于所收集的统计数据（资料）只能反映事物的局部特征，数理统计的任务就在于从统计资料所反映的局部特征以概率论作为理论基础去推断事物的整体特征。\n本质：由局部（有限样本）推断整体（总体）\r\r数据\r模型\r推断\r\r\r\r数据是什么？\r\r模型是什么？\r\r刻画实际问题\r能够进行统计分析\r\rEssentially, all models are wrong, but some are useful. —— George Box\r\r\r什么样的推断？\r由样本到总体的推理称为统计推断，有两种基本形式：\n参数估计\r\r模型中未知参数\r与“业务相关”的未知量\r\r\r假设检验\r\r判断命题的真假\r\r\r\r案例\r\r案例\r问题1：OPPO手机充电五分钟通话时间为多少？（参数估计）\n问题2：“OPPO手机充电五分钟通话2小时”是否可信？（假设检验）\n一般步骤\r\r数据收集：用\\(n\\)部手机进行测试，记录通话时间\\(X_1,\\dots,X_n\\)\r模型假定：假设通话时间\\(X\\)服从正态分布\\(N(\\mu,\\sigma^2)\\)\r数据分析：通过观测数据\\(x_1,\\dots,x_n\\)作出统计推断\r\r\r\r1.2 统计学的发展简史\r第一个时期（萌芽阶段）\r\r20世纪以前，描述性统计\r代表性人物：高斯(C. F. Gauss, 1777-1855), 皮尔逊(K. Pearson, 1857-1936)等\r\r\r第二个时期（蓬勃发展阶段）\r\r20世纪初至第二次世界大战\r代表性人物：费希尔(R. A. Fisher, 1890-1962), 奈曼(J. Neyman, 1894-1981), 小皮尔逊(E. S. Pearson, 1895-1980), 许宝騄(1910-1970)等\r\r\r第三个时期（理论与应用高速发展）\r\r二战后至今，得益于计算机的发展，统计方法渗透许多学科\r贝叶斯学派的兴起\r大数据时代与人工智能的发展\r现代统计依赖强大的计算能力\r\r\r\r频率学派与贝叶斯学派\r频率学派（传统学派）\r\r频率学派认为样本信息来自总体，仅通过研究样本信息可以对总体信息做出合理的推断和估计，并且样本越多，就越准确。\n\r代表性人物：费希尔 (R. A. Fisher, 1890-1962)\n\r\r\r贝叶斯学派\r\r起源于英国学者贝叶斯(T. Bayes, 1702-1761)在1763年发表的著名论文《论有关机遇问题的求解》\r最基本观点：任何一个未知量都可以看作是随机的，应该用一个概率分布去描述未知参数，而不是频率派认为的固定值。这种信息称为先验信息，是主观信息。\r\rGood (1973)评价道：\n\r“主观主义者直抒他们的判断，而客观主义者以假设来掩盖其判断，并以此享受科学客观性的荣耀。”\n\r\r\r贝叶斯公式\r\r贝叶斯统计的发展\r应用领域\r\r自然语言处理：计算机翻译语言、识别语音、认识文字和海量文献的检索\r\r\r南京市长江大桥欢迎您!\n\r\r人工智能、无人驾驶\r垃圾短信、垃圾邮件识别\r\r\r贝叶斯决策\r\r如何在一个陌生的地方找餐馆吃饭？\r\r\r\r贝叶斯统计课程(研究生课程)\r本课程不涉及贝叶斯统计内容，欢迎对贝叶斯统计感兴趣的同学参加以下课程。\n教材\r\r贝叶斯数据分析（第三版）, A. Gelman等，机械工业出版社\rhttps://item.jd.com/11886268.html\r\r\r上课时间\r\r1-12周，周一下午第5—8节，共48学时\r\r\r上课地点\r\r四号楼4135\r\r\r\r统计学专业\r统计学的应用涉及金融、经济、社会学、工程学、环境等多个领域，从而形成的相应的研究分支。其特点是多学科交叉、实用为主。\n统计学专业包含理论统计和应用统计两方面\r\r理论统计：模型选择，非参统计方法，贝叶斯统计，时间序列与生存分析，高维数据分析与机器学习，数据挖掘等等。\n\r应用统计：目前发展最为突出的是生物统计，金融统计等等。\n\r\r\r\r统计学专业\r统计学经过漫长的发展，尤其是计算机的大量应用，目前包括但不限于下面这些分支（或者交叉领域）：\n\r理论研究：概率论（比如Stochastic Process），计算统计理论（比如Asymptotic Theory，在CS系的Computational Theory下面）等\r统计模型（在前人基础上继续发展各种Regression Model，Stratification，Clustering，Blocking，classification等等）、各种Test的发展（比如Time Series，Likelihood Ratio Test, Wald test, Permutation test 等）\r计算统计方法的发展（比如Monte Carlo Simulation，Bootstrap）\r数据采集（Census，Survey和Clinical Trial等）\r生物统计（比如Longitudinal Analysis，Spatial Analysis）\rMachine Learning\rData Mining\r\r目前最火热的学科都是跟计算机结合比较紧密的。\n\rStatistician Salaries in the United States\r\r1.3 基本概念\r(1) 总体\r\r(2) 样本\r\r(3) 分布族\r\r(4) 统计量与估计量\r\r(5) 经验分布函数\r\r\r总体\r我们把研究对象的全体（包括有形的和潜在的）称作总体，其中每个成员称为个体。常用随机变量\\(X\\)来刻画一个总体（或者总体的特征值）。\n例\r\r网上购物居民占全市居民的比例\r过去一年内网购居民的购物次数\r某品牌灯泡的寿命\r\r注：总体\\(X\\)的分布函数\\(F(x)\\)未知或者部分未知，统计学的核心任务就是要对总体进行观测，并对所得数据推断总体的分布信息。\n\r\r样本\r研究总体可分为普查和抽样这两种方法。\n普查（全数检查）\r\r对总体中的每个个体进行观察，如我国每十年一次的人口普查\r缺点：费用高、时间长、不适合破坏性试验\r\r\r抽样\r\r从总体中抽取若干个体进行观察，用所获得数据对总体进行统计推断\r优点：费用低、时间短\r抽取的部分组成的集合\\((X_1,\\dots,X_n)\\)称为样本，\\(X_i\\)称为样品\r样品个数\\(n\\)称为样本量或者样本容量\r\r\r\r简单随机抽样\r简单随机抽样满足以下两个特征：\n\r随机性：每个个体都有相同的机会选中（有放回随机抽取/独立重复观测），即\\(X_i\\)与\\(X\\)同分布\r独立性：每个样本的选取是独立的\r\r这种方式得到的样本称为简单随机样本（简称样本）\n\r\\(X_1,\\dots,X_n\\)独立同分布(independent and identically distributed, iid)\r本课程所研究的均为简单随机样本\r\r样本具有两重性\r\r抽取之前无法预知它们的数值，故\\((X_1,\\dots,X_n)\\)为\\(n\\)维随机向量\r抽取后样本为具体的数，用小写字母\\((x_1,\\dots,x_n)\\)表示，称为样本观测值\r\r注：所有的统计分析都是基于随机变量，统计推断结论基于样本观测值（数据）。\n\r\r案例：\r“二战”期间，为了加强对战机的防护，英美军方调查了作战后幸存飞机上弹痕的分布，决定哪里弹痕多就加强哪里，你支持这种做法吗？\n\r案例：2018年高考全国II卷作文\r2018年高考全国II卷（适用地区: 内蒙古、黑龙江、辽宁、吉林、重庆、陕西、甘肃、宁夏、青海、新疆、西藏、海南）作文题目如下:\n“二战”期间，为了加强对战机的防护，英美军方调查了作战后幸存飞机上弹痕的分布，决定哪里弹痕多就加强哪里，然而统计学家瓦尔德(Abrahom Wald, 1902–1950)力排众议，指出更应该注意弹痕少的部位，因为这些部位受到重创的战机，很难有机会返航，而这部分数据被忽略了。事实证明沃德是正确的。\n要求: 综合材料内容及含义，选好角度，确定立意，明确文体，自拟标题; 不要套作，不得抄袭; 不少于800字。\n这就是所谓的“幸存者偏见”\r\r\r概率分布族\r模型假定：总体\\(X\\)分布\\(F(x)\\)属于某个分布族\\(\\mathcal{F}\\). 分为以下三类：\n参数族\r\r\\(\\mathcal{F}\\)中的分布的一般数学形式已知，但包含若干未知参数\\(\\theta=(\\theta_1,\\dots,\\theta_m)\\)\r\\(\\mathcal{F}:=\\{F_\\theta,\\theta\\in\\Theta\\}\\), 其中\\(\\Theta\\subset \\mathbb{R}^m\\)称为参数空间。\r该模型为参数统计问题，\\(m\\)为模型的维数\r\\(m=1\\)为单参数统计问题，\\(m\u0026gt;1\\)为多参数统计问题\r\r\r非参数族\r\r当\\(\\mathcal{F}\\)中的分布不能通过有限个未知参数来刻画\r该模型为非参数统计问题\r\r\r半参数族\r\r\\(\\mathcal{F}\\)中的分布有一部分可以用参数刻画，一部分则不可以。\r\r\r\r常用的参数族\r离散型\r\r二项分布族\\(\\{b(n,p);0\u0026lt;p\u0026lt;1\\}\\)\r几何分布族\\(\\{Ge(p);0\u0026lt;p\u0026lt;1\\}\\)\r泊松分布族\\(\\{P(\\lambda);\\lambda\u0026gt;0\\}\\)\r\r\r连续型\r\r正态分布族\\(\\{N(\\mu,\\sigma^2);-\\infty\u0026lt;\\mu\u0026lt;\\infty,\\sigma\u0026gt;0\\}\\)\r均匀分布族\\(\\{U(a,b);-\\infty\u0026lt;a\u0026lt;b\u0026lt;\\infty\\}\\)\r指数分布族\\(\\{Exp(\\lambda);\\lambda\u0026gt;0\\}\\)\r\r\r\r伽玛分布族\r伽玛分布族\\(\\{Ga(\\alpha,\\lambda),\\alpha\u0026gt;0,\\lambda\u0026gt;0\\}\\)，\\(\\alpha\\)为形状参数，\\(1/\\lambda\\)为尺度参数\n密度函数\r\\[f(x) = \\frac{\\lambda^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha-1}e^{-\\lambda x}1\\{x\\ge 0\\}\\]\n其中\\(\\Gamma(\\alpha)=\\int_0^{+\\infty} x^{\\alpha-1}e^{-x}dx\\)\n\r\\(\\Gamma(1)=1,\\Gamma(1/2)=\\sqrt{\\pi}\\)\r\\(\\Gamma(\\alpha+1)=\\alpha\\Gamma(\\alpha)\\)\r当\\(\\alpha\\)为整数\\(n\\)时，\\(\\Gamma(n+1)=n!\\)\r\r\r期望：\\(\\frac \\alpha \\lambda\\)；方差：\\(\\frac \\alpha {\\lambda^2}\\)\r\r两个特例\r\r\\(\\alpha=1\\)时伽玛分布为指数分布，即\\(Ga(\\alpha,\\lambda)=Exp(\\lambda)\\)\r\\(\\alpha=n/2,\\lambda=1/2\\)时伽玛分布为自由度为\\(n\\)的卡方分布，即\\(Ga(n/2,1/2)=\\chi^2(n)\\)\r\r\r\r伽玛密度函数\r\r伽玛分布的性质\r性质1（可加性）：设\\(X_1\\sim Ga(\\alpha_1,\\lambda),\\ X_2\\sim Ga(\\alpha_2,\\lambda)\\)。如果\\(X_1\\)与\\(X_2\\)独立，则\r\\[X_1+X_2\\sim Ga(\\alpha_1+\\alpha_2,\\lambda).\\]\n性质2：设\\(X\\sim Ga(\\alpha,\\lambda)\\),则\\(kX\\sim Ga(\\alpha,\\lambda/k)\\), 其中\\(k\u0026gt;0\\).\n提示：\\(Ga(\\alpha,\\lambda)\\)分布的特征函数为\n\\[\\phi(t)=E[e^{itX}]=\\left(1-\\frac{it}\\lambda\\right)^{-\\alpha}\\]\n\r贝塔分布族\r贝塔分布族\\(\\{Beta(\\alpha,\\beta),\\alpha\u0026gt;0,\\beta\u0026gt;0\\}\\)，\\(\\alpha,\\beta\\)为形状参数\n密度函数\r\\[f(x) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}1\\{0\u0026lt;x\u0026lt;1\\}\\]\n\r期望：\\(\\frac{\\alpha}{\\alpha+\\beta}\\)；方差：\\(\\frac{\\alpha \\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}\\)\r\r特例：当\\(\\alpha=\\beta=1\\)时，\\(Beta(1,1)=U(0,1)\\)\r\r适用场合\r\r不合格率\r市场占有率\r命中率\r\r\r\r贝塔密度函数\r\r指数型分布族\r定义：指数型分布族\\(\\mathcal{F}=\\{f_\\theta(x);\\theta\\in\\Theta\\}\\)中的分布（分布列或者密度函数）都可以表示成如下形式：\n\\[f_\\theta(x)=c(\\theta)\\exp\\{\\sum_{j=1}^kc_j(\\theta)T_j(x)\\}h(x)\\]\n其中，\n\r\\(k\\)为正整数\r分布的支撑与参数\\(\\theta\\)无关\r\\(c(\\theta),c_j(\\theta)\\)为参数空间\\(\\Theta\\)上的函数\r\\(h(x)\u0026gt;0\\), \\(T_1(x),\\dots,T_k(x)\\)线性无关\r\r\r常见的指数型分布族\r正态分布族是指数型分布族\r\\[f(x,\\mu,\\sigma)=\\frac 1{\\sqrt{2\\pi}\\sigma}e^{-\\mu^2/(2\\sigma^2)}\\exp\\{\\frac{\\mu}{\\sigma^2}x-\\frac{1}{2\\sigma^2}x^2\\}\\]\n\r二项分布族是指数型分布族\r\\[P(X=x) = C_n^x p^x(1-p)^{n-x}=(1-p)^n\\exp\\{\\ln[p/(1-p)]x \\}C_n^x\\]\n\r伽玛/贝塔分布族是指数型分布族\r\r均匀分布族不是指数型分布族\r\r\r指数型分布族的优点\r性质：如果\\(X_1,\\dots,X_n\\)是来自某指数型分布族中某分布的样本，则样本的联合分布还是指数型分布。\n\\(f_\\theta(x_1,\\dots,x_n)=\\prod_{i=1}^np_\\theta(x_i)=c(\\theta)^n\\exp\\{\\sum_{j=1}^kc_j(\\theta)\\sum_{i=1}^nT_j(x_i)\\}\\prod_{i=1}^nh(x_i)\\)\n\r统计量与估计量\r样本是总体的反映，但样本所含信息不能直接用于解决我们所要研究的问题，而需要把样本所含的信息进行数学上的加工使其浓缩起来，从而解决我们的问题。为此，数理统计学往往构造一个合适的依赖于样本的函数，我们称之为统计量。\n定义：如果\\((X_1,\\dots,X_n)\\)为来自总体的样本，若样本函数\\[T=T(X_1,\\dots,X_n)\\]中不含有任何未知参数，则称\\(T\\)为统计量。统计量的分布称为抽样分布。\n定义：用于估计未知参数的统计量称为点估计量，或者简称估计量。\n注：这里的未知参数常指以下几种：\n\r分布中所含的未知参数\r分布的数字特征：期望、方差、标准差、分位数等\r某事件的概率\r\r\r例\r设\\(X_1,\\dots,X_n\\)为来自\\(X\\sim N(\\mu,\\sigma^2)\\)的样本, 若\\(\\mu\\)已知，\\(\\sigma\\)未知，判断\\(T_1,T_2\\)是否为统计量。\n\\[T_1 = \\frac{\\sqrt{n}(\\sum_{i=1}^n X_i-\\mu)}{\\sigma}\\]\n\\[T_2 = \\min(X_1,\\dots,X_n)\\]\n\r常见的统计量\r样本均值\r\\[\\bar{X}=\\bar X_n=\\frac{1}{n}\\sum_{i=1}^n X_i\\]\n\r样本方差\r\\[S_n^2=\\frac{1}{n}\\sum_{i=1}^n (X_i-\\bar{X})^2\\]\n\r修正样本方差\r\\[S_n^{*2}=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar{X})^2=\\frac{n}{n-1}S_n^2\\]\n\r样本标准差\r\\[S_n=\\sqrt{S_n^2}\\]\n\r\r常见的统计量\r样本\\(k\\)阶原点矩\r\\[\\overline{X^k}=\\frac{1}{n}\\sum_{i=1}^n X_i^k\\]\n\r样本\\(k\\)阶中心矩\r\\[\\frac{1}{n}\\sum_{i=1}^n (X_i-\\bar X)^k\\]\n\r顺序统计量\r\\[X_{(1)}\\le X_{(2)}\\le \\dots\\le X_{(n)}\\]\n其中\\(X_{(1)}=\\min\\{X_1,\\dots,X_n\\}\\), \\(X_{(n)}=\\max\\{X_1,\\dots,X_n\\}\\), \\(X_{(k)}\\)为\\({X_1,\\dots,X_n}\\)的递增排序的第\\(k\\)位。\\(X_{(n)}-X_{(1)}\\)样本极差。\n\r样本中位数\r\\[\r\\tilde{X}=\r\\begin{cases}\rX_{(\\frac{n+1}{2})},\\ \u0026amp;\\text{$n$为奇数}\\\\\r(X_{(\\frac{n}{2})}+X_{(\\frac{n}{2}+1)})/2,\\ \u0026amp;\\text{$n$为偶数}\r\\end{cases}\r\\]\n\r\r经验分布函数\r通过样本的观测值构造一种函数来近似总体的分布函数\n定义：设总体\\(X\\)的样本\\((X_1,\\dots,X_n)\\)的一次观测值\\((x_1,\\dots,x_n)\\), 并将它们由小到大排列\\(x_{(1)}\\le x_{(2)}\\le \\dots\\le x_{(n)}\\), 经验分布函数(或称样本分布函数)定义为\n\\[\rF_n(x) =\\frac{1}{n}\\sum_{i=1}^n 1\\{x_i\\le x\\} = \\begin{cases}\r0,\u0026amp;\\ x\u0026lt;x_{(1)}\\\\\r1/n,\u0026amp;\\ x_{(1)}\\le x\u0026lt;x_{(2)}\\\\\r2/n,\u0026amp;\\ x_{(2)}\\le x\u0026lt;x_{(3)}\\\\\r\u0026amp;\\vdots\\\\\rk/n,\u0026amp;\\ x_{(k)}\\le x\u0026lt;x_{(k+1)}\\\\\r\u0026amp;\\vdots\\\\\r1,\u0026amp;\\ x\u0026gt;x_{(n)}\\\\\r\\end{cases}.\r\\]\n\r经验分布函数示意图\r\r经验分布函数的性质\r固定的\\(x\\)和\\(n\\)，\\(F_n(x)\\)表示事件\\(\\{X\\le x\\}\\)的频率，由强大数定律知，\r\\[F_n(x)\\to P(X\\le x)=F(x),\\]\r即\r\\[P\\left(\\lim_{n\\to\\infty}F_n(x)=F(x)\\right)=1.\\]\n格里汶科定理(定理4.1, p48)给出更强的结果（几乎处处一致收敛）:\n\\[P\\left(\\lim_{n\\to\\infty}\\sup_{x\\in \\mathbb{R}}|F_n(x)-F(x)|=0\\right)=1.\\]\n注：由此可见，当\\(n\\)相当大时，经验分布函数\\(F_n(x)\\)是母体分布函数\\(F(x)\\)的一个良好近似。数理统计学中一切都以样本为依据，其理由就在于此。\n\r1.4 抽样分布\r(1) 样本均值的分布\r\r(2) 正态总体的抽样分布\r\r(3) 顺序统计量的分布\r\r(4) 样本分位数的分布\r\r\r抽样分布\r定义：统计量的概率分布称为抽样分布，分为如下三类：\n精确抽样分布\r\r当总体\\(X\\)的分布已知时，如果对任意自然数\\(n\\)都能导出统计量\\(T(X_1,\\dots,X_n)\\)的分布的显示表达式\r对样本量\\(n\\)较小的统计推断问题（小样本问题）特别有用\r精确抽样分布多数是在正态总体下得到\r\r\r渐近抽样分布\r\r寻求在样本量\\(n\\)无限大时统计量\\(T(X_1,\\dots,X_n)\\)的极限分布\r适用于对样本量\\(n\\)较大的统计推断问题（大样本问题）\r常用的方法是中心极限定理\r\r\r近似抽样分布\r\r寻找一种分布来近似统计量\\(T(X_1,\\dots,X_n)\\)的分布\r\r\r\r样本均值的抽样分布\r定理：设\\(X_1,\\dots,X_n\\)为来自总体\\(X\\)的样本，\\(\\bar X\\)为其样本均值。\n\r如果\\(X\\sim N(\\mu,\\sigma^2)\\)，则\\(\\bar X\\)的精确分布为\\(N(\\mu,\\sigma^2/n)\\).\r如果总体不是正态分布，但\\(E[X]=\\mu,Var[X]=\\sigma^2\\)存在，则\\(\\bar X\\)的渐近分布为\r\\(N(\\mu,\\sigma^2/n)\\)，记为\\(\\bar X\\stackrel{\\cdot}\\sim N(\\mu,\\sigma^2/n)\\).\r\r\r不同总体均值的分布\r\r卡方分布\r定义：设\\(X_i\\stackrel{iid}\\sim N(0,1),i=1,\\dots,n\\)，则称随机变量\r\\[X = X_1^2+\\dots+X_n^2\\]\r服从自由度为\\(n\\)的卡方分布，记为\\(X\\sim \\chi^2(n)\\).\n密度函数：\\(f(x)=\\frac{1}{2^{\\frac n2}\\Gamma(n/2)}x^{\\frac n2-1}e^{-\\frac x2} 1\\{x\u0026gt;0\\}\\)\n期望和方差：\\(E[X]=n,\\ Var[X]=2n\\).\r可加性：如果\\(X\\sim \\chi^2(n)\\), \\(Y\\sim \\chi^2(m)\\)且它们独立，则\r\\[X+Y\\sim \\chi^2(n+m).\\]\n中心极限定理：\r\\[\\frac{X-n}{\\sqrt{2n}}\\stackrel{d}\\to N(0,1).\\]\n\r\r卡方分布的密度函数曲线\r\r样本方差的抽样分布（正态总体）\r定理（定理3.3, p38）：设\\((X_1,\\dots,X_n)\\)为来自总体\\(X\\sim N(\\mu,\\sigma^2)\\)的样本，则\n\r\\(\\bar{X}\\sim N(\\mu,\\sigma^2/n)\\)\r\\[\\frac{nS_n^2}{\\sigma^2}=\\frac{\\sum_{i=1}^n(X_i-\\bar{X})^2}{\\sigma^2}\\sim \\chi^2(n-1)\\]\n\r样本均值\\(\\bar{X}\\)与样本方差\\(S_n^2\\)相互独立\n\r\r详细证明见书P39.\n研究发现，只有正态总体才有“样本均值与方差独立”这一性质。\n\rt分布\r定义：设\\(X\\sim N(0,1), Y\\sim \\chi^2(n)\\), 且它们独立，则称随机变量\n\\[T = \\frac{X}{\\sqrt{Y/n}}\\]\r服从自由度为\\(n\\)的学生氏t分布（简称\\(t\\)分布），记为\\(T\\sim t(n)\\).\n密度函数\r\\[\rf(x)=\r\\frac{\\Gamma\\left(\\frac{n+1}2\\right)}{\\sqrt{n\\pi }\\Gamma\\left(\\frac n2\\right)}\\left(1+\\frac{x^2}n\\right)^{-\\frac{n+1}{2}}\r\\]\n\r两种特例\r\r当\\(n=1\\)时，t分布成为柯西分布.\r可以证明：\\(\\lim_{n\\to\\infty}f(x)=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}\\).\r当\\(n\\ge 25\\)时,可以认为t分布与\\(N(0,1)\\)接近。\r\r\r\rt分布的密度函数曲线\r\rt分布的起源\r\r样本均值与标准差之比的抽样分布\r定理：设\\((X_1,\\dots,X_n)\\)为来自总体\\(X\\sim N(\\mu,\\sigma^2)\\)的样本，则\n\\[\\frac{\\sqrt{n-1}(\\bar{X}-\\mu)}{S_n}=\\frac{\\sqrt{n}(\\bar{X}-\\mu)}{S_n^*}\\sim t(n-1)\\]\n比较：\n\\[\\frac{\\sqrt{n}(\\bar{X}-\\mu)}{\\sigma}\\sim N(0,1)\\]\n\r尾部概率P(|X|\u0026gt;c)的比较\r\r\r分布\rc=2\rc=2.5\rc=3\rc=3.5\r\r\r\rX~N(0,1)\r0.0455\r0.0124\r0.0027\r0.000465\r\rX~t(4)\r0.1161\r0.0668\r0.0399\r0.0249\r\r\r\r\rF分布\r定义：设\\(X\\sim \\chi^2(m), Y\\sim \\chi^2(n)\\), 且\\(X,Y\\)相互独立，则随机变量\n\\[Z=\\frac{X/m}{Y/n}\\]\r称为服从第一自由度为\\(m\\)、第二自由度为\\(n\\)的F分布，记\\(Z\\sim F(m,n)\\). 其密度函数为\n\\[f(x)=\r\\frac{\\Gamma((m+n)/2)}{\\Gamma(m/2)\\Gamma(n/2)}\\left(\\frac{m}{n}\\right)^{m/2}x^{\\frac m2-1}(1+mx/n)^{-(m+n)/2} 1\\{x\u0026gt;0\\}\r\\]\nF分布的性质\r\r\\(Z\\sim F(m,n)\\), 则\\(1/Z\\sim F(n,m)\\).\r如果\\(T\\sim t(n)\\), 则\\(T^2\\sim F(1,n)\\).\r\r\r\rF分布的密度函数曲线\r\r两个独立正态总体的抽样分布\r定理：设两独立总体\\(X\\sim N(\\mu_1,\\sigma_1^2)\\),\\(Y\\sim N(\\mu_2,\\sigma_2^2)\\)的样本分别为\\((X_1,\\dots,X_m),(Y_1,\\dots,Y_n)\\). 样本方差分别为\\(S_{1m}^2,S_{2n}^2\\). 则\n\\[\\frac{(\\bar X-\\bar Y)-(\\mu_1-\\mu_2)}{\\sqrt{\\sigma_1^2/m+\\sigma_2^2/n}}\\sim N(0,1).\\]\n\\[\\frac{mS_{1m}^2/\\sigma_1^2/(m-1)}{nS_{2n}^2/\\sigma_2^2/(n-1)}=\\frac{S_{1m}^{*2}\\sigma_2^2}{S_{2n}^{*2}\\sigma_1^2}\\sim F(m-1,n-1).\r\\]\n如果\\(\\sigma_1=\\sigma_2=\\sigma\\),\n\\[\\frac{(\\bar X-\\bar Y)-(\\mu_1-\\mu_2)}{S_w\\sqrt{1/m+1/n}}\\sim t(m+n-2),\\]\n其中\\(S_w =\\sqrt{(mS_{1m}^2+nS_{2n}^2)/(m+n-2)}\\).\n\r顺序统计量\r定理：若\\(X_1,\\dots,X_n\\)独立同分布，分布函数和密度函数分别为\\(F(x),f(x)\\). 则\\(X_{(1)}=\\min(X_1,\\dots,X_n)\\)的分布函数和密度函数分别\n\\[\\begin{cases}\rF_{X_{(1)}}(x) = 1-(1-F(x))^n\\\\\rf_{X_{(1)}}(x) = n(1-F(x))^{n-1}f(x).\r\\end{cases}\r\\]\r\\(X_{(n)}=\\max(X_1,\\dots,X_n)\\)的分布函数和密度函数分别\n\\[\\begin{cases}\rF_{X_{(n)}}(x) = F(x)^n\\\\\rf_{X_{(n)}}(x) = nF(x)^{n-1}f(x).\r\\end{cases}\r\\]\r更一般地，\n\\[f_{X_{(k)}}(x) = \\frac{n!}{(n-k)!(k-1)!}F(x)^{k-1}(1-F(x))^{n-k}f(x),k=1,\\dots,n.\r\\]\n\r顺序统计量的联合分布\r定理：顺序统计量\\((X_{(i)},X_{(j)})(i\u0026lt;j)\\)的联合密度函数为\n\\[\r\\begin{align}\rf_{X_{(i)},X_{(j)}}(x,y) = \u0026amp;\\frac{n!}{(i-1)!(j-i-1)!(n-j)!}F(x)^{i-1}\\\\\r\u0026amp;[F(y)-F(x)]^{j-i-1}[1-F(y)]^{n-j}f(x)f(y) 1\\{x\\le y\\}.\r\\end{align}\r\\]\n定理：顺序统计量\\((X_{(1)},\\dots,X_{(n)})\\)的联合密度函数为\n\\[\rf(y_1,\\dots,y_n)=\r\\begin{cases}\rn!\\prod_{i=1}^nf(y_i),\u0026amp;y_1\u0026lt;y_2\u0026lt;\\dots\u0026lt;y_n\\\\\r0,\u0026amp;else\r\\end{cases}\r\\]\n\r分位数\r定义：设\\(X\\)的分布函数为\\(F(x)\\). 对于任意\\(\\alpha\\in(0,1)\\), \\(\\alpha\\)分位数\\(x_\\alpha\\)满足\\(F(x_\\alpha)=\\alpha\\).\n\r标准正态分布分位数记为\\(u_{\\alpha}\\)\r\\(t\\)分布分位数记为\\(t_{\\alpha}(n)\\)\r\\(\\chi^2\\)分布分位数记为\\(\\chi^2_{\\alpha}(n)\\)\r\\(F\\)分布分位数记为\\(F_{\\alpha}(m,n)\\)\r\r一些说明\r\r在分位点表中对于标准正态分布、\\(t\\)分布和F分布只能查到\\(\\alpha\u0026gt;1/2\\)的分位数，需利用以下对称性间接查\\(\\alpha\u0026lt;1/2\\)的分位数：\r\r\\[u_\\alpha=-u_{1-\\alpha},\\ t_\\alpha(n)=-t_{1-\\alpha}(n),\\ F_{\\alpha}(m,n)=\\frac{1}{F_{1-\\alpha}(n,m)}\\]\n\r对于\\(t(n)\\)分布，由于当\\(n\\to \\infty\\)时，其极限分布为\\(N(0,1)\\), 所以自由度\\(n\\)比较大时，\\(t_{\\alpha}(n)\\approx u_{\\alpha}\\).\n\r若\\(X\\sim \\chi^2(n)\\)分布，由于当\\(n\\to \\infty\\)时，\\((X-n)/\\sqrt{2n}\\stackrel{d}\\to N(0,1)\\), 所以自由度\\(n\\)比较大时，\\(\\chi^2_{\\alpha}(n)\\approx u_{\\alpha}\\sqrt{2n}+n\\).\n\r\r\r\r分位数示意图\r\r样本分位数\r定义：设\\(X_1,\\dots,X_n\\)为样本，其顺序统计量为\\(X_{(1)},\\dots,X_{(n)}\\).\r对给定的\\(0\u0026lt;\\alpha\u0026lt;1\\), 该样本的\\(\\alpha\\)分位数定义为：\n\\[\rm_\\alpha = \\begin{cases}\r\\frac{1}2[X_{([n\\alpha])}+X_{([n\\alpha]+1)}],\u0026amp;np\\text{是整数}\\\\\rX_{([n\\alpha]+1)},\u0026amp;np\\text{不是整数}\r\\end{cases}\r\\]\n其中\\([a]\\)表示\\(a\\)的整数部分。\n样本分位数的渐近分布\r定理：设总体的密度为\\(f(x)\\),\\(x_\\alpha\\)为其\\(\\alpha\\)分位数，如果\\(f(x)\\)在\\(x_\\alpha\\)处连续，且\\(f(x_\\alpha)\u0026gt;0\\), 则当\\(n\\to \\infty\\)时，样本的分位数\\(m_\\alpha\\)的渐近分布为：\n\\[\rm_\\alpha \\stackrel{\\cdot}\\sim N\\left(x_\\alpha,\\frac{\\alpha(1-\\alpha)}{nf^2(x_\\alpha)}\\right).\r\\]\n\r\r6种杀虫剂的数据\r\rTable 1: \r\rspray\rcount\rspray\rcount\rspray\rcount\rspray\rcount\rspray\rcount\rspray\rcount\r\r\r\rA\r10\rB\r11\rC\r0\rD\r3\rE\r3\rF\r11\r\rA\r7\rB\r17\rC\r1\rD\r5\rE\r5\rF\r9\r\rA\r20\rB\r21\rC\r7\rD\r12\rE\r3\rF\r15\r\rA\r14\rB\r11\rC\r2\rD\r6\rE\r5\rF\r22\r\rA\r14\rB\r16\rC\r3\rD\r4\rE\r3\rF\r15\r\rA\r12\rB\r14\rC\r1\rD\r3\rE\r6\rF\r16\r\rA\r10\rB\r17\rC\r2\rD\r5\rE\r1\rF\r13\r\rA\r23\rB\r17\rC\r1\rD\r5\rE\r1\rF\r10\r\rA\r17\rB\r19\rC\r3\rD\r5\rE\r3\rF\r26\r\rA\r20\rB\r21\rC\r0\rD\r5\rE\r2\rF\r26\r\rA\r14\rB\r7\rC\r1\rD\r2\rE\r6\rF\r24\r\rA\r13\rB\r13\rC\r4\rD\r4\rE\r4\rF\r13\r\r\r\r\r箱线图\r\r1.5 充分统计量\r(1) 充分统计量的定义\r\r(2) 因子分解定理\r\r\r充分统计量\r目标\n\r简化数据\r不损失重要信息\r\r定义：设有一个分布族\\(\\mathcal{F}\\), \\(X_1,\\dots,X_n\\)是从某分布\\(F\\in\\mathcal{F}\\)中抽取的一个样本。\\(T=T(X_1,\\dots,X_n)\\)是一个（向量）统计量。若在给定\\(T=t\\)下，样本\\((X_1,\\dots,X_n)\\)的条件分布与总体分布\\(F\\)无关，则称\\(T\\)为此分布族\\(\\mathcal{F}\\)的充分统计量。如果\\(\\mathcal{F}=\\{F_\\theta;\\theta\\in\\Theta\\}\\)是参数分布族，在给定\\(T=t\\)下，样本\\((X_1,\\dots,X_n)\\)的条件分布与参数\\(\\theta\\)无关，则称\\(T\\)为参数\\(\\theta\\)的充分统计量。\n\r例1：两点分布\r例：总体\\(X\\sim b(1,p),0\u0026lt;p\u0026lt;1\\). 判断以下两个统计量是否是充分统计量\n\r\\(T_1=\\sum_{i=1}^nX_i\\)\r\\(T_2=X_1+X_2\\)\r\r\r例2：几何分布\r例：总体\\(X\\sim Ge(p),0\u0026lt;p\u0026lt;1\\), 证明\\(T=\\sum_{i=1}^nX_i\\)是参数\\(p\\)的充分统计量。\n\r例3：正态分布\r例：总体\\(X\\sim N(\\mu,\\sigma^2)\\)，其中\\(\\sigma\\)已知。证明\\(T=\\sum_{i=1}^nX_i\\)是参数\\(\\mu\\)的充分统计量。\n引理\r设总体的密度为\\(f_\\theta(x)\\). 则在给定\\(T=t\\)下，样本的条件密度函数为\n\\[\rf_\\theta(x_1,\\dots,x_n|T=t)=\\frac{\\prod_{i=1}^nf_\\theta(x_i) 1\\{T(x_1,\\dots,x_n)=t\\}}{f^T_\\theta(t)},\r\\]\n其中\\(f^T_\\theta(t)\\)为\\(T\\)的密度函数。\n\r\r思考题\r顺序统计量\\(X_{(1)},\\dots,X_{(n)}\\)是否充分统计量？\n\r连续分布族\r离散分布族\r\r\r因子分解定理\r\rJ. Neyman和P. R. Halmos在20世纪40年代提出\r判断充分统计量的法则\r\r定理：设样本的分布为\\(f_\\theta(x_1,\\dots,x_n)\\)（在离散总体情况下表示样本的分布列，在连续总体情况下表示样本的密度函数）。则在统计量\\(T\\)是充分的当且仅当存在两个函数满足\n\\(h(x_1,\\dots,x_n)\\)非负\r在统计量\\(T\\)取值空间上的函数\\(g_\\theta(t)\\), 使得\r\r\\[\rf_\\theta(x_1,\\dots,x_n) = g_\\theta(T(x_1,\\dots,x_n))h(x_1,\\dots,x_n),\\ \\forall\\theta\\in\\Theta, \\forall x_i\r\\]\n\r因子分解定理的应用\r例1：总体分布为\\(U(0,\\theta)\\), 求参数\\(\\theta\\)的充分统计量。\n例2：总体分布为\\(N(\\mu,\\sigma^2)\\)，求\n\r参数\\((\\mu,\\sigma^2)\\)的充分统计量\r当\\(\\sigma^2\\)已知时，\\(\\mu\\)的充分统计量\r当\\(\\mu\\)已知时，\\(\\sigma^2\\)的充分统计量\r\r\r指数型分布族的充分统计量\r指数型分布族下的样本分布为\n\\[f_\\theta(x_1,\\dots,x_n)=\\prod_{i=1}^np_\\theta(x_i)=c(\\theta)^n\\exp\\{\\sum_{j=1}^kc_j(\\theta)\\sum_{i=1}^nT_j(x_i)\\}\\prod_{i=1}^nh(x_i)\\]\n由因子分解定理知，参数\\(\\theta\\)的一个充分统计量为\n\\[\\left(\\sum_{i=1}^nT_1(x_i),\\dots,\\sum_{i=1}^nT_k(x_i)\\right).\\]\n\r充分统计量有无穷多个\r定理：如果\\(T\\)是充分统计量，且\\(T=\\psi(S)\\), 其中\\(\\psi\\)是可测函数，\\(S\\)是另一个统计量，则\\(S\\)也是充分统计量。\n例：总体分布为\\(N(\\mu,\\sigma^2)\\)，以下哪些统计量为参数\\((\\mu,\\sigma^2)\\)的充分统计量\n\r\\((\\bar X, S_n)\\)\r\\((\\bar X, S_n^2)\\)\r\\((\\bar X, S_n^*)\\)\r\\((\\bar X, S_n^{*2})\\)\r\\((\\sum_{i=1}^n X_i,\\sum_{i=1}^n X_i^2)\\)\r\\((\\sum_{i=1}^n X_i,\\sum_{i=1}^n |X_i|)\\)\r\\((\\sum_{i=1}^n X_i,\\sum_{i=1}^n |X_i|,\\sum_{i=1}^n X_i^2)\\)\r\r思考：哪种最好？最小充分统计量\n\r","date":1536537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536537600,"objectID":"40db368c820df74f37fe72f8e839cbac","permalink":"/post/chap01/","publishdate":"2018-09-10T00:00:00Z","relpermalink":"/post/chap01/","section":"post","summary":"第一章：绪论","tags":["课件","数理统计"],"title":"第一章：绪论","type":"post"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n","date":1536422400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536422400,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"/tutorial/example/","publishdate":"2018-09-09T00:00:00+08:00","relpermalink":"/tutorial/example/","section":"tutorial","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;","tags":null,"title":"Example Page","type":"docs"},{"authors":["Zhijian He"],"categories":null,"content":"","date":1535731200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535731200,"objectID":"b59f1e9783994e875d189d597d65f327","permalink":"/publication/qmcrate/","publishdate":"2018-09-01T00:00:00+08:00","relpermalink":"/publication/qmcrate/","section":"publication","summary":"This paper studies randomized quasi-Monte Carlo (QMC) sampling for discontinuous integrands having singularities along the boundary of the unit cube $[0,1]^d$","tags":[],"title":"Quasi-Monte Carlo for Discontinuous Integrands with Singularities along the Boundary of the Unit Cube","type":"publication"},{"authors":null,"categories":["slides"],"content":"2.1 Binomial models\rEstimating a probability from binomial data\r\rlet \\(\\theta\\) be the proportion of successes in the population\rthe data \\((y_1,\\dots,y_n)\\in \\{0,1\\}^n\\)\rthe total number of successes in the \\(n\\) trials is denoted by \\(y\\)\rthe binomial model is\r\\[p(y|\\theta) = C_n^y\\theta^y(1-\\theta)^{n-y}\\]\n\rthe posterior distribution is\r\\[p(\\theta|y) \\propto p(\\theta)p(y|\\theta)\\propto p(\\theta)\\theta^y(1-\\theta)^{n-y}\\]\n\r\rExample: estimating the probability of a female birth\n\rA total of 241,945 girls and 251,527 boys were born in Paris from 1745 to 1770.\r\r\rHow to choose a proper prior?\r\rA naive choice for \\(p(\\theta)\\) is uniform on the interval \\([0,1]\\). Then\r\\[p(\\theta|y) \\propto \\theta^y(1-\\theta)^{n-y}\\]\n\rthat is, \\(\\theta|y\\sim Beta(y+1,n-y+1)\\)\n\r\r- \\(P(\\theta\\ge 0.5|y=241945,n=241945+251527)\\approx 1.15\\times 10^{-42}\\)\n\rPrediction\r\rLet \\(\\tilde y\\) be the result of a new trial\r\\[P(\\tilde y =1|y) = \\int_0^1 P(\\tilde y=1|\\theta,y)p(\\theta|y)d \\theta=E[\\theta|y]=\\frac{y+1}{n+2}\\]\r\rPosterior as compromise between data and prior information\n\rprior mean is \\(1/2\\)\rsample mean is \\(y/n\\)\rposterior mean is \\((y+1)/(n+2)\\)\rthe compromise is controlled to a greater extent by the data as the sample size\rincreases.\r\r\rPosterior quantiles and intervals\r\rlet \\(T_1\\) be the \\(\\alpha/2\\) quantile of the posterior distribution\rlet \\(T_2\\) be the \\(1-\\alpha/2\\) quantile of the posterior distribution\r\\(100(1-\\alpha)\\%\\) posterior interval is \\([T_1,T_2]\\)\r\r\rcompare with the usual confidence interval\r\r\rInformative prior distributions\rGoal: assigning a prior distribution that reflects substantive info.\n\rthe likelihood is\r\\[p(y|\\theta) \\propto \\theta^y(1-\\theta)^{n-y}\\]\n\rchoose a prior as a \\(Beta(\\alpha,\\beta)\\) distribution:\r\\[p(\\theta) \\propto \\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\\]\rthe parameters \\(\\alpha,\\beta\u0026gt;0\\) of the prior distribution is called hyperparameters.\n\rthe posterior is\r\\[p(\\theta|y)\\propto \\theta^{\\alpha+y-1}(1-\\theta)^{n-y+\\beta-1}=Beta(\\alpha+y,\\beta+n-y)\\]\n\r\r\rInformative prior distributions\r\rthe posterior mean is\r\\[E[\\theta|y]=\\frac{\\alpha+y}{\\alpha+\\beta+n}\\]\rwhich lies between the sample proportion \\(y/n\\) and the prior mean \\(\\alpha/(\\alpha+\\beta)\\)\n\rthe posterior variance is\r\\[Var[\\theta|y]=\\frac{(\\alpha+y)(\\beta+n-y)}{(\\alpha+\\beta+n)^2(\\alpha+\\beta+n+1)}=\\frac{E[\\theta|y](1-E[\\theta|y])}{\\alpha+\\beta+n+1}\\]\n\ras \\(y\\) and \\(n\\) become large with fixed \\(\\alpha,\\beta\\),\r\\[E[\\theta|y]\\approx \\frac yn,\\ Var[\\theta|y]\\approx \\frac 1n\\frac yn(1-\\frac yn).\\]\n\r\r\rConjugate prior distributions\rDefinition: If \\(\\mathcal{F}\\) is a class of sampling distribution \\(p(y|\\theta)\\), and \\(\\mathcal{P}\\) is a class of prior distributions for \\(\\theta\\), then the class \\(\\mathcal{P}\\) is conjugate for \\(\\mathcal{F}\\) if\r\\[p(\\theta|y)\\in \\mathcal{P} \\text{ for all } p(\\cdot|\\theta)\\in\\mathcal{F} \\text{ and }p(\\cdot)\\in\\mathcal{P}.\\]\nAdvantages of conjugate prior distributions\n\rcomputational convenience\rcan be interpreted as additional data\r\r\rExponential families\rDefinition: The class \\(\\mathcal{F}\\) is an exponential family if all its members have the form\r\\[p(y_i|\\theta)=f(y_i)g(\\theta)\\exp[\\phi(\\theta)^\\top u(y_i)].\\]\n\r\\(f(\\cdot)\\ge 0\\)\r\\(\\phi(\\theta)\\) is called the natural parameter\r\rFor iid samples, we have\r\\[p(y|\\theta)=\\left(\\prod_{i=1}^n f(y_i)\\right)g(\\theta)^n\\exp\\left[\\phi(\\theta)^\\top \\sum_{i=1}^nu(y_i)\\right]\r\\]\r\\[p(y|\\theta)\\propto g(\\theta)^n\\exp[\\phi(\\theta)^\\top t(y)]\\]\n\rwhere \\(t(y)=\\sum_{i=1}^nu(y_i)\\) (i.e., a sufficient statistic for \\(\\theta\\)).\r\r\rConjugate prior distribution for exponential families\rIf the prior distribution is specified as\r\\[p(\\theta)\\propto g(\\theta)^\\eta \\exp[\\phi(\\theta)^\\top \\nu],\\]\rthen the posterior density is\r\\[p(\\theta|y)\\propto g(\\theta)^{\\eta+n} \\exp[\\phi(\\theta)^\\top (\\nu+t(y))].\\]\nA list of exponential families\n\rbinomial distributions\rnormal distributions\rexponential distributions\rpossion distributions\r\r\rExample: Probability of a girl birth given placenta previa\rAn early study concerning the sex of placenta previa births in Germany found that of a total of 980 births, 437 were female.\nHow much evidence does this provide for the claim that the proportion of female births in the population of placenta previa births is less than 0.485, the proportion of female births in the general population?\n\rusing a uniform prior: the posterior is \\(Beta(438,544)\\). The central \\(95\\%\\) posterior interval is \\([0.415,0.477]\\).\n\rusing conjugate prior \\(Beta(\\alpha,\\beta)\\)\rusing nonconjugate prior\n\r\r\rDifferent conjugate prior distributions\r\r\r\\(\\alpha/(\\alpha+\\beta)\\)\r\\(\\alpha+\\beta\\)\rposterior median\r\\(95\\%\\) posterior interval\r\r\r\r0.500\r2\r0.446\r[0.415, 0.477]\r\r0.485\r2\r0.446\r[0.415, 0.477]\r\r0.485\r5\r0.446\r[0.415, 0.477]\r\r0.485\r10\r0.446\r[0.415, 0.477]\r\r0.485\r20\r0.447\r[0.416, 0.478]\r\r0.485\r100\r0.450\r[0.420, 0.479]\r\r0.485\r200\r0.453\r[0.424, 0.481]\r\r\r\r\rPosterior inferences based on a large sample are not sensitive to the prior distribution.\rAll the \\(95\\%\\) posterior intervals exclude the prior mean.\r\r\rThe effect of prior distributions\r\rUsing a nonconjugate prior distribution\r\\(95\\%\\) posterior interval is [0.419, 0.480]\n\r\r2.2 Normal models\rEstimating a normal mean with known variance\rLikelihood function:\r\\[p(y|\\theta) = \\prod_{i=1}^n \\frac 1{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(y_i-\\theta)^2}{2\\sigma^2}}\\propto e^{-\\frac{n\\theta^2}{2\\sigma^2}}e^{\\frac{n\\theta\\bar y}{\\sigma^2}}\\]\nConjugate prior: \\(\\theta\\sim N(\\mu_0,\\tau_0^2)\\)\nPosterior distribution:\r\\[p(\\theta|y)\\propto e^{-\\frac{n\\theta^2}{2\\sigma^2}}e^{\\frac{n\\theta\\bar y}{\\sigma^2}}e^{-\\frac{\\theta^2}{2\\tau_0^2}}e^{\\frac{\\mu_0\\theta}{\\tau_0^2}}=N(\\mu_n,\\tau_n^2)\\]\nwhere\r\\[\\mu_n=\\frac{\\frac 1{\\tau_0^2}\\mu_0+\\frac n{\\sigma^2}\\bar y}{\\frac 1{\\tau_0^2}+\\frac n{\\sigma^2}},\\ \\frac1{\\tau_n^2}=\\frac{1}{\\tau_0^2}+\\frac n{\\sigma^2}.\\]\n\rComments\r\rthe inverse of the variance plays a prominet role and is called the precision\rposterior precision = prior precision + data precision\rthe posterior mean is expressed as a weighted average of the prior mean \\(\\mu_0\\) and the sample mean \\(\\bar y\\), with weights proportional to the precisions.\rwhat happens if \\(n\\to \\infty\\) with \\(\\tau_0^2\\) fixed? data info. dominated!\rwhat happens if \\(\\tau_0\\to \\infty\\) with \\(n\\) fixed? This would result from assuming \\(p(\\theta)\\) is proportional to a constant for \\(\\theta\\in(-\\infty,\\infty)\\). (improper prior, serves as an noninformative prior)\r\r\rNormal distribution with known mean but unknown variance\rLikelihood function:\r\\[p(y|\\sigma^2)=\\prod_{i=1}^n \\frac 1{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(y_i-\\theta)^2}{2\\sigma^2}}\\propto \\sigma^{-n}\\exp\\left[-\\frac{n}{2\\sigma^2}\\nu\\right]\\]\rwhere the sufficient statistic is\r\\[\\nu=\\frac 1n\\sum_{i=1}^n(y_i-\\theta)^2.\\]\rConjugate prior density:\r\\[p(\\sigma^2)\\propto (\\sigma^2)^{-(\\alpha+1)}e^{-\\beta/\\sigma^2},\\]\rwhere the hyperparameters is \\((\\alpha,\\beta)\\).\nWe may take \\(\\theta\\sim \\text{Inv-}\\chi^2(\\nu_0,\\sigma^2_0)\\) as a prior (i.e., \\(\\theta\\stackrel d {=}\\sigma_0^2\\nu_0/\\chi^2_{\\nu_0}\\)).\n\rNormal distribution with known mean but unknown variance\rPrior density:\r\\[p(\\sigma^2)= \\text{Inv-}\\chi^2(\\nu_0,\\sigma^2_0)\\]\nPosterior density:\r\\[p(\\sigma^2|y)=\\text{Inv-}\\chi^2\\left(\\nu_0+n,\\frac{\\nu_0\\sigma_0^2+n\\nu}{\\nu_0+n}\\right)\\]\n\rdegree of freedom = sum of the prior and data\rscale = weighted average of the prior and data\n\rif \\(\\nu_0=0\\), \\(p(\\sigma^2|y)=\\text{Inv-}\\chi^2(n,\\nu)\\), as effectively taking \\(p(\\sigma^2)\\propto 1/\\sigma^2\\) (improper prior, serves as an noninformative prior)\n\r\r\r\r2.3 Poisson models\rPoisson models\rApplications: The Possion distribution arises naturally in the study of data taking the form of counts.\n\rnumber of customer on the queue over an unit time\repidemiology – the incidence of diseases\r\rLikelihood function:\r\\[p(y|\\theta) = \\prod_{i=1}^n\\frac{\\theta^{y_i}e^{-\\theta}}{y_i!}\\propto \\theta^{t(y)}e^{-n\\theta}\\propto e^{-n\\theta}e^{t(y)\\log \\theta}\\]\n\r\\(t(y)=\\sum_{i=1}^n y_i\\) is the sufficient statistic\rthe natural parameter is \\(\\log \\theta\\)\r\rConjugate prior:\r\\[p(\\theta)\\propto e^{-\\eta\\theta}e^{\\nu\\log \\theta}\\]\nSo we may choose \\(p(\\theta)=Gamma(\\alpha,\\beta)\\propto \\theta^{\\alpha-1}e^{-\\beta\\theta}\\)\n\rPoisson models\rPrior density: \\(p(\\theta)=Gamma(\\alpha,\\beta)\\)\nPosterior density: \\(p(\\theta|y)=Gamma(\\alpha+n\\bar y,\\beta+n)\\)\nMarginal density:\r\\[p(y_i)=C_{\\alpha+y_i-1}^{y_i} \\left(\\frac{\\beta}{\\beta+1}\\right)^\\alpha\\left(\\frac{1}{\\beta+1}\\right)^{y_i}\\]\n\r\\(y_i\\sim \\text{Neg-bin}(\\alpha,\\beta)\\), i.e., the negative binomial distribution\r\r\rPossion models: an extension\rIn many applications, it is convenient to extend the Possion model for data pionts \\(y_1,\\dots,y_n\\) to the form\r\\[y_i\\sim Poission(x_i\\theta),\\]\n\rthe values \\(x_i\\) are known positive values of an explanatory variable \\(x\\), called the exposure of the \\(i\\)th unit\n\r\\(\\theta\\) is unknown, called the rate\n\r\rPrior density: \\(p(\\theta)=Gamma(\\alpha,\\beta)\\)\nPosterior density:\r\\[p(\\theta|y)=Gamma(\\alpha+\\sum_{i=1}^ny_i,\\beta+\\sum_{i=1}^nx_i)\\]\nExample: Bayesian inference for the cancer death rates (p.48)\n\r\r2.4 Exponential models\rExponential models\rApplications: The expoential distribution is commonly used to model ‘waiting times’ and other continuous, poisitive, real-valued random variables. It has a ‘memoryless’ property that makes it a natural model for survival or lifetime data.\nLikelihood function:\r\\[p(y|\\theta) = \\prod_{i=1}^n\\theta \\exp(-y_i\\theta)= \\theta^{n}e^{-n\\bar y \\theta}\\]\nPrior density: \\(p(\\theta)=Gamma(\\alpha,\\beta)\\)\nPosterior density:\r\\[p(\\theta|y)=Gamma(\\alpha+n,\\beta+n\\bar y)\\]\n\rSummary\r\r\rPopulation\rParameter\rConjugate prior\r\r\r\rBinomial\rprobability of success\rBeta dist.\r\rPossion\rmean\rGamma dist.\r\rExponential\rinverse mean\rGamma dist.\r\rNormal (known variance)\rmean\rNormal dist.\r\rNormal (known mean)\rvariance\rInv-Gamma dist.\r\r\r\r\rEnd notes\r\rtwo kinds of prior distributions: uniform (noninformative) and conjugate (informative)\n\rsome other noninformative prior distributions: Jeffreys’ prior etc. See pp.52-56\n\rnoninformative prior are often useful when it does not seem to be worth the effort to quantify one’s real prior knowledge as a probability distribution\n\rwhen using conjugate prior, it remains to choose the hyperparameters; see Chapter 5 for hierarchical models\n\r\r\r\r","date":1535587200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535587200,"objectID":"aecfc5d2573d910883e89f314487eb32","permalink":"/post/bayes_chap02/","publishdate":"2018-08-30T00:00:00Z","relpermalink":"/post/bayes_chap02/","section":"post","summary":"Chapter 2: Single-parameter models","tags":["slides","BDA"],"title":"Chapter 2: Single-parameter models","type":"post"},{"authors":null,"categories":["slides"],"content":"3 steps in BDA\r\rset up the statistical model\n\rcompute the posterior distribution\n\rmodel checking and model improvement\n\r\r\rStatistical inference\rGoal: draw conclusions about unobserved quantities from the data (observed)\n\rpotentially observable quantities, e.g., future observations of a process\n\rnot directly observable quantities, e.g., unobservable population parameters\n\r\r\rNotations and assumptions\r\runobservable population parameters of interest: \\(\\theta=(\\theta_1,\\dots,\\theta_m)\\)\n\rthe observed data: \\(y=(y_1,\\dots,y_n)\\)\n\rpotentially observable quantities: \\(\\tilde y\\)\n\r\rAssumption 1\n\rexchangeability: the \\(n\\) values \\(y_i\\) are exchangeable, e.g., iid samples.\r\rAssumption 2\n\rconditional independence of \\(y\\) and \\(\\tilde y\\) given \\(\\theta\\)\r\r\rBayesian inference\rTo make inferences about the posterior distributions, such as \\(p(\\theta|y)\\) and \\(p(\\tilde y|y)\\)\nBayes’ rule\n\\[p(\\theta|y)=\\frac{p(\\theta,y)}{p(y)}=\\frac{p(y|\\theta)p(\\theta)}{p(y)}\\]\n\\[p(\\theta|y)\\propto p(y|\\theta)p(\\theta)\\]\nThe imiplied constant is\r\\[p(y)=\\int p(y|\\theta)p(\\theta) d \\theta.\\]\n\rPrediction\rTo make inferences about an unknown observable quantity\n\rprior predictive distribution: \\(p(y)\\)\n\rposterior predictive dsitribution: \\(p(\\tilde y|y)\\)\n\r\r\\[\rp(\\tilde y|y) = \\int p(\\tilde y,\\theta|y)d\\theta = \\int p(\\tilde y|\\theta,y)p(\\theta|y)d \\theta = \\int p(\\tilde y|\\theta)p(\\theta|y)d \\theta\r\\]\nAgain, \\(y\\) and \\(\\tilde y\\) are conditionally independent given \\(\\theta\\).\n\rLikelihood\r\\(p(y|\\theta)\\) is called the likelihood function, which is regarded as a function of \\(\\theta\\).\nodds ratios\n\\[\\frac{p(\\theta_1|y)}{p(\\theta_2|y)}=\\frac{p(\\theta_1)p(y|\\theta_1)/p(y)}{p(\\theta_2)p(y|\\theta_2)/p(y)}=\\frac{p(\\theta_1)}{p(\\theta_2)}\\frac{p(y|\\theta_1)}{p(y|\\theta_2)}\\]\nposterior odds = prior odds \\(\\times\\) likelihood ratio\n\rExample 1: inference about a genetic status\r\rmales: one X-chromosome + one Y-chromosome\rfemales: two X-chromosomes\r\rHemophilia is a disease that exhibits X-chromosome-linked recessive inheritance.\rThe disease is generally fatal for women who inherit two such genes.\nConsider a woman who has an affected brother and her father is not affected.\rLet \\(\\theta\\) be the state of the woman: a carrier of the gene (\\(\\theta=1\\)) or not (\\(\\theta=0\\)).\nPrior distribution: \\(P(\\theta=1)=P(\\theta=0)=0.5\\)\nData and model: She has two sons. Let \\(y_i=1\\) or 0 denote the state of her sons. Now observe that her sons are not affected. Given \\(\\theta\\), \\(y_1\\) and \\(y_2\\) are iid.\n\rExample 1: inference about a genetic status\rLikelihood function:\n\\[P(y_1=0,y_2=0|\\theta=1)=0.5\\times 0.5=0.25\\]\n\\[P(y_1=0,y_2=0|\\theta=0)=1\\times 1=1\\]\nPosterior distribution:\n\\[P(\\theta=1|y) = \\frac{p(y|\\theta=1)p(\\theta=1)}{p(y)}=0.2\\]\n\rExample 1: inference about a genetic status\rAdding more data: suppose that the woman has a third son, who is also unaffacted.\n\\[P(\\theta=1|y_1,y_2,y_3) = \\frac{0.5\\times 0.2}{0.5\\times 0.2+1\\times 0.8}=0.111\\]\nA key aspect of Bayesian analysis is the ease with which sequential analyses can be performed.\nQuestion: What happen if we suppose that the third son is affected?\n\rExample 2: spelling correction\rClassification of words is a problem of managing uncertainty. Suppose someone types radom. How should that be read?\n\rrandom\rradon\rradom\r\rData and model: Let \\(\\theta\\) be the word that the person was intending to type, and let \\(y\\) as the data. Now \\(y=\\)’radom’ and \\(\\theta\\in\\){\\(\\theta_1\\)=‘random’,\\(\\theta_2\\)=‘radon’,\\(\\theta_3\\)=‘radom’}. The posterior density is\n\\[P(\\theta|y=\\text{\u0026#39;radom\u0026#39;})\\propto p(\\theta)P(y=\\text{\u0026#39;radom\u0026#39;}|\\theta).\\]\n\rExample 2: spelling correction\rPrior distribution: Here are probabilities supplied by researchers at Google.\rGoole Ngram Viewer: https://books.google.com/ngrams\n\r\r\\(\\theta\\)\r\\(p(\\theta)\\)\r\r\r\rrandom\r\\(7.60\\times 10^{-5}\\)\r\rradon\r\\(6.05\\times 10^{-6}\\)\r\rradom\r\\(3.12\\times 10^{-7}\\)\r\r\r\rLikelihood: Here are some conditional probabilities from Google’s model of spelling and typing errors:\n\r\r\\(\\theta\\)\r\\(p(\\text{\u0026#39;radom\u0026#39;}|\\theta)\\)\r\r\r\rrandom\r\\(0.00193\\)\r\rradon\r\\(0.000143\\)\r\rradom\r\\(0.975\\)\r\r\r\r\rExample 2: spelling correction\rPosterior distribution:\n\r\r\\(\\theta\\)\r\\(p(\\theta)P(y=\\text{\u0026#39;radom\u0026#39;}|\\theta)\\)\r\\(P(\\theta|y=\\text{\u0026#39;radom\u0026#39;})\\)\r\r\r\rrandom\r\\(1.47\\times 10^{-7}\\)\r0.325\r\rradon\r\\(8.65\\times 10^{-10}\\)\r0.002\r\rradom\r\\(3.04\\times 10^{-7}\\)\r0.673\r\r\r\rModel improvement:\n\rincluding contextual info in the prior probabilities, e.g., statistical book.\rlet \\(x\\) be the contextual information used by the model.\r\r\\[p(\\theta|x,y)\\propto p(\\theta|x)p(y|\\theta,x)\\]\n\rfor simplicity, we may assume \\(p(y|\\theta,x)=p(y|\\theta)\\).\r\r\r","date":1535500800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535500800,"objectID":"5cd976182ee1b65b387d3ee03dc3fae5","permalink":"/post/bayes_chap01/","publishdate":"2018-08-29T00:00:00Z","relpermalink":"/post/bayes_chap01/","section":"post","summary":"Chapter 1: Probability and Inference","tags":["slides","BDA"],"title":"Chapter 1: Probability and Inference","type":"post"},{"authors":["Zhijian He"],"categories":null,"content":"","date":1530374400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530374400,"objectID":"02a265cfa737c3778af3360a69cdb213","permalink":"/publication/2018hilbert/","publishdate":"2018-07-01T00:00:00+08:00","relpermalink":"/publication/2018hilbert/","section":"publication","summary":"This paper studies the asymptotic normality of the Hilbert's space filling curve based estimate when using scrambled van der Corput sequence as input.","tags":[],"title":"Asymptotic Normality of Extensible Grid Sampling","type":"publication"},{"authors":["Chengfeng Weng","Xiaoqun Wang","Zhijian He"],"categories":null,"content":"","date":1498838400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498838400,"objectID":"4f278db3cb0b9a02f0396b31ec421e92","permalink":"/publication/2016siam/","publishdate":"2017-07-01T00:00:00+08:00","relpermalink":"/publication/2016siam/","section":"publication","summary":"Discontinuities and high dimensionality are common in the problems of pricing and hedging of derivative securities. Both factors have a tremendous impact on the accuracy of the quasi--Monte Carlo (QMC) method. An ideal approach to improve the QMC method is to transform the functions to make them smoother and having smaller effective dimension. This paper develops a two-step procedure to tackle the challenging problems of both the discontinuity and the high dimensionality concurrently. In the first step, we adopt the smoothing method to remove the discontinuities of the payoff function, improving the smoothness. In the second step, we propose a general dimension reduction method (called the CQR method) to reduce the effective dimension such that the better quality of QMC points in their initial dimensions can be fully exploited. The CQR method is based on the combination of the k-means clustering algorithm and the QR decomposition. The k-means clustering algorithm, a classical algorithm of machine learning, is used to find some representative linear structures inherent in the function, which are used to construct a matching function of the smoothed function. The matching function serves as an approximation of the smoothed function but has a simpler form, and it is used to find the required transformation. Extensive numerical experiments on option pricing and Greeks estimation demonstrate that the combination of the smoothing method and dimension reduction in QMC achieves substantially larger variance reduction even in high dimension than dealing with either discontinuities or high dimensionality single sidedly.","tags":["QMC","Simulation"],"title":"An Auto-Realignment Method in Quasi-Monte Carlo for Pricing Financial Derivatives with Jump Structures","type":"publication"},{"authors":[],"categories":null,"content":"Embed your slides or video here using shortcodes. Further details can easily be added using Markdown and $\\rm \\LaTeX$ math code.\n","date":1483200000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483200000,"objectID":"cd6d9d084287506b4668ad90c6aff50a","permalink":"/talk/example-talk/","publishdate":"2017-01-01T00:00:00+08:00","relpermalink":"/talk/example-talk/","section":"talk","summary":"Embed your slides or video here using shortcodes. Further details can easily be added using Markdown and $\\rm \\LaTeX$ math code.","tags":[],"title":"Example Talk","type":"talk"},{"authors":["Chengfeng Weng","Xiaoqun Wang","Zhijian He"],"categories":null,"content":"","date":1469980800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1469980800,"objectID":"8e3f0fa14d3b10dcb770850a5db1322c","permalink":"/publication/2016ejor/","publishdate":"2016-08-01T00:00:00+08:00","relpermalink":"/publication/2016ejor/","section":"publication","summary":"Discontinuities are common in the pricing of financial derivatives and have a tremendous impact on the accuracy of quasi-Monte Carlo (QMC) method. While if the discontinuities are parallel to the axes, good efficiency of the QMC method can still be expected. By realigning the discontinuities to be axes-parallel, [Wang \u0026 Tan, 2013] succeeded in recovering the high efficiency of the QMC method for a special class of functions. Motivated by this work, we propose an auto-realignment method to deal with more general discontinuous functions. The k-means clustering algorithm, a classical algorithm of machine learning, is used to select the most representative normal vectors of the discontinuity surface. By applying this new method, the discontinuities of the resulting function are realigned to be friendly for the QMC method. Numerical experiments demonstrate that the proposed method significantly improves the performance of the QMC method.","tags":["QMC","Simulation"],"title":"An Auto-Realignment Method in Quasi-Monte Carlo for Pricing Financial Derivatives with Jump Structures","type":"publication"},{"authors":["Zhijian He","Art B. Owen"],"categories":null,"content":"","date":1467302400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1467302400,"objectID":"037e29574a17605616eb974fe1416966","permalink":"/publication/hilbert/","publishdate":"2016-07-01T00:00:00+08:00","relpermalink":"/publication/hilbert/","section":"publication","summary":"We study the properties of points in $[0,1]^d$ generated by applying Hilbert's space-filling curve to uniformly distributed points in $[0,1]$.","tags":[],"title":"Extensible Grids: Uniform Sampling on a Space-Filling Curve","type":"publication"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461686400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461686400,"objectID":"80be3c9fcc86014efab0cec0f14957f6","permalink":"/project/deep-learning/","publishdate":"2016-04-27T00:00:00+08:00","relpermalink":"/project/deep-learning/","section":"project","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit.","tags":["Deep Learning"],"title":"Deep Learning","type":"project"},{"authors":null,"categories":null,"content":"","date":1461686400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461686400,"objectID":"553a94c5dfd3b8b099d8a12b2d248093","permalink":"/project/example-external-project/","publishdate":"2016-04-27T00:00:00+08:00","relpermalink":"/project/example-external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":[],"categories":null,"content":" Academic is a framework to help you create a beautiful website quickly. Perfect for personal sites, blogs, or business/project sites. Check out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes. Then head on over to the Quick Start guide or take a look at the Release Notes.\n$$a=b^2$$\n\nKey features:\n Easily manage various content including homepage, blog posts, publications, talks, and projects Extensible via color themes and widgets/plugins Write in Markdown for easy formatting and code highlighting, with LaTeX for mathematical expressions Social/academic network linking, Google Analytics, and Disqus comments Responsive and mobile friendly Simple and refreshing one page design Multilingual and easy to customize  Color Themes Academic is available in different color themes and font themes.\n       Install You can choose from one of the following four methods to install:\n one-click install using your web browser (recommended) install on your computer using Git with the Command Prompt/Terminal app install on your computer by downloading the ZIP files install on your computer with RStudio  Quick install using your web browser  Install Academic with Netlify  Netlify will provide you with a customizable URL to access your new site  On GitHub, go to your newly created academic-kickstart repository and edit config.toml to personalize your site. Shortly after saving the file, your site will automatically update Read the Quick Start Guide to learn how to add Markdown content. For inspiration, refer to the Markdown content which powers the Demo  Install with Git Prerequisites:\n Download and install Git Download and install Hugo   Fork the Academic Kickstart repository and clone your fork with Git:\ngit clone https://github.com/sourcethemes/academic-kickstart.git My_Website  Note that if you forked Academic Kickstart, the above command should be edited to clone your fork, i.e. replace sourcethemes with your GitHub username.\n Initialize the theme:\ncd My_Website git submodule update --init --recursive   Install with ZIP  Download and extract Academic Kickstart Download and extract the Academic theme to the themes/academic/ folder from the above step  Install with RStudio View the guide to installing Academic with RStudio\nQuick start  If you installed on your computer, view your new website by running the following command:\nhugo server  Now visit localhost:1313 and your new Academic powered website will appear. Otherwise, if using Netlify, they will provide you with your URL.\n Read the Quick Start Guide to learn how to add Markdown content, customize your site, and deploy it. For inspiration, refer to the Markdown content which powers the Demo\n Build your site by running the hugo command. Then host it for free using Github Pages or Netlify (refer to the first installation method). Alternatively, copy the generated public/ directory (by FTP, Rsync, etc.) to your production web server (such as a university\u0026rsquo;s hosting service).\n  Updating Feel free to star the project on Github to help keep track of updates and check out the release notes prior to updating your site.\nBefore updating the framework, it is recommended to make a backup of your entire website directory (or at least your themes/academic directory) and record your current version number.\nBy default, Academic is installed as a Git submodule which can be updated by running the following command:\ngit submodule update --remote --merge  Check out the update guide for full instructions and alternative methods.\nFeedback \u0026amp; Contributing Please use the issue tracker to let me know about any bugs or feature requests, or alternatively make a pull request.\nFor support, head over to the Hugo discussion forum.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1461081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1484236800,"objectID":"ba6423d815d4f5949b7a69912feb741d","permalink":"/post/getting-started/","publishdate":"2016-04-20T00:00:00+08:00","relpermalink":"/post/getting-started/","section":"post","summary":"Create a beautifully simple website or blog in under 10 minutes.","tags":["Academic"],"title":"Academic: the website designer for Hugo","type":"post"},{"authors":null,"categories":["R"],"content":"R Markdown\rThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nYou can embed an R code chunk like this:\nsummary(cars)\r## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00\rfit \u0026lt;- lm(dist ~ speed, data = cars)\rfit\r## ## Call:\r## lm(formula = dist ~ speed, data = cars)\r## ## Coefficients:\r## (Intercept) speed ## -17.579 3.932\r\rIncluding Plots\rYou can also embed plots. See Figure 1 for example:\npar(mar = c(0, 1, 0, 1))\rpie(\rc(280, 60, 20),\rc(\u0026#39;Sky\u0026#39;, \u0026#39;Sunny side of pyramid\u0026#39;, \u0026#39;Shady side of pyramid\u0026#39;),\rcol = c(\u0026#39;#0292D8\u0026#39;, \u0026#39;#F7EA39\u0026#39;, \u0026#39;#C4B632\u0026#39;),\rinit.angle = -50, border = NA\r)\r\rFigure 1: A fancy pie chart.\r\r\r","date":1437703994,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1437703994,"objectID":"10065deaa3098b0da91b78b48d0efc71","permalink":"/post/2015-07-23-r-rmarkdown/","publishdate":"2015-07-23T21:13:14-05:00","relpermalink":"/post/2015-07-23-r-rmarkdown/","section":"post","summary":"R Markdown\rThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nYou can embed an R code chunk like this:\nsummary(cars)\r## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.","tags":["R Markdown","plot","regression"],"title":"Hello R Markdown","type":"post"},{"authors":["Zhijian He","Xiaoqun Wang"],"categories":null,"content":"","date":1435680000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1435680000,"objectID":"8c3bcb1de2052d2f3d7ab2d06dfd43a7","permalink":"/publication/2015siam2/","publishdate":"2015-07-01T00:00:00+08:00","relpermalink":"/publication/2015siam2/","section":"publication","summary":"The quasi-Monte Carlo (QMC) method is a powerful numerical tool for pricing complex derivative securities, whose accuracy is affected by the smoothness of the integrands. The payoff functions of many financial derivatives involve two types of nonsmooth factors: an indicator function (called jump structure) and a positive part of a smooth function (called kink structure). This paper develops a good path generation method (PGM) for recovering the superiority of the QMC method on problems involving multiple such structures. This is achieved by realigning these structures such that the associated nonsmooth surfaces are parallel to as many coordinate axes as possible. The proposed method has the advantage of addressing different structures according to their importance. We also offer a systematic measurement of different structures for quantifying and then ranking their importance. Numerical experiments demonstrate that the proposed method is more efficient than traditional PGMs for pricing exotic options, such as straddle Asian options, digital options, and barrier options. The numerical results confirm that both the jumps and kinks have tremendous impacts on the performance of the QMC method.","tags":["QMC","Simulation"],"title":"On the Convergence Rate of Randomized Quasi-Monte Carlo for Discontinuous Functions","type":"publication"},{"authors":["Zhijian He","Xiaoqun Wang"],"categories":null,"content":"","date":1404144000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1404144000,"objectID":"2bae107ebec90b76521d10f2627ca737","permalink":"/publication/2015siam/","publishdate":"2014-07-01T00:00:00+08:00","relpermalink":"/publication/2015siam/","section":"publication","summary":"The quasi-Monte Carlo (QMC) method is a powerful numerical tool for pricing complex derivative securities, whose accuracy is affected by the smoothness of the integrands. The payoff functions of many financial derivatives involve two types of nonsmooth factors: an indicator function (called jump structure) and a positive part of a smooth function (called kink structure). This paper develops a good path generation method (PGM) for recovering the superiority of the QMC method on problems involving multiple such structures. This is achieved by realigning these structures such that the associated nonsmooth surfaces are parallel to as many coordinate axes as possible. The proposed method has the advantage of addressing different structures according to their importance. We also offer a systematic measurement of different structures for quantifying and then ranking their importance. Numerical experiments demonstrate that the proposed method is more efficient than traditional PGMs for pricing exotic options, such as straddle Asian options, digital options, and barrier options. The numerical results confirm that both the jumps and kinks have tremendous impacts on the performance of the QMC method.","tags":["QMC","Simulation"],"title":"Good Path Generation Methods in Quasi-Monte Carlo for Pricing Financial Derivatives","type":"publication"},{"authors":null,"categories":["课件"],"content":"教师简介\r\r\r姓名\r何志坚\r\r\r\r职称\r数学学院副教授\r\r研究兴趣\r统计模拟与计算、随机算法、金融工程\r\r办公地址\r五山校区四号楼4301\r\r个人主页\r主页链接\r\r邮箱\rhezhijian@scut.edu.cn\r\r\r\r\r教材\r主要教材\r\r数理统计学讲义（第3版），陈家鼎、孙山泽、李东风、刘力平编著，高等教育出版社。\r\r\r参考教材\r\r数理统计学（第2版），茆诗松、吕晓玲编著，中国人民大学出版社。\r高等数理统计学，陈希孺编著，中国科技大学出版社。\rIntroduction to Mathematical Statistics (Fifth Edition), R. V. Hogg, A. T. Craig. （高等教育出版社，影印版）\rMathematical Statistics (Second Edition), Jun Shao. （世界图书出版公司，影印版）\r\r\r\r教学安排\r上课时间\r\r1-16周，周一第三、四节，周三第一、二节，共64学时\r\r\r上课地点\r\r博学101\r\r\r教学内容\r\r《数理统计学讲义》第1-5章\r《数理统计学》第1章\r\r\r\r教学日历\r\r\r内容\r学时\r要求\r\r\r\r第一章 绪论\r6\r了解统计学的发展以及基本概念\r\r第二章 估计\r12\r熟练掌握各种估计方法\r\r第三章 假设检验\r16\r熟练掌握各种假设检验方法\r\r第四章 回归分析\r16\r熟练掌握线性模型的建立与分析的技巧\r\r第五章 方差分析\r12\r掌握方差分析的技巧\r\r复习\r2\r考试重点复习\r\r\r\r\r统计软件\r本课程使用\r\rR软件: https://www.r-project.org/\n\rRStudio编辑器: https://www.rstudio.com/\n\r\r\rR软件参考资料\r\rRudy Angeles’ R tutorial\rR语言实用教程，薛毅、陈立萍编著，清华大学出版社。\r\r\r其它选择\r\rMatlab\rSAS\rPython\rC/C++\rStata\r\r\r\r数据科学中常用软件的比较\r\r考核方式\r最终成绩 = 平时成绩（30%）+期末成绩（70%）\r平时成绩\r\r作业（20%），其中2-3次大作业（案例分析）\r课堂表现（5%）\r考勤（5%）\r\r\r期末考试\r\r闭卷\r百分制\r\r\r\r课程管理\r雨课堂APP管理平台\r\r发布课程通知（课件、参考资料、作业等）\r课堂互动（课堂小测、提问）\r收集作业（只接受电子版作业：手写拍照/word/Rmarkdown/Latex）\r\r具体功能介绍见雨课堂网址：http://ykt.io/\n推荐使用Rmarkdown写作：https://rmarkdown.rstudio.com/\n\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2b5fba1b94ec70ec28f099e794e1eaa7","permalink":"/post/chap00/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/chap00/","section":"post","summary":"《数理统计》课程介绍","tags":["课件","数理统计"],"title":"《数理统计》课程简介","type":"post"}]