<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dr. Zhijian He on Dr. Zhijian He</title>
    <link>/</link>
    <description>Recent content in Dr. Zhijian He on Dr. Zhijian He</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Zhijian He &amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +0800</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Approximate Bayesian Computation</title>
      <link>/course/abc/</link>
      <pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/course/abc/</guid>
      <description>&lt;div id=&#34;bayesian-inference&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bayesian inference&lt;/h2&gt;
&lt;p&gt;Our goal is to estimate an expectation of &lt;span class=&#34;math inline&#34;&gt;\(a(\theta)\)&lt;/span&gt; under the posterior distribution&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\pi(\theta|y_{obs})=\frac{\pi(\theta)p(y_{obs}|\theta)}{p(y_{obs})}\propto \pi(\theta)p(y_{obs}|\theta)\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\pi(\theta)\)&lt;/span&gt; is the prior distribution&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(p(y|\theta)\)&lt;/span&gt; is the likelihood function&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the constant &lt;span class=&#34;math inline&#34;&gt;\(p(y_{obs})\)&lt;/span&gt; is intractable&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Suppose that one can generate samples &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(1)},\dots,\theta^{(N)}\sim \pi(\theta|y_{obs})\)&lt;/span&gt;, then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[E[a(\theta)|y_{obs}]\approx \frac 1 N\sum_{i=1}^N a(\theta^{(i)})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is the basic idea of Monte Carlo (MC).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;acceptance-rejection-ar-methods&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Acceptance rejection (AR) methods&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the target density &lt;span class=&#34;math inline&#34;&gt;\(f(\theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the proposal &lt;span class=&#34;math inline&#34;&gt;\(g(\theta)\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(g(\theta)&amp;gt;0\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(f(\theta)&amp;gt;0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(K\ge \sup_{\theta}\frac{f(\theta)}{g(\theta)}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The algorithm goes below, for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\dots,N\)&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Step 1: generate &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(i)}\sim g(\theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 2: accept &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(i)}\)&lt;/span&gt; with probability &lt;span class=&#34;math inline&#34;&gt;\(r(\theta^{(i)})=\frac{f(\theta^{(i)})}{Kg(\theta^{(i)})}\)&lt;/span&gt;. Else go to Step 1.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To fit into Bayesian inference, let &lt;span class=&#34;math inline&#34;&gt;\(f(\theta)=\pi(\theta)p(y_{obs}|\theta)\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the proposal &lt;span class=&#34;math inline&#34;&gt;\(g(\theta)=\pi(\theta)\)&lt;/span&gt;, the acceptance probability is the likelihood &lt;span class=&#34;math inline&#34;&gt;\(r(\theta)=p(y_{obs}|\theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;generally, the acceptance probablity is
&lt;span class=&#34;math display&#34;&gt;\[r(\theta)=\frac{\pi(\theta)}{Kg(\theta)}p(y_{obs}|\theta),\ K=\sup_{\theta}\frac{\pi(\theta)}{g(\theta)}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;importance-sampling-is&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Importance sampling (IS)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the target density &lt;span class=&#34;math inline&#34;&gt;\(f(\theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the proposal &lt;span class=&#34;math inline&#34;&gt;\(g(\theta)\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(g(\theta)&amp;gt;0\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(f(\theta)&amp;gt;0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The algorithm goes below, for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\dots,N\)&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Step 1: generate &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(i)}\sim g(\theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 2: compute the weight &lt;span class=&#34;math inline&#34;&gt;\(w(\theta^{(i)})=\frac{f(\theta^{(i)})}{g(\theta^{(i)})}\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Return the weight samples &lt;span class=&#34;math inline&#34;&gt;\((w(\theta^{(i)}),\theta^{(i)}), i=1,\dots,N\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To fit into Bayesian inference, let &lt;span class=&#34;math inline&#34;&gt;\(f(\theta)=\pi(\theta)p(y_{obs}|\theta)\)&lt;/span&gt;, then the self-normalized IS estimator is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[E[a(\theta)|y_{obs}]\approx \frac{\frac 1 N\sum_{i=1}^N w(\theta^{(i)})a(\theta^{(i)})}{\frac 1 N\sum_{i=1}^N w(\theta^{(i)})}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mcmc&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;MCMC&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the target density &lt;span class=&#34;math inline&#34;&gt;\(f(\theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the proposal distribution: &lt;span class=&#34;math inline&#34;&gt;\(q(\theta&amp;#39;|\theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The algorithm goes below:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;for &lt;span class=&#34;math inline&#34;&gt;\(i=0\)&lt;/span&gt;, draw a starting poing &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(0)}\sim f_0(x)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\dots,N\)&lt;/span&gt;, sample &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(i)}\sim q(\theta&amp;#39;|\theta^{(i-1)})\)&lt;/span&gt;, accept &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(i)}\)&lt;/span&gt; with probability
&lt;span class=&#34;math display&#34;&gt;\[r(\theta^{(i-1)},\theta^{(i)})=\min\left(\frac{f(\theta^{(i)})q(\theta^{(i-1)}|\theta^{(i)})}{f(\theta^{(i-1)})q(\theta^{(i)}|\theta^{(i-1)})},1\right)\]&lt;/span&gt;
Otherwise, taking &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(i)}=\theta^{(i-1)}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To fit into Bayesian inference, let &lt;span class=&#34;math inline&#34;&gt;\(f(\theta)=\pi(\theta)p(y_{obs}|\theta)\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the acceptance probablity is
&lt;span class=&#34;math display&#34;&gt;\[r(\theta,\theta&amp;#39;)=\min\left(\frac{p(y_{obs}|\theta&amp;#39;)q(\theta|\theta&amp;#39;)}{p(y_{obs}|\theta)q(\theta&amp;#39;|\theta)},1\right)\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;motivation-for-abc&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Motivation for ABC&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Both the AR and MCMC require the evaluation of likelihood function.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;However, for an increasing range of scientific problems, numerical evaluation of the likelihood function is
either &lt;strong&gt;computationally prohibitive&lt;/strong&gt;, or simply &lt;strong&gt;not possible&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Instances when the complete likelihood function is
unavailable can occur when the model density function is only implicitly
defined, for example, through quantile or characteristic functions&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;a-g-and-k-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A g-and-k distribution&lt;/h2&gt;
&lt;p&gt;The univariate g-and-k distribution is a flexible unimodal distribution that
is able to describe data with significant amounts of skewness and kurtosis. Its density function has no closed form, but
is alternatively defined through its quantile function as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Q(q|A,B,g,k)=A+B\left[1+c\frac{1-\exp\{-gz(q)\}}{1+\exp\{-gz(q)\}}\right](1+z(q)^2)^kz(q)\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(c=0.8,\ B&amp;gt;0, k&amp;gt;-1/2\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(z(q)=\Phi^{-1}(q)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;if &lt;span class=&#34;math inline&#34;&gt;\(g=k=0\)&lt;/span&gt;, it is the normal density&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;likelihood-free-rejection-sampling-i&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Likelihood-free rejection sampling I&lt;/h2&gt;
&lt;p&gt;While direct evaluation of the
acceptance probability is not available if the likelihood is computationally intractable,
it is possible to stochastically determine whether or not to accept
or reject a draw from the sampling density, &lt;em&gt;without&lt;/em&gt; numerical evaluation of
the acceptance probability.&lt;/p&gt;
&lt;p&gt;Assume that the data &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; are discrete. The algorithm goes below, for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\dots,N\)&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Step 1: generate &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(i)}\sim g(\theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 2: generate &lt;span class=&#34;math inline&#34;&gt;\(y\sim p(y|\theta^{(i)})\)&lt;/span&gt; from the likelihood&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 3: if &lt;span class=&#34;math inline&#34;&gt;\(y=y_{obs}\)&lt;/span&gt;, then accept &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(i)}\)&lt;/span&gt; with probability &lt;span class=&#34;math inline&#34;&gt;\(r(\theta^{(i)})=\frac{\pi(\theta^{(i)})}{Kg(\theta^{(i)})}\)&lt;/span&gt;. Else go to Step 1.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;stereological-extremes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Stereological extremes&lt;/h2&gt;
&lt;p&gt;Interest is in the distribution of the size of inclusions, microscopically small
particles introduced during the production of steel. The steel strength is
thought to be directly related to the size of the largest inclusion. Commonly,
the sampling of inclusions involves measuring the maximum cross-sectional diameter
of each observed inclusion, &lt;span class=&#34;math inline&#34;&gt;\(y_{obs}=(y_{obs,1},\dots,y_{obs,n})\)&lt;/span&gt;, obtained from a
two-dimensional planar slice through the steel block.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;unobserved true inclusion diameter &lt;span class=&#34;math inline&#34;&gt;\(V_i\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(y_{obs,i}\le V_i\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;inclusions were &lt;strong&gt;spherical&lt;/strong&gt; with diameters &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; follows a generalised Pareto distribution
&lt;span class=&#34;math display&#34;&gt;\[P(V\le v|V&amp;gt;v_0)=1-\max\{1+\xi(v-v_0)/\sigma,0\}^{-1/\xi}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the centres followed a homogeneous Poisson process
with rate &lt;span class=&#34;math inline&#34;&gt;\(\lambda&amp;gt;0\)&lt;/span&gt; in the volume of steel&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the parameter is &lt;span class=&#34;math inline&#34;&gt;\(\theta=(\lambda,\sigma,\xi)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;stereological-extremes-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Stereological extremes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Anderson and Coles (2002) were able to construct a tractable likelihood
function for this model. However, while their model assumptions of a Poisson
process are not unreasonable, the assumption that the inclusions are spherical
is not plausible in practice.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Bortot et al. (2007) generalised this model to a family of &lt;strong&gt;ellipsoidal
inclusions&lt;/strong&gt;. While this model is more realistic than the spherical inclusion
model, there are &lt;strong&gt;analytic and computational difficulties&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Analysis&lt;/h2&gt;
&lt;p&gt;Consider the simple case &lt;span class=&#34;math inline&#34;&gt;\(\sigma=1.5,\xi = 0.1\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; unknown. Let &lt;span class=&#34;math inline&#34;&gt;\(n_{obs}\)&lt;/span&gt; be the observed
number of inclusions, so that &lt;span class=&#34;math inline&#34;&gt;\(p(\theta|y_{obs})=p(\lambda|n_{obs})\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;prior density &lt;span class=&#34;math inline&#34;&gt;\(\pi(\lambda)\sim U(0,100)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the data &lt;span class=&#34;math inline&#34;&gt;\(n_{obs}\in \{92,102,112,122,132\}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Acceptance rates for the right panel are &lt;span class=&#34;math inline&#34;&gt;\(0.5\%,10.5\%,20.5\%\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;extremes.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;likelihood-free-rejection-sampling-ii&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Likelihood-free rejection sampling II&lt;/h2&gt;
&lt;p&gt;For a given &lt;span class=&#34;math inline&#34;&gt;\(h&amp;gt;0\)&lt;/span&gt;, the algorithm goes below, for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\dots,N\)&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Step 1: generate &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(i)}\sim g(\theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 2: generate &lt;span class=&#34;math inline&#34;&gt;\(y\sim p(y|\theta^{(i)})\)&lt;/span&gt; from the likelihood&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 3: if &lt;span class=&#34;math inline&#34;&gt;\(||y- y_{obs}||\le h\)&lt;/span&gt;, then accept &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(i)}\)&lt;/span&gt; with probability &lt;span class=&#34;math inline&#34;&gt;\(r(\theta^{(i)})=\frac{\pi(\theta^{(i)})}{Kg(\theta^{(i)})}\)&lt;/span&gt;. Else go to Step 1.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-g-and-k-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The g-and-k distribution&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(y_{obs}\)&lt;/span&gt; of length &lt;span class=&#34;math inline&#34;&gt;\(1000\)&lt;/span&gt; is generated from the &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt;-and-&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; distribution with parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta_0=(3,1,2,0.5)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;prior density&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\pi(\theta) = \pi(A)\pi(B)\pi(g)\pi(k) = N(1,5)\times N(0.25,2) \times U(0,10) \times U(0,1)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The results&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;gandk.png&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;grey dots: for the AR rule &lt;span class=&#34;math inline&#34;&gt;\(||y-y_{obs}||\le h\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;black dots: for the AR rule &lt;span class=&#34;math inline&#34;&gt;\(||S(y)-S(y_{obs})||\le h\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-use-of-summary-statistic&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The use of summary statistic&lt;/h2&gt;
&lt;p&gt;Summary statistic (Drovandi and Pettitt, 2011): &lt;span class=&#34;math inline&#34;&gt;\(S(y) = (S_A,S_B,S_g,S_k)\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(S_A=E_4\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(S_B=E_6-E_2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(S_g=(E_6+E_2-2E_4)/S_B\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(S_k = (E_7-E_5+E_3-E_1)/S_B\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(E_1\le E_2 \le \cdots \le E_8\)&lt;/span&gt; are the octiles of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;likelihood-free-rejection-sampling-iii&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Likelihood-free rejection sampling III&lt;/h2&gt;
&lt;p&gt;For a given &lt;span class=&#34;math inline&#34;&gt;\(h&amp;gt;0\)&lt;/span&gt;, the algorithm goes below, for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\dots,N\)&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Step 1: generate &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(i)}\sim g(\theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 2: generate &lt;span class=&#34;math inline&#34;&gt;\(y\sim p(y|\theta^{(i)})\)&lt;/span&gt; from the likelihood&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 3: if &lt;span class=&#34;math inline&#34;&gt;\(||S(y)- S(y_{obs})||\le h\)&lt;/span&gt;, then accept &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(i)}\)&lt;/span&gt; with probability &lt;span class=&#34;math inline&#34;&gt;\(r(\theta^{(i)})=\frac{\pi(\theta^{(i)})}{Kg(\theta^{(i)})}\)&lt;/span&gt;. Else go to Step 1.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;NOTE: usually &lt;span class=&#34;math inline&#34;&gt;\(\text{dim}(S(y))\ll\text{dim}(y)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;approximate-bayesian-computation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Approximate Bayesian Computation&lt;/h2&gt;
&lt;p&gt;ABC is a kind of likelihood-free methods developed
for when the likelihood function is computationally intractable or otherwise unavailable.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The likelihood-free rejection algorithm with &lt;span class=&#34;math inline&#34;&gt;\(h=0\)&lt;/span&gt; which is exact (work for discrete cases), is not an ABC algorithm&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For &lt;span class=&#34;math inline&#34;&gt;\(h&amp;gt;0\)&lt;/span&gt;, it is an ABC algorithm as the samples will be drawn from an approximation to the posterior distribution.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The aim of any ABC analysis is to find a practical way of
performing the Bayesian analysis, while keeping the approximation and the
computation to a minimum.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;abc-with-general-kernels&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ABC with general kernels&lt;/h2&gt;
&lt;p&gt;The approximate joint distribution:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\pi_{ABC}(\theta,y|y_{obs})\propto I\{||y-y_{obs}||\le h\}p(y|\theta)\pi(\theta)\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;generalize the AR rule &lt;span class=&#34;math inline&#34;&gt;\(I\{||y-y_{obs}||\le h\}\)&lt;/span&gt; to smoothing kernel function &lt;span class=&#34;math inline&#34;&gt;\(K_h(||y-y_{obs}||)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\pi_{ABC}(\theta,y|y_{obs})\propto K_h(||y-y_{obs}||)p(y|\theta)\pi(\theta)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The kernel: &lt;span class=&#34;math inline&#34;&gt;\(K_h(u)=\frac 1 h K(u/h)\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(K(u)\)&lt;/span&gt; is a symmetric function&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(K(u)\ge 0\)&lt;/span&gt; for all &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\int K(u) d u=1,\ \int uK(u)du=0,\ \int u^2K(u)du&amp;lt;\infty\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt; is the bandwidth&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;common-kernels&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Common kernels&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Uniform &lt;span class=&#34;math inline&#34;&gt;\(\frac 1 2 I\{|u|\le 1\}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Tringular &lt;span class=&#34;math inline&#34;&gt;\((1-|u|)I\{|u|\le 1\}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Epanechnikov &lt;span class=&#34;math inline&#34;&gt;\(\frac 3 4(1-u^2)I\{|u|\le 1\}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Biweight &lt;span class=&#34;math inline&#34;&gt;\(\frac{15}{16}(1-u^2)^3I\{|u|\le 1\}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Gaussian &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{\sqrt{2\pi}}e^{-u^2/2}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;kernels.png&#34; width=&#34;60%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;abc-rejection-sampling-algorithm-i&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ABC Rejection Sampling Algorithm I&lt;/h2&gt;
&lt;p&gt;For a given &lt;span class=&#34;math inline&#34;&gt;\(h&amp;gt;0\)&lt;/span&gt;, the algorithm goes below, for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\dots,N\)&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Step 1: generate &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(i)}\sim g(\theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 2: generate &lt;span class=&#34;math inline&#34;&gt;\(y\sim p(y|\theta^{(i)})\)&lt;/span&gt; from the likelihood&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 3: accept &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(i)}\)&lt;/span&gt; with probability &lt;span class=&#34;math inline&#34;&gt;\(r(\theta^{(i)})=\frac{K_h(||y-y_{obs}||)\pi(\theta^{(i)})}{Kg(\theta^{(i)})}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(K\ge \sup_{\theta}\frac{\pi(\theta)}{g(\theta)}\sup_uK(u)\)&lt;/span&gt;. Else go to Step 1.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;approximate-posterior-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Approximate Posterior Distribution&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\pi_{ABC}(\theta,y|y_{obs})\propto K_h(||y-y_{obs}||)p(y|\theta)\pi(\theta)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The approximate posterior:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\pi_{ABC}(\theta|y_{obs})=\int \pi_{ABC}(\theta,y|y_{obs}) d y\propto \pi(\theta)\int K_h(||y-y_{obs}||)p(y|\theta)dy\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ABC approximation to the true likelihood:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p_{ABC}(y_{obs}|\theta) = \int K_h(||y-y_{obs}||)p(y|\theta)dy.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\pi_{ABC}(\theta|y_{obs})\to \pi(\theta|y_{obs}) \text{ as }h\to 0.\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 1&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;likelihood function &lt;span class=&#34;math inline&#34;&gt;\(p(y|\theta)=\theta e^{-\theta y}\)&lt;/span&gt;, i.e., &lt;span class=&#34;math inline&#34;&gt;\(Exp(\theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;prior &lt;span class=&#34;math inline&#34;&gt;\(\pi(\theta) \propto \theta^{\alpha-1}e^{-\beta\theta}\)&lt;/span&gt;, i.e., &lt;span class=&#34;math inline&#34;&gt;\(Gamma(\alpha,\beta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;posterior density &lt;span class=&#34;math inline&#34;&gt;\(Gamma(\alpha+1,\beta+y_{obs})\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;uniform kernel &lt;span class=&#34;math inline&#34;&gt;\(K_h(u) = \frac 1{2h}1\{|u|\le h\}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p_{ABC}(y_{obs}|\theta) = \int K_h(||y-y_{obs}||)p(y|\theta)dy=\frac 1{2h}e^{-\theta y_{obs}}(e^{\theta h}-e^{-\theta h})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p_{ABC}(y_{obs}|\theta)-p(y_{obs}|\theta)=p(y_{obs}|\theta)\left(\frac{e^{\theta h}-e^{-\theta h}}{2\theta h}-1\right)\approx \frac{h^2\theta^3e^{-\theta y_{obs}}}{6}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\pi_{ABC}(\theta|y_{obs}) = \frac{\theta^{\alpha-1}e^{-\theta (y_{obs}+\beta)}(e^{\theta h}-e^{-\theta h})}{\frac{\Gamma(\alpha)}{(y_{obs}+\beta-h)^\alpha}-\frac{\Gamma(\alpha)}{(y_{obs}+\beta+h)^\alpha}}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plots&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;ex1.png&#34; width=&#34;80%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 2&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;likelihood &lt;span class=&#34;math inline&#34;&gt;\(N(\theta,\sigma_0^2)\)&lt;/span&gt; with known &lt;span class=&#34;math inline&#34;&gt;\(\sigma_0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;prior &lt;span class=&#34;math inline&#34;&gt;\(N(m_0,s_0^2)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Guassian kernel &lt;span class=&#34;math inline&#34;&gt;\(K_h(u)=N(0,h^2)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(p_{ABC}(\bar y_{obs}|\theta)=N(\theta,\sigma^2/n+h^2)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\pi_{ABC}(\theta|y_{obs})=N(\mu_{ABC},\sigma_{ABC}^2)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mu_{ABC} = \frac{\frac{m_0}{s_0^2}+\frac{\bar y_{obs}}{\sigma_0^2/n+h^2}}{\frac{1}{s_0^2}+\frac{1}{\sigma_0^2/n+h^2}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{1}{\sigma_{ABC}^2}=\frac{1}{s_0^2}+\frac{1}{\sigma_0^2/n+h^2}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plots-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plots&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;ex2.png&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Gaussian kernel vs. uniform kernel&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The ABC posterior approximation may
be improved simply by rescaling the posterior variance to remove the term &lt;span class=&#34;math inline&#34;&gt;\(h^2\)&lt;/span&gt; (Drovandi 2012).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-use-of-summary-statistics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Use of Summary Statistics&lt;/h2&gt;
&lt;p&gt;For a given &lt;span class=&#34;math inline&#34;&gt;\(h&amp;gt;0\)&lt;/span&gt;, the algorithm goes below, for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\dots,N\)&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Step 1: generate &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(i)}\sim g(\theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 2: generate &lt;span class=&#34;math inline&#34;&gt;\(y\sim p(y|\theta^{(i)})\)&lt;/span&gt; from the likelihood&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 3: accept &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(i)}\)&lt;/span&gt; with probability &lt;span class=&#34;math inline&#34;&gt;\(r(\theta^{(i)})=\frac{K_h(||S(y)-S(y_{obs})||)\pi(\theta^{(i)})}{Kg(\theta^{(i)})}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(K\ge \sup_{\theta}\frac{\pi(\theta)}{g(\theta)}\sup_uK(u)\)&lt;/span&gt;. Else go to Step 1.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;example-3&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 3&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the model &lt;span class=&#34;math inline&#34;&gt;\(y=(y_1,y_2), y_i\sim B(n,\theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;prior &lt;span class=&#34;math inline&#34;&gt;\(\theta\sim U(0,1)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;three sufficient statistics: &lt;span class=&#34;math inline&#34;&gt;\(s^1=(y_1,y_2)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(s^2 =(y_{(1)},y_{(2)})\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(s^3=y_1+y_2\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Acceptance rates (&lt;span class=&#34;math inline&#34;&gt;\(h=0\)&lt;/span&gt;) are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(r_1=C_n^{y_1}C_n^{y_2} B(y_1+y_2+1,2n-y_1-y_2+1)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(r_2 = [2-1\{y_{(1)}=y_{(2)}\}]r_1\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(r_3 = 1/(2n+1)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;E.g., &lt;span class=&#34;math inline&#34;&gt;\(y_{obs}=(1,2)\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\(n=5\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(p_1\approx 0.038,\ p_2\approx 0.076,\ p_3\approx 0.091\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The most efficient choice is the &lt;strong&gt;minimal sufficient statistic&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This may still be non-viable in practice; low-dimensional sufficient statistic may not exist – Weibull distribution&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;example-2-continued&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 2 (continued)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;likelihood &lt;span class=&#34;math inline&#34;&gt;\(N(\theta,\sigma_0^2)\)&lt;/span&gt; with known &lt;span class=&#34;math inline&#34;&gt;\(\sigma_0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;prior &lt;span class=&#34;math inline&#34;&gt;\(N(m_0,s_0^2)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Guassian kernel &lt;span class=&#34;math inline&#34;&gt;\(K_h(u)=N(0,h^2)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;summary statistic &lt;span class=&#34;math inline&#34;&gt;\(s=\bar{y}_{1{:}n&amp;#39;}=\frac{1}{n&amp;#39;}\sum_{i=1}^{n&amp;#39;}y_i\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(n&amp;#39;&amp;lt;n\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(p_{ABC}(s_{obs}|\theta)=N(\theta,\sigma^2/n&amp;#39;+h^2)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\pi_{ABC}(\theta|y_{obs})=N(\mu_{ABC},\sigma_{ABC}^2)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mu_{ABC} = \frac{\frac{m_0}{s_0^2}+\frac{s_{obs}}{\sigma_0^2/n&amp;#39;+h^2}}{\frac{1}{s_0^2}+\frac{1}{\sigma_0^2/n&amp;#39;+h^2}},\ \frac{1}{\sigma_{ABC}^2}=\frac{1}{s_0^2}+\frac{1}{\sigma_0^2/n&amp;#39;+h^2}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;two sources of error: the degree
of inefficiency of replacing &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(s=S(y)\)&lt;/span&gt; and the matching of the
simulated and observed data through the Gaussian kernel&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-choice-of-summary-statistics-for-abc&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The choice of summary statistics for ABC&lt;/h2&gt;
&lt;p&gt;The choice of summary statistics for an ABC analysis is a critical decision
that directly affects the quality of the posterior approximation.&lt;/p&gt;
&lt;p&gt;Many
approaches for determining these statistics are available, and these are reviewed
in Blum et al. (2013) and Prangle (2019). These methods seek to trade off two aspects of the ABC posterior
approximation that directly result from the choice of summary statistics.&lt;/p&gt;
&lt;p&gt;The dimension of the summary statistic should be large enough so
that it contains as much information about the observed data as possible, but
also low enough so that the curse-of-dimensionality of matching &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(s_{obs}\)&lt;/span&gt;
is avoided.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-practical-issues-with-summary-statistics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Some practical issues with summary statistics&lt;/h2&gt;
&lt;p&gt;It is NOT always viable to continue to add summary statistics to s until the
resulting ABC posterior approximation does not change for the worse.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the model &lt;span class=&#34;math inline&#34;&gt;\(y=(y_1,\dots,y_n), y_i\sim Poission(\lambda)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;prior &lt;span class=&#34;math inline&#34;&gt;\(\lambda \sim Gamma(\alpha,\beta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;posterior &lt;span class=&#34;math inline&#34;&gt;\(\lambda|y\sim Gamma(\alpha+n\bar y,\beta+n)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;data &lt;span class=&#34;math inline&#34;&gt;\(y_{obs}=(0,0,0,0,5)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;summary statistics: &lt;span class=&#34;math inline&#34;&gt;\(s^1=\bar y,\ s^2= v=\sqrt{\frac{1}{n-1}\sum_{i=1}^n(y_i-\bar y)^2}, s^3=(\bar y,v)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;using uniform kernel with &lt;span class=&#34;math inline&#34;&gt;\(h=0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(h=0.3\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\alpha=\beta=1\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;comparison&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comparison&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;ss.png&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;top (&lt;span class=&#34;math inline&#34;&gt;\(h=0\)&lt;/span&gt;), bottom (&lt;span class=&#34;math inline&#34;&gt;\(h=0.3\)&lt;/span&gt;)&lt;/li&gt;
&lt;li&gt;prior &lt;span class=&#34;math inline&#34;&gt;\(Gamma(\alpha,\beta)\)&lt;/span&gt; (dashed lines), target distribution &lt;span class=&#34;math inline&#34;&gt;\(Gamma(\alpha+n\bar y,\beta+n)\)&lt;/span&gt; (solid line)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-choice-of-distance-measure&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The choice of distance measure&lt;/h2&gt;
&lt;p&gt;The distance measure &lt;span class=&#34;math inline&#34;&gt;\(||s-s_{obs}||\)&lt;/span&gt; can also have a substantial impact on ABC algorithm efficiency, and therefore the quality of the posterior approximation.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the most common one is the Euclidean distance&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the Mahalanobis distance (Peters
et al. 2012; Erhardt and Sisson 2016):&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[||s-s_{obs}||=(s-s_{obs})^\top\Sigma^{-1}(s-s_{obs})\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; is the covariance matrix of &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;comparison-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comparison&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;distance.png&#34; width=&#34;60%&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the entries of &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; may dependent&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;implementing a circular acceptance region (implying independence
and identical scales) induces both type I and type II errors.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;example-4&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 4&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the model &lt;span class=&#34;math inline&#34;&gt;\(y_1,\dots,y_{50}\sim N(\theta,1)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;prior &lt;span class=&#34;math inline&#34;&gt;\(\theta\sim U(-5,5)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;summary statistics: &lt;span class=&#34;math inline&#34;&gt;\(s^1=(\bar y_{1{:}40},\bar y_{41{:}50})\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(s^2=(\bar y_{1{:}25}-\bar y_{26{:}50},\bar y_{26{:}50})\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\Sigma^1 = Cov(s^1|\theta) = \left(
\begin{matrix}
1/40 &amp;amp; 0\\
0 &amp;amp; 1/10
\end{matrix}
\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\Sigma^2 = Cov(s^2|\theta) = \left(
\begin{matrix}
2/25 &amp;amp; -1/25\\
-1/25 &amp;amp; 1/25
\end{matrix}
\right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulation&lt;/h2&gt;
&lt;p&gt;Mean Number of Summary StatisticGenerations per Final Accepted Particle (with Standard Errors inParentheses)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;tabdistance.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pilot-analysis-on-the-covariance-matrix&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pilot analysis on the covariance matrix&lt;/h2&gt;
&lt;p&gt;To estimate the covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;, Luciani
et al. (2009) and Erhardt and Sisson (2016) identify some value of &lt;span class=&#34;math inline&#34;&gt;\(\theta=\theta^*\)&lt;/span&gt; in a high posterior density region via a pilot analysis and then estimate
&lt;span class=&#34;math inline&#34;&gt;\(Cov(s|\theta^*)\)&lt;/span&gt; based on repeated draws from &lt;span class=&#34;math inline&#34;&gt;\(p(s|\theta^*)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;abc-is-algorithm&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ABC-IS algorithm&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the target density &lt;span class=&#34;math inline&#34;&gt;\(\pi_{ABC}(\theta|s_{obs})\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the proposal &lt;span class=&#34;math inline&#34;&gt;\(g(\theta)\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(g(\theta)&amp;gt;0\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(f(\theta)&amp;gt;0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For a given &lt;span class=&#34;math inline&#34;&gt;\(h&amp;gt;0\)&lt;/span&gt;, the algorithm goes below, for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\dots,N\)&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Step 1: generate &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(i)}\sim g(\theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 2: generate &lt;span class=&#34;math inline&#34;&gt;\(y\sim p(y|\theta^{(i)})\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 3: compute the weight &lt;span class=&#34;math display&#34;&gt;\[w(\theta^{(i)})=\frac{K_h(||S(y)-S(y_{obs})||)\pi(\theta^{(i)})}{g(\theta^{(i)})}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Return the weight samples &lt;span class=&#34;math inline&#34;&gt;\((w(\theta^{(i)}),\theta^{(i)}), i=1,\dots,N\)&lt;/span&gt;. The self-normalized IS estimator is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[E[a(\theta)|y_{obs}]\approx \frac{\frac 1 N\sum_{i=1}^N w(\theta^{(i)})a(\theta^{(i)})}{\frac 1 N\sum_{i=1}^N w(\theta^{(i)})}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;All models are approximations to the real data-generation process.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;use of summary statistics &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;, the effect of &lt;span class=&#34;math inline&#34;&gt;\(\text{dim}(s)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;use of kernel appriximation &lt;span class=&#34;math inline&#34;&gt;\(K_h\)&lt;/span&gt;, the effect of bandwidth &lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;use of distance measure, the effect of covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Monte Carlo error, the effect of the sample size &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 1: Probability and Inference</title>
      <link>/course/bchap01/</link>
      <pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/course/bchap01/</guid>
      <description>&lt;div id=&#34;steps-in-bda&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3 steps in BDA&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;set up the statistical model&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;compute the &lt;code&gt;posterior distribution&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;model checking and model improvement&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;statistical-inference&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Statistical inference&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: draw conclusions about &lt;strong&gt;unobserved quantities&lt;/strong&gt; from the data (observed)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;potentially observable quantities, e.g., future observations of a process&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;not directly observable quantities, e.g., unobservable population parameters&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;notations-and-assumptions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Notations and assumptions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;unobservable population parameters of interest: &lt;span class=&#34;math inline&#34;&gt;\(\theta=(\theta_1,\dots,\theta_m)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the observed data: &lt;span class=&#34;math inline&#34;&gt;\(y=(y_1,\dots,y_n)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;potentially observable quantities: &lt;span class=&#34;math inline&#34;&gt;\(\tilde y\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Assumption 1&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;exchangeability: the &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; values &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; are exchangeable, e.g., iid samples.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Assumption 2&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;conditional independence of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\tilde y\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;bayesian-inference&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bayesian inference&lt;/h2&gt;
&lt;p&gt;To make inferences about the posterior distributions, such as &lt;span class=&#34;math inline&#34;&gt;\(p(\theta|y)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(p(\tilde y|y)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bayes’ rule&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(\theta|y)=\frac{p(\theta,y)}{p(y)}=\frac{p(y|\theta)p(\theta)}{p(y)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(\theta|y)\propto  p(y|\theta)p(\theta)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The imiplied constant is
&lt;span class=&#34;math display&#34;&gt;\[p(y)=\int p(y|\theta)p(\theta) d \theta.\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;prediction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Prediction&lt;/h2&gt;
&lt;p&gt;To make inferences about an unknown observable quantity&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;prior predictive distribution: &lt;span class=&#34;math inline&#34;&gt;\(p(y)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;posterior predictive dsitribution: &lt;span class=&#34;math inline&#34;&gt;\(p(\tilde y|y)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
p(\tilde y|y) = \int p(\tilde y,\theta|y)d\theta = \int p(\tilde y|\theta,y)p(\theta|y)d \theta = \int p(\tilde y|\theta)p(\theta|y)d \theta
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Again, &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\tilde y\)&lt;/span&gt; are conditionally independent given &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;likelihood&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Likelihood&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(p(y|\theta)\)&lt;/span&gt; is called the &lt;strong&gt;likelihood function&lt;/strong&gt;, which is regarded as a function of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;odds ratios&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{p(\theta_1|y)}{p(\theta_2|y)}=\frac{p(\theta_1)p(y|\theta_1)/p(y)}{p(\theta_2)p(y|\theta_2)/p(y)}=\frac{p(\theta_1)}{p(\theta_2)}\frac{p(y|\theta_1)}{p(y|\theta_2)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;posterior odds = prior odds &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; likelihood ratio&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-1-inference-about-a-genetic-status&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 1: inference about a genetic status&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;males: one X-chromosome + one Y-chromosome&lt;/li&gt;
&lt;li&gt;females: two X-chromosomes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hemophilia is a disease that exhibits X-chromosome-linked recessive inheritance.
The disease is generally fatal for women who inherit two such genes.&lt;/p&gt;
&lt;p&gt;Consider a woman who has an affected brother and her father is not affected.
Let &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; be the state of the woman: a carrier of the gene (&lt;span class=&#34;math inline&#34;&gt;\(\theta=1\)&lt;/span&gt;) or not (&lt;span class=&#34;math inline&#34;&gt;\(\theta=0\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Prior distribution&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(P(\theta=1)=P(\theta=0)=0.5\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data and model&lt;/strong&gt;: She has two sons. Let &lt;span class=&#34;math inline&#34;&gt;\(y_i=1\)&lt;/span&gt; or 0 denote the state of her sons. Now observe that her sons are not affected. Given &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(y_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_2\)&lt;/span&gt; are iid.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-1-inference-about-a-genetic-status-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 1: inference about a genetic status&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Likelihood function&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(y_1=0,y_2=0|\theta=1)=0.5\times 0.5=0.25\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(y_1=0,y_2=0|\theta=0)=1\times 1=1\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Posterior distribution&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(\theta=1|y) = \frac{p(y|\theta=1)p(\theta=1)}{p(y)}=0.2\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-1-inference-about-a-genetic-status-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 1: inference about a genetic status&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Adding more data&lt;/strong&gt;: suppose that the woman has a third son, who is also unaffacted.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(\theta=1|y_1,y_2,y_3) = \frac{0.5\times 0.2}{0.5\times 0.2+1\times 0.8}=0.111\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;A key aspect of Bayesian analysis is the ease with which sequential analyses can be performed.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;: What happen if we suppose that the third son is affected?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-2-spelling-correction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 2: spelling correction&lt;/h2&gt;
&lt;p&gt;Classification of words is a problem of managing uncertainty. Suppose someone types &lt;strong&gt;radom&lt;/strong&gt;. How should that be read?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;random&lt;/li&gt;
&lt;li&gt;radon&lt;/li&gt;
&lt;li&gt;radom&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Data and model&lt;/strong&gt;: Let &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; be the word that the person was intending to type, and let &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; as the data. Now &lt;span class=&#34;math inline&#34;&gt;\(y=\)&lt;/span&gt;’radom’ and &lt;span class=&#34;math inline&#34;&gt;\(\theta\in\)&lt;/span&gt;{&lt;span class=&#34;math inline&#34;&gt;\(\theta_1\)&lt;/span&gt;=‘random’,&lt;span class=&#34;math inline&#34;&gt;\(\theta_2\)&lt;/span&gt;=‘radon’,&lt;span class=&#34;math inline&#34;&gt;\(\theta_3\)&lt;/span&gt;=‘radom’}. The posterior density is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(\theta|y=\text{&amp;#39;radom&amp;#39;})\propto p(\theta)P(y=\text{&amp;#39;radom&amp;#39;}|\theta).\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-2-spelling-correction-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 2: spelling correction&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Prior distribution&lt;/strong&gt;: Here are probabilities supplied by researchers at Google.
Goole Ngram Viewer: &lt;a href=&#34;https://books.google.com/ngrams&#34; class=&#34;uri&#34;&gt;https://books.google.com/ngrams&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class=&#34;math inline&#34;&gt;\(p(\theta)\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;random&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(7.60\times 10^{-5}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;radon&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(6.05\times 10^{-6}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;radom&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(3.12\times 10^{-7}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Likelihood&lt;/strong&gt;: Here are some conditional probabilities from Google’s model of spelling and typing errors:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class=&#34;math inline&#34;&gt;\(p(\text{&amp;#39;radom&amp;#39;}|\theta)\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;random&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(0.00193\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;radon&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(0.000143\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;radom&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(0.975\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;example-2-spelling-correction-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 2: spelling correction&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Posterior distribution&lt;/strong&gt;:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class=&#34;math inline&#34;&gt;\(p(\theta)P(y=\text{&amp;#39;radom&amp;#39;}|\theta)\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(\theta|y=\text{&amp;#39;radom&amp;#39;})\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;random&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(1.47\times 10^{-7}\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;0.325&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;radon&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(8.65\times 10^{-10}\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;0.002&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;radom&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(3.04\times 10^{-7}\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.673&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Model improvement&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;including contextual info in the prior probabilities, e.g., statistical book.&lt;/li&gt;
&lt;li&gt;let &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; be the contextual information used by the model.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(\theta|x,y)\propto p(\theta|x)p(y|\theta,x)\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;for simplicity, we may assume &lt;span class=&#34;math inline&#34;&gt;\(p(y|\theta,x)=p(y|\theta)\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 10: Bayesian computation</title>
      <link>/course/bchap10/</link>
      <pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/course/bchap10/</guid>
      <description>&lt;div id=&#34;introduction-to-bayesian-computation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction to Bayesian computation&lt;/h2&gt;
&lt;p&gt;The goals are to estimate&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the posterior distribution &lt;span class=&#34;math inline&#34;&gt;\(p(\theta|y)\propto p(\theta)p(y|\theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the posterior predictive distribution
&lt;span class=&#34;math display&#34;&gt;\[p(\tilde y|y) = \int p(\tilde y|\theta)p(\theta|y)d \theta =E[p(\tilde y|\theta)|y]\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We are therefore insterested in estimating the posterior expectation
&lt;span class=&#34;math display&#34;&gt;\[\mu=E[h(\theta)|y]=\int h(\theta)p(\theta|y)d \theta\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;moments: &lt;span class=&#34;math inline&#34;&gt;\(h(\theta)=\theta^k\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;probability: &lt;span class=&#34;math inline&#34;&gt;\(h(\theta)=1_A(\theta)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(A\subseteq \Theta\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;predictive density: &lt;span class=&#34;math inline&#34;&gt;\(h(\theta)=p(\tilde y|\theta)\)&lt;/span&gt; for fixed &lt;span class=&#34;math inline&#34;&gt;\(\tilde y\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;monte-carlo-methods&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Monte Carlo methods&lt;/h2&gt;
&lt;p&gt;Suppose we can simulate &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(i)},\dots,\theta^{(N)}\sim p(\theta|y)\)&lt;/span&gt; independently. Monte Carlo (MC) esimate is then the sample average:
&lt;span class=&#34;math display&#34;&gt;\[\hat{\mu}_N = \frac1N\sum_{i=1}^N h(\theta^{(i)})\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LLN: &lt;span class=&#34;math inline&#34;&gt;\(\hat\mu_N\to \mu\)&lt;/span&gt; w.p.1 as &lt;span class=&#34;math inline&#34;&gt;\(N\to \infty\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;CLT: &lt;span class=&#34;math inline&#34;&gt;\(\hat\mu_N-\mu=O_p(N^{-1/2})\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Traditional quadrature rules’ have error rate &lt;span class=&#34;math inline&#34;&gt;\(O(N^{-r/d})\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(r\ge 1\)&lt;/span&gt; depends on the smoothness of the functions, and &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is the dimension of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. This suffers &lt;strong&gt;the curse of dimensionality&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;MC has an error rate &lt;span class=&#34;math inline&#34;&gt;\(O(N^{-1/2})\)&lt;/span&gt; independently of the smoothness and the dimension of the functions. The task is to simulate iid samples &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(i)}\sim p(\theta|y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;random-number-generators&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Random number generators&lt;/h2&gt;
&lt;p&gt;We start with a pseudo-random number generator:
&lt;span class=&#34;math display&#34;&gt;\[u_1,\dots,u_n,\dots\stackrel{iid}\sim U(0,1)\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Mersenne Twister&lt;/strong&gt; by Matsumoto &amp;amp; Nishimura (1998), whose period is &lt;span class=&#34;math inline&#34;&gt;\(2^{19937}-1&amp;gt;10^{6000}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RngStreams&lt;/strong&gt; by L’Ecuyer, Simard, Chen, Kelton (2002)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;They aren’t really uniform random, but good ones are close enough.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;non-uniform-random-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Non-uniform random variables&lt;/h2&gt;
&lt;p&gt;Some common distributions (such as Normal, exponential, binomial, Poisson etc.) are already in some scientific softwares (R, Python, Matlab, Julia, Mathematica, etc.)&lt;/p&gt;
&lt;p&gt;We are now concerned with a general distribution. Principled approaches are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;inversion&lt;/li&gt;
&lt;li&gt;acceptance-rejection&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;inversion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Inversion&lt;/h2&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(F(x)\)&lt;/span&gt; be the CDF of the distribution of interest. We can simulate the distribution via iid uniforms &lt;span class=&#34;math inline&#34;&gt;\(U_1,\dots,U_N\stackrel{iid}\sim U(0,1)\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X_i=F^{-1}(U_i),i=1,\dots,N\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(F^{-1}\)&lt;/span&gt; is the inverse of the CDF &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;, definited by
&lt;span class=&#34;math display&#34;&gt;\[F^{-1}(u)=\inf\{x\in\mathbb{R}|F(x)\ge u\}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;it is easy to see that &lt;span class=&#34;math inline&#34;&gt;\(X_i\stackrel{iid}\sim F\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;inversion-examples&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Inversion: examples&lt;/h2&gt;
&lt;div id=&#34;gaussian&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Gaussian&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Z=\Phi^{-1}(U)\sim N(0,1)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X=\mu+\sigma\Phi^{-1}(U) \sim N(\mu,\sigma^2)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exponential&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exponential&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X= -\frac 1\lambda \log (1-U)\sim Exp(\lambda)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bernoulli&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bernoulli&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X=1\{U\le p\}\sim Bin(1,p)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;multivariate-inverse-transformation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multivariate inverse transformation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;let &lt;span class=&#34;math inline&#34;&gt;\(F(x_1,\dots,x_d)\)&lt;/span&gt; be the PDF of &lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_d\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;let &lt;span class=&#34;math inline&#34;&gt;\(F_i(x_i)\)&lt;/span&gt; be the marginal distribution of &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;for &lt;span class=&#34;math inline&#34;&gt;\(i=2,\dots,d\)&lt;/span&gt;, let &lt;span class=&#34;math inline&#34;&gt;\(F_i(x_i|x_1,\dots,x_{i-1})\)&lt;/span&gt; be the conditional CDF&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The multivariate inverse transformation is proposed by Rosenblatt (1952), which simulates the components &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt; recursively, i.e.,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X_1=F_1^{-1}(U_1)\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[X_i=F_i^{-1}(U_i|X_1,\dots,X_{i-1}),\ i=2,\dots,d\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the output has the destribution &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;the order of simulating the components can be arbitrary&lt;/li&gt;
&lt;li&gt;the critical issue is to know the conditional CDFs in advance&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;acceptance-rejection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Acceptance-rejection&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;suppose the target distrubtion is &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; with the support &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;we can sample &lt;span class=&#34;math inline&#34;&gt;\(Y\sim g\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; is another density satisfying: there exists &lt;span class=&#34;math inline&#34;&gt;\(M&amp;gt;0\)&lt;/span&gt; such that
&lt;span class=&#34;math display&#34;&gt;\[\frac{f(x)}{g(x)}\le M\ \forall x\in \mathcal{X}\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;we can compute &lt;span class=&#34;math inline&#34;&gt;\(f(x)/g(x)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The algorithm goes below&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Step 1: simulate &lt;span class=&#34;math inline&#34;&gt;\(Y\sim g\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 2: accept &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; as a draw from &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; with probability &lt;span class=&#34;math inline&#34;&gt;\(f(Y)/(Mg(Y))\)&lt;/span&gt;. If the draw is rejected, return to Step 1.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 2’: simulate &lt;span class=&#34;math inline&#34;&gt;\(U\sim U(0,1)\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[
\begin{cases}
\text{accept } Y &amp;amp; U\le f(Y)/(Mg(Y))\\
\text{go to Step 1 }&amp;amp; else
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;acceptance-rejection-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Acceptance-rejection&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;AR.png&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the acceptance probability: &lt;span class=&#34;math display&#34;&gt;\[E[f(Y)/(Mg(Y))]=\frac 1 M\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;we may choose the smallest &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(f(x)\le Mg(x)\)&lt;/span&gt; for all &lt;span class=&#34;math inline&#34;&gt;\(x\in\mathcal{X}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;acceptance-rejection-for-bayesian-computation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Acceptance-rejection for Bayesian computation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the target density
&lt;span class=&#34;math display&#34;&gt;\[p(\theta|y)=\frac{p(\theta)p(y|\theta)}{p(y)}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the constant &lt;span class=&#34;math inline&#34;&gt;\(p(y)\)&lt;/span&gt; is unknown&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the AR algorithm works well if taking &lt;span class=&#34;math inline&#34;&gt;\(f(\theta)=p(\theta)p(y|\theta)\)&lt;/span&gt;
and using proposal density &lt;span class=&#34;math inline&#34;&gt;\(\propto g(\theta)\)&lt;/span&gt; with
&lt;span class=&#34;math display&#34;&gt;\[\frac{p(\theta)p(y|\theta)}{g(\theta)}\le M\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;example-gamma-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example: Gamma distribution&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Gamma(\alpha,\lambda)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\alpha&amp;gt;0\)&lt;/span&gt; is the shape, &lt;span class=&#34;math inline&#34;&gt;\(\lambda&amp;gt;0\)&lt;/span&gt; is the rate&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;density&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f(x) = \frac{\lambda^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\lambda x}1\{x&amp;gt; 0\}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Gamma(\alpha,\lambda)\stackrel{d}{=}\frac 1 \lambda Gamma(\alpha,1)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Gamma(\alpha,1)\stackrel{d}{=}U(0,1)^{1/\alpha}Gamma(\alpha+1,1)\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;so our target is &lt;span class=&#34;math inline&#34;&gt;\(Gamma(\alpha,1)\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\alpha&amp;gt;1\)&lt;/span&gt;. For this case, the density is bounded.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the proposal
&lt;span class=&#34;math display&#34;&gt;\[g(x)=?\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;gamma-density&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gamma density&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/course/bchap10_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Ahrens and Dieter (1974) took proposals from a density that combines a
Gaussian density in the center and an exponential density in the right tail.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Marsaglia and Tsang (2000) present an AR algorithm from a truncated
&lt;span class=&#34;math inline&#34;&gt;\(N(0,1)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;example-beta-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example: Beta distribution&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Beta(\alpha,\beta)\)&lt;/span&gt; density
&lt;span class=&#34;math display&#34;&gt;\[f(x)=\frac{\Gamma(\alpha,\beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1}1\{0&amp;lt;x&amp;lt;1\}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;generate a Beta from two independent Gammas
&lt;span class=&#34;math display&#34;&gt;\[Beta(\alpha,\beta)\stackrel{d}{=}\frac{Gamma(\alpha,\lambda)}{Gamma(\alpha,\lambda)+Gamma(\beta,\lambda)}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;for &lt;span class=&#34;math inline&#34;&gt;\(\alpha&amp;gt;1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta&amp;gt;1\)&lt;/span&gt;, the beta density is unimodal and achieves its maximum at &lt;span class=&#34;math inline&#34;&gt;\(x^*=(\alpha-1)/(\alpha+\beta-2)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;beta-density&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Beta density&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/course/bchap10_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Proposal distribution: &lt;span class=&#34;math inline&#34;&gt;\(U(0,1)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(M=f(x^*)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;accept &lt;span class=&#34;math inline&#34;&gt;\(U\sim U(0,1)\)&lt;/span&gt; with probability &lt;span class=&#34;math inline&#34;&gt;\(f(U)/M\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;beta-generator-r-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Beta generator: R code&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;myBeta &amp;lt;- function(n,alpha,beta){
  if(alpha&amp;lt;=1 | beta&amp;lt;=1)
    stop(&amp;quot;alpha, beta cannot be &amp;lt;= 1&amp;quot;)
  M = dbeta((alpha-1)/(alpha+beta-2),alpha,beta)
  x = rep(0,n)
  for(i in 1:n){
    while (TRUE){
      U = runif(1)
      if(dbeta(U,alpha,beta)&amp;gt;= M*runif(1)){
        x[i] = U
        break
      }
    }
  }
  return(x)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;simulation-results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulation results&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/course/bchap10_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-5&#34;&gt;Table 1: &lt;/span&gt;&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mean&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;sd&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;myBeta&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4001780&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1968478&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;dbeta&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3996059&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2000705&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;true values&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2000000&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;importance-sampling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Importance Sampling&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;the target is to estimate &lt;span class=&#34;math inline&#34;&gt;\(\mu=E_f[h(X)]\)&lt;/span&gt; w.r.t. the density &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the proposal density &lt;span class=&#34;math inline&#34;&gt;\(q(x)\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(q(x)&amp;gt;0\)&lt;/span&gt; whenever &lt;span class=&#34;math inline&#34;&gt;\(h(x)f(x)&amp;gt;0\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\mu=\int h(x)f(x)dx=\int h(x)\frac{f(x)}{g(x)}g(x)dx=E_g[h(X)f(X)/g(X)]\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(f(x)/g(x)\)&lt;/span&gt; called the &lt;strong&gt;likelihood ratio (LR)&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The IS algorithm goes below&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Step 1: simulate &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; samples &lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\(g(x)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Step 2: compute the sample average:
&lt;span class=&#34;math display&#34;&gt;\[\hat{\mu}_{IS}=\frac 1N\sum_{i=1}^N \frac{h(X_i)f(X_i)}{g(X_i)}\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;choosing-the-proposal&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Choosing the proposal&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Var[\hat{\mu}_{IS}] = \frac{\sigma^2_g}{N}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma^2_g = \int \left(\frac{h(x)f(x)}{g(x)}-\mu\right)^2g(x)d x=\int\frac{(h(x)f(x)-\mu g(x))^2}{g(x)}dx\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;if &lt;span class=&#34;math inline&#34;&gt;\(g(x)=h(x)f(x)/\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(h\ge 0\)&lt;/span&gt;, then we have &lt;strong&gt;the optimal case&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_g=0\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;but unattainable: &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is unknown constant&lt;/li&gt;
&lt;li&gt;we may find &lt;span class=&#34;math inline&#34;&gt;\(g(x)\approx h(x)f(x)/\mu\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-weight-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The weight function&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;let &lt;span class=&#34;math inline&#34;&gt;\(w(x)=f(x)/g(x)\)&lt;/span&gt; be the LR
&lt;span class=&#34;math display&#34;&gt;\[\sigma^2_g =\int \frac{(hf)^2}{g}dx -\mu^2\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\int \frac{(hf)^2}{g}dx=E_f[w(X)h(X)^2]=E_g[w(X)^2h(X)^2]\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;if &lt;span class=&#34;math inline&#34;&gt;\(w(x)\)&lt;/span&gt; is bounded, then &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_g\)&lt;/span&gt; is bounded&lt;/li&gt;
&lt;li&gt;if &lt;span class=&#34;math inline&#34;&gt;\(w(x)\)&lt;/span&gt; is unbounded, then &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_g\)&lt;/span&gt; may be unbounded (&lt;strong&gt;the worst case!&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;self-normalized-is-snis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Self-normalized IS (SNIS)&lt;/h2&gt;
&lt;p&gt;What if we cannot compute &lt;span class=&#34;math inline&#34;&gt;\(f/g\)&lt;/span&gt;? Suppose that
&lt;span class=&#34;math display&#34;&gt;\[f(x)=c_f\tilde{f}(x),\ g(x)=c_g\tilde{g}(x)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and we can compute &lt;span class=&#34;math inline&#34;&gt;\(\tilde f,\tilde g\)&lt;/span&gt; but not the constants &lt;span class=&#34;math inline&#34;&gt;\(c_f,c_g\)&lt;/span&gt;. Then we use&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{\mu}_{SNIS}= \frac{\frac 1 N\sum_{i=1}^nh(X_i)\tilde{f}(X_i)/\tilde{g}(X_i)}{\frac 1 N\sum_{i=1}^n\tilde{f}(X_i)/\tilde{g}(X_i)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;or, equivalently,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{\mu}_{SNIS}= \frac{\frac 1 N\sum_{i=1}^Nh(X_i){f}(X_i)/{g}(X_i)}{\frac 1 N\sum_{i=1}^N{f}(X_i)/{g}(X_i)}=\frac{\frac 1 N\sum_{i=1}^Nh(X_i)w(X_i)}{\frac 1 N\sum_{i=1}^Nw(X_i)}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;variance-of-snis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Variance of SNIS&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Taylor expansions
&lt;span class=&#34;math display&#34;&gt;\[f(\bar X,\bar Y)\approx f(\mu_1,\mu_2)+f_x(\mu_1,\mu_2)(\bar X-\mu_1)+f_y(\mu_1,\mu_2)(\bar Y-\mu_2)\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[E[f(\bar X,\bar Y)]\approx f(\mu_1,\mu_2)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Var[f(\bar X,\bar Y)]\approx f_x^2Var[\bar X]+f_y^2Var[\bar Y]+2f_xf_yCov(\bar X,\bar Y)\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;for &lt;span class=&#34;math inline&#34;&gt;\(f(x,y)=x/y\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(f_x=1/y,f_y=-x/y^2\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[Var[f(\bar X,\bar Y)]\approx \frac{\sigma_X^2}{N\mu_2^2}+\frac{\mu_1^2\sigma_Y^2}{N\mu_2^4}-\frac{2\mu_1}{N\mu_2^3}Cov(X,Y)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Var[\hat{\mu}_{SNIS}]\approx \frac{1}{N}E_g[w(X)^2(h(X)-\mu)^2]\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Var[\hat{\mu}_{IS}]= \frac{1}{N}E_g[(h(X)w(X)-\mu)^2]\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;optimal-snis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Optimal SNIS&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;SNIS: &lt;span class=&#34;math inline&#34;&gt;\(g_{opt}(x)\propto f(x)|h(x)-\mu|\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;IS: &lt;span class=&#34;math inline&#34;&gt;\(g_{opt}(x)\propto f(x)|h(x)|\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;effective-sample-size&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Effective sample size&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Unequal weighting raises variance, see, Kong (1992), Evans and Swartz (1995)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;for iid &lt;span class=&#34;math inline&#34;&gt;\(Y_i\)&lt;/span&gt; with variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; and fixed &lt;span class=&#34;math inline&#34;&gt;\(w_i\ge 0\)&lt;/span&gt;,
&lt;span class=&#34;math display&#34;&gt;\[Var\left(\frac{\sum_{i}w_iY_i}{\sum_iw_i}\right)=\frac{\sum_iw_i^2\sigma^2}{(\sum_iw_i)^2}=\frac{\sigma^2}{N_{eff}}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where the effective sample size &lt;span class=&#34;math inline&#34;&gt;\(N_{eff}\)&lt;/span&gt; is defined as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[N_{eff} = \frac{(\sum_{i=1}^Nw_i)^2}{\sum_{i=1}^Nw_i^2}\in [1,N]\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(N_{eff}\)&lt;/span&gt; is small if there are few extremely high weights which would unduly influence the distribution&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;for equal weights, we have &lt;span class=&#34;math inline&#34;&gt;\(N_{eff}=N\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;example-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 1&lt;/h2&gt;
&lt;p&gt;Suppose the posterior distribution is &lt;span class=&#34;math inline&#34;&gt;\(N(\mu,\sigma^2)\)&lt;/span&gt;, the proposal distribution is &lt;span class=&#34;math inline&#34;&gt;\(t_3(\mu,\sigma^2)\)&lt;/span&gt;. Consider &lt;span class=&#34;math inline&#34;&gt;\(\mu=\sigma=2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/course/bchap10_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-1-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 1&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Effective sample size is  9178 / 10000&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/course/bchap10_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-7&#34;&gt;Table 2: &lt;/span&gt;&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n=100&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n=1000&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n=10000&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;exact_value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Mean&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.214328&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.011347&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.945335&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Variance&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.069694&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.688183&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.052519&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;example-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 2&lt;/h2&gt;
&lt;p&gt;Suppose the posterior distribution is &lt;span class=&#34;math inline&#34;&gt;\(t_3(\mu,\sigma^2)\)&lt;/span&gt;, the proposal distribution is &lt;span class=&#34;math inline&#34;&gt;\(N(\mu,\sigma^2)\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Effective sample size is  6180 / 10000&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/course/bchap10_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-8&#34;&gt;Table 3: &lt;/span&gt;&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n=100&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n=1000&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n=10000&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;exact_value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Mean&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.802681&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.784954&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.019707&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Variance&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.630305&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.931088&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.875331&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;is-vs-acceptance-rejection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;IS vs acceptance rejection&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Acceptance-rejection requires bounded LR &lt;span class=&#34;math inline&#34;&gt;\(f/g\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We also have to know a bound&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;IS and SNIS require us to keep track of weights&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Plain IS requires normalized &lt;span class=&#34;math inline&#34;&gt;\(f/g\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Acceptance-rejection samples cost more (due to rejections)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;is-for-rare-events&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;IS for rare events&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;rare events:
&lt;span class=&#34;math display&#34;&gt;\[h(x)=1_A(x), \mu = E_f[h(x)]=\int_A f(x) dx=\epsilon\approx 0\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;coefficient of variation of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[cv:=\frac{\sigma/\sqrt{N}}{\mu}=\frac{\sqrt{\epsilon(1-\epsilon)}}{\sqrt{n}\epsilon}\approx \frac{1}{\sqrt{n\epsilon}}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;to get &lt;span class=&#34;math inline&#34;&gt;\(cv=0.1\)&lt;/span&gt; takes &lt;span class=&#34;math inline&#34;&gt;\(N\ge 100/\epsilon\)&lt;/span&gt;, e.g., &lt;span class=&#34;math inline&#34;&gt;\(\epsilon = 10^{-5}\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(N\ge 10^7\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Taking &lt;span class=&#34;math inline&#34;&gt;\(X\sim f\)&lt;/span&gt; does not get enough data from the important region &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Get more data from &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; (from a proper proposal &lt;span class=&#34;math inline&#34;&gt;\(g(x)\)&lt;/span&gt;), and then correct the bias (the LR function)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;changing-a-parameter&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Changing a parameter&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;norminal distribution &lt;span class=&#34;math inline&#34;&gt;\(p(x;\theta_0)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\theta_0\in\Theta\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;proposal distribution &lt;span class=&#34;math inline&#34;&gt;\(p(x;\theta)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\theta\in\Theta\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;estimator
&lt;span class=&#34;math display&#34;&gt;\[\hat\mu_\theta=\frac{1}{N}\sum_{i=1}^N h(X_i) \frac{p(X_i;\theta_0)}{p(X_i;\theta)}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The importance ratio often simplifies, e.g., in exponential families.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exponential-tilting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exponential tilting&lt;/h2&gt;
&lt;p&gt;Many important distributions can be written in the form
&lt;span class=&#34;math display&#34;&gt;\[p(x;\theta) = a(\theta)\exp[\eta(\theta)^\top T(x)]b(x), \theta\in \Theta\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat\mu_\theta=\frac{a(\theta_0)}{a(\theta)}\frac{1}{N}\sum_{i=1}^N h(X_i) \exp[(\eta(\theta_0)-\eta(\theta))^\top T(X_i)]\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\eta(\theta)\)&lt;/span&gt; is the natrual parameter&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This is called the ‘exponential twisting’.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The goal is to choose &lt;span class=&#34;math inline&#34;&gt;\(\theta\in\Theta\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(Var[\hat\mu_\theta]\)&lt;/span&gt; is minimized.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;a-simple-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A simple example&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;norminal distribution &lt;span class=&#34;math inline&#34;&gt;\(p(x;\theta_0)=N(x;0,1)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;proposal distribution &lt;span class=&#34;math inline&#34;&gt;\(p(x;\theta)=N(x;\theta,1)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\theta\in\mathbb{R}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;target function &lt;span class=&#34;math inline&#34;&gt;\(h(x) = 1\{x&amp;gt;c\}\)&lt;/span&gt;, for large &lt;span class=&#34;math inline&#34;&gt;\(c&amp;gt;0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mu=E[h(X)]=1-\Phi(c)\approx 0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;IS estimator
&lt;span class=&#34;math display&#34;&gt;\[\hat\mu_\theta=\frac{1}{N}\sum_{i=1}^N h(X_i) \frac{N(X_i;0,1)}{N(X_i;\theta,1)}=\frac{1}{N}\sum_{i=1}^N h(X_i) e^{-\frac{2\theta X_i-\theta^2}{2}}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;IS variance &lt;span class=&#34;math inline&#34;&gt;\(Var[\hat\mu_\theta]=\sigma^2_\theta/N\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\sigma^2_\theta=\frac{e^{\theta^2}}{\sqrt{2\pi}}\int_c^\infty e^{-\frac{(x+\theta)^2}{2}}dx-\mu^2=\frac{e^{\theta^2}[1-\Phi(c+\theta)]}{\sqrt{2\pi}}-\mu^2\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the optimal parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta^*=\arg \min_{\theta\in \mathbb{R}} e^{\theta^2}[1-\Phi(c+\theta)]\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-effect-of-different-parameters&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The effect of different parameters&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/course/bchap10_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;the threshold c = 3&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;the true value is 0.0013498980316301&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;the optimal theta is 3.155&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;variance reduction factor is 404&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;applications-in-computational-finance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Applications in Computational Finance&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;P. Glasserman, P. Heidelberger, and P. Shahabuddin. Asymptotically optimal importance
sampling and stratification for pricing path-dependent options. &lt;em&gt;Mathematical Finance&lt;/em&gt;, 9
(2):117–152, 1999.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;P. Glasserman, P. Heidelberger, and P. Shahabuddin. Variance reduction techniques for
estimating value-at-risk. &lt;em&gt;Management Science&lt;/em&gt;, 46(10):1349–1364, 2000.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;P. Glasserman, J. Li. Importance Sampling for Portfolio Credit Risk. &lt;em&gt;Management Science&lt;/em&gt;, 51(11):1643–1656, 2005.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Xie, Fei, &lt;strong&gt;Zhijian He&lt;/strong&gt;, and Xiaoqun Wang. An Importance Sampling-Based Smoothing Approach for Quasi-Monte Carlo Simulation of Discrete Barrier Options. &lt;em&gt;European Journal of Operational Research&lt;/em&gt;, October 17, 2018.
&lt;a href=&#34;https://doi.org/10.1016/j.ejor.2018.10.030&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/j.ejor.2018.10.030&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;importance-sampling-for-portfolio-credit-risk&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Importance Sampling for Portfolio Credit Risk&lt;/h2&gt;
&lt;p&gt;Our interest centers on the distribution of losses
from default over a fixed horizon.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt;: number of obligors&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y_k\)&lt;/span&gt;: default indicator for &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;th obligor, &lt;span class=&#34;math inline&#34;&gt;\(Y_k=1\)&lt;/span&gt; denotes the default; &lt;span class=&#34;math inline&#34;&gt;\(Y_k=0\)&lt;/span&gt; otherwise&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(p_k\)&lt;/span&gt;: marginal probability that &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;th obligor defaults&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(c_k\)&lt;/span&gt;: loss resulting from default of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;th obligor&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(L=c_1Y_1+\dots+c_mY_m\)&lt;/span&gt;: total loss from defaults&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Our goal is to estimate tail probabilities &lt;span class=&#34;math inline&#34;&gt;\(P(L&amp;gt;x)\)&lt;/span&gt;, especially at large values of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;normal-copula-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Normal copula model&lt;/h2&gt;
&lt;p&gt;In the normal copula model, dependence
is introduced through a multivariate normal vector &lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_m\)&lt;/span&gt; of latent variables. Each default indicator is represented as
&lt;span class=&#34;math display&#34;&gt;\[Y_k = 1\{X_k&amp;gt; x_k\},\ k=1,\dots,m.\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[X_k = a_{k1}Z_1+\dots+a_{kd}Z_d+b_k\epsilon_k\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_k\)&lt;/span&gt; are chosen to match &lt;span class=&#34;math inline&#34;&gt;\(P(X_k&amp;gt;x_k)=p_k\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Z_1,\dots,Z_d\stackrel{iid}{\sim} N(0,1)\)&lt;/span&gt; are systematic risk factors&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\epsilon_k\stackrel{iid}{\sim} N(0,1)\)&lt;/span&gt; is an idiosyncratic risk&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(a_{k1},\dots,a_{kd}\)&lt;/span&gt; are the loading factors satisfying &lt;span class=&#34;math inline&#34;&gt;\(\sum_{j=1}^d a_{kj}^2\le 1\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(b_k=\sqrt{1-\sum_{j=1}^d a_{kj}^2}\)&lt;/span&gt; so that &lt;span class=&#34;math inline&#34;&gt;\(X_k\sim N(0,1)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;is-for-independent-obligors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;IS for independent obligors&lt;/h2&gt;
&lt;p&gt;Consider the simple case of independent obligors: &lt;span class=&#34;math inline&#34;&gt;\(a_{ij}=0,\ b_k=1\)&lt;/span&gt;, i.e., &lt;span class=&#34;math inline&#34;&gt;\(Y_k\sim Bin(1,p_k)\)&lt;/span&gt; independently. The idea is to replace each default probability &lt;span class=&#34;math inline&#34;&gt;\(p_k\)&lt;/span&gt; by some other default probability &lt;span class=&#34;math inline&#34;&gt;\(q_k\)&lt;/span&gt;, the basic IS identity is
&lt;span class=&#34;math display&#34;&gt;\[P(L&amp;gt;x)= \tilde{E}\left[1\{L&amp;gt;x\}\prod_{k=1}^m\frac{p_k^{Y_k}(1-p_k)^{1-Y_k}}{q_k^{Y_k}(1-q_k)^{1-Y_k}}\right]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Exponential Twisting&lt;/strong&gt;: Glasserman and Li (2005) chooses
&lt;span class=&#34;math display&#34;&gt;\[q_{k,\theta} = \frac{p_ke^{\theta c_k}}{1+p_k(e^{\theta c_k}-1)}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The original probabilities correspond to &lt;span class=&#34;math inline&#34;&gt;\(\theta=0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;if &lt;span class=&#34;math inline&#34;&gt;\(\theta&amp;gt;0\)&lt;/span&gt;, this does indeed increase the default
probabilities; a larger exposure &lt;span class=&#34;math inline&#34;&gt;\(c_k\)&lt;/span&gt; results in a greater
increase in the default probability.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;choosing-the-optimal-parameter&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Choosing the optimal parameter&lt;/h2&gt;
&lt;p&gt;The LR is reduced to
&lt;span class=&#34;math display&#34;&gt;\[\prod_{k=1}^m\frac{p_k^{Y_k}(1-p_k)^{1-Y_k}}{q_k^{Y_k}(1-q_k)^{1-Y_k}}=\exp(-\theta L+\psi(\theta))\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where
&lt;span class=&#34;math display&#34;&gt;\[\psi(\theta)=\log E[e^{\theta L}]=\sum_{k=1}^m \log(1+p_k(e^{\theta c_k}-1))\]&lt;/span&gt;
is the cumulant generating function (CGF) of L.&lt;/p&gt;
&lt;p&gt;The optimal parameter is
&lt;span class=&#34;math display&#34;&gt;\[\theta^* = \arg \min_{\theta\ge 0} \{M_2(\theta)=E_\theta[1\{L&amp;gt;x\}e^{-2\theta L+2\psi(\theta)}]\}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;choosing-the-sub-optimal-parameter&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Choosing the sub-optimal parameter&lt;/h2&gt;
&lt;p&gt;Observe that for &lt;span class=&#34;math inline&#34;&gt;\(\theta\ge 0\)&lt;/span&gt;,
&lt;span class=&#34;math display&#34;&gt;\[M_2(\theta)\le e^{-2\theta x+2\psi(\theta)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Minimizing &lt;span class=&#34;math inline&#34;&gt;\(M_2(\theta)\)&lt;/span&gt; is difficult, but minimizing
the upper bound is easy:
&lt;span class=&#34;math display&#34;&gt;\[\theta_x = \arg \min_{\theta\ge 0}e^{-2\theta x+2\psi(\theta)}=\arg \max_{\theta\ge 0} \{\theta x-\psi(\theta)\}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The function &lt;span class=&#34;math inline&#34;&gt;\(\psi(\theta)\)&lt;/span&gt; is strictly convex and passes through the origin, so the maximum
is attained at
&lt;span class=&#34;math display&#34;&gt;\[\theta_x = 
\begin{cases}
\text{unique solution to }\psi&amp;#39;(\theta)=x,\ &amp;amp;x&amp;gt;\psi&amp;#39;(0)\\
0,\ &amp;amp;x\le \psi&amp;#39;(0).
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;for the first case, &lt;span class=&#34;math inline&#34;&gt;\(E_{\theta_x}[L]=\psi&amp;#39;(\theta_x)=x\)&lt;/span&gt;, thus, we have shifted the distribution of L so that x is now its mean.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;for the second case, the event &lt;span class=&#34;math inline&#34;&gt;\(\{L&amp;gt;x\}\)&lt;/span&gt; is not rare, so we do not change the probabilities.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;dependent-obligors-conditional-importance-sampling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Dependent Obligors: Conditional Importance Sampling&lt;/h2&gt;
&lt;p&gt;For general factor models, &lt;span class=&#34;math inline&#34;&gt;\(Y_k\)&lt;/span&gt; are dependent; but they are independent conditinal on the systematic risk factors &lt;span class=&#34;math inline&#34;&gt;\(Z_1,\dots,Z_d\)&lt;/span&gt;. So we can apply the so-called conditional IS.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Step 1: simulate &lt;span class=&#34;math inline&#34;&gt;\(Z_1,\dots,Z_d\sim N(0,1)\)&lt;/span&gt; and compute the default probability
&lt;span class=&#34;math inline&#34;&gt;\(p_k=p_k(Z_1,\dots,Z_d)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 2: for simulated &lt;span class=&#34;math inline&#34;&gt;\(p_k\)&lt;/span&gt;, obtain the twisting parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta_x=\theta_x(Z_1,\dots,Z_d)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 3: compute the LR for the &lt;span class=&#34;math inline&#34;&gt;\(\theta_x\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 4: repeat Steps 1–4 &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; times and then obtain the final IS estimate&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;numerical-results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Numerical results&lt;/h2&gt;
&lt;p&gt;The numerical results were reported in Glasserman and Li (2005).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(21\)&lt;/span&gt;-factor model with &lt;span class=&#34;math inline&#34;&gt;\(m=1000\)&lt;/span&gt; obligors&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(p_k = 0.01(1+\sin(16\pi k/m))\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(c_k=(\lceil5k/m\rceil)^2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;VRF = “Variance Reduction Factor”&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(L&amp;gt;x)\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;VRF&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;10,000&lt;/td&gt;
&lt;td&gt;0.0114&lt;/td&gt;
&lt;td&gt;33&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;14,000&lt;/td&gt;
&lt;td&gt;0.0065&lt;/td&gt;
&lt;td&gt;53&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;18,000&lt;/td&gt;
&lt;td&gt;0.0037&lt;/td&gt;
&lt;td&gt;83&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;22,000&lt;/td&gt;
&lt;td&gt;0.0021&lt;/td&gt;
&lt;td&gt;125&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;30,000&lt;/td&gt;
&lt;td&gt;0.0006&lt;/td&gt;
&lt;td&gt;278&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;40,000&lt;/td&gt;
&lt;td&gt;0.0001&lt;/td&gt;
&lt;td&gt;977&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;extensions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Extensions&lt;/h2&gt;
&lt;p&gt;The defual indicators&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y_k=1\{X_k&amp;gt;x_k\}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(X_k\)&lt;/span&gt; follow t copula model&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Joshua C.C. Chan, Dirk P. Kroese. Efficient estimation of large portfolio loss probabilities in t-copula models. &lt;em&gt;European Journal of Operational Research&lt;/em&gt;, 205:361–367, 2010.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(X_k\)&lt;/span&gt; follow another advanced models, e.g., self-exciting model, Giesecke et al. (2010)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Random default exposures: &lt;span class=&#34;math inline&#34;&gt;\(c_k=e_k\ell_k\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\ell_k\in[0,1]\)&lt;/span&gt; denotes a random
percentage loss, and &lt;span class=&#34;math inline&#34;&gt;\(e_k&amp;gt;0\)&lt;/span&gt; are constants.
&lt;span class=&#34;math display&#34;&gt;\[L = \sum_{k=1}^m e_k\ell_k1\{X_k&amp;gt;x_k\}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\ell_k\)&lt;/span&gt; are iid truncated normals or betas&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;cross-entropy&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Cross-entropy&lt;/h2&gt;
&lt;p&gt;The optimal proposal
density is obtained by locating the member &lt;span class=&#34;math inline&#34;&gt;\(p(x;\theta),\theta\in\Theta\)&lt;/span&gt; that minimizes
its cross-entropy distance to the zero-variance proposal
density &lt;span class=&#34;math inline&#34;&gt;\(q^*(x)\propto h(x)p(x;\theta_0)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The minimization of the cross-entropy is equivalent to solving
the following maximization problem
&lt;span class=&#34;math display&#34;&gt;\[\max_{\theta\in\Theta} \int h(x)p(x;\theta_0)\log p(x;\theta)d x=\max_{\theta\in\Theta}  E_{\theta_0}[h(X)\log p(X;\theta)]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since most often an analytical solution to the above maximization
problem is not available, we consider instead its stochastic
counterpart
&lt;span class=&#34;math display&#34;&gt;\[\theta^*=\arg \max_{\theta\in\Theta}\frac 1{N_0}\sum_{i=1}^{N_0}h(X_i)\log p(X_i;\theta),\ X_i\stackrel{iid}{\sim} p(x;\theta_0)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;More detials see Rubinstein (1997), Rubinstein &amp;amp; Kroese (2004).&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 2: Single-parameter models</title>
      <link>/course/bchap02/</link>
      <pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/course/bchap02/</guid>
      <description>&lt;div id=&#34;binomial-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;2.1 Binomial models&lt;/h1&gt;
&lt;div id=&#34;estimating-a-probability-from-binomial-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Estimating a probability from binomial data&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;let &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; be the proportion of successes in the population&lt;/li&gt;
&lt;li&gt;the data &lt;span class=&#34;math inline&#34;&gt;\((y_1,\dots,y_n)\in \{0,1\}^n\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;the total number of successes in the &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; trials is denoted by &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the binomial model is
&lt;span class=&#34;math display&#34;&gt;\[p(y|\theta) = C_n^y\theta^y(1-\theta)^{n-y}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the posterior distribution is
&lt;span class=&#34;math display&#34;&gt;\[p(\theta|y) \propto p(\theta)p(y|\theta)\propto p(\theta)\theta^y(1-\theta)^{n-y}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;: estimating the probability of a female birth&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A total of 241,945 girls and 251,527 boys were born in Paris from 1745 to 1770.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-choose-a-proper-prior&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How to choose a proper prior?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A naive choice for &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)\)&lt;/span&gt; is uniform on the interval &lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt;. Then
&lt;span class=&#34;math display&#34;&gt;\[p(\theta|y) \propto \theta^y(1-\theta)^{n-y}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;that is, &lt;span class=&#34;math inline&#34;&gt;\(\theta|y\sim Beta(y+1,n-y+1)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/course/bchap02_files/figure-html/beta-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
- &lt;span class=&#34;math inline&#34;&gt;\(P(\theta\ge 0.5|y=241945,n=241945+251527)\approx 1.15\times 10^{-42}\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;prediction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Prediction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\tilde y\)&lt;/span&gt; be the result of a new trial
&lt;span class=&#34;math display&#34;&gt;\[P(\tilde y =1|y) = \int_0^1 P(\tilde y=1|\theta,y)p(\theta|y)d \theta=E[\theta|y]=\frac{y+1}{n+2}\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Posterior as compromise between data and prior information&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;prior mean is &lt;span class=&#34;math inline&#34;&gt;\(1/2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;sample mean is &lt;span class=&#34;math inline&#34;&gt;\(y/n\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;posterior mean is &lt;span class=&#34;math inline&#34;&gt;\((y+1)/(n+2)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;the compromise is controlled to a greater extent by the data as the sample size
increases.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;posterior-quantiles-and-intervals&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Posterior quantiles and intervals&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;let &lt;span class=&#34;math inline&#34;&gt;\(T_1\)&lt;/span&gt; be the &lt;span class=&#34;math inline&#34;&gt;\(\alpha/2\)&lt;/span&gt; quantile of the posterior distribution&lt;/li&gt;
&lt;li&gt;let &lt;span class=&#34;math inline&#34;&gt;\(T_2\)&lt;/span&gt; be the &lt;span class=&#34;math inline&#34;&gt;\(1-\alpha/2\)&lt;/span&gt; quantile of the posterior distribution&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(100(1-\alpha)\%\)&lt;/span&gt; posterior interval is &lt;span class=&#34;math inline&#34;&gt;\([T_1,T_2]\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;binterval.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;compare with the usual confidence interval&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;informative-prior-distributions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Informative prior distributions&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: assigning a prior distribution that reflects substantive info.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the likelihood is
&lt;span class=&#34;math display&#34;&gt;\[p(y|\theta) \propto \theta^y(1-\theta)^{n-y}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;choose a prior as a &lt;span class=&#34;math inline&#34;&gt;\(Beta(\alpha,\beta)\)&lt;/span&gt; distribution:
&lt;span class=&#34;math display&#34;&gt;\[p(\theta) \propto \theta^{\alpha-1}(1-\theta)^{\beta-1}\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the parameters &lt;span class=&#34;math inline&#34;&gt;\(\alpha,\beta&amp;gt;0\)&lt;/span&gt; of the prior distribution is called &lt;em&gt;hyperparameters&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the posterior is
&lt;span class=&#34;math display&#34;&gt;\[p(\theta|y)\propto \theta^{\alpha+y-1}(1-\theta)^{n-y+\beta-1}=Beta(\alpha+y,\beta+n-y)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;informative-prior-distributions-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Informative prior distributions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the posterior mean is
&lt;span class=&#34;math display&#34;&gt;\[E[\theta|y]=\frac{\alpha+y}{\alpha+\beta+n}\]&lt;/span&gt;
which lies between the sample proportion &lt;span class=&#34;math inline&#34;&gt;\(y/n\)&lt;/span&gt; and the prior mean &lt;span class=&#34;math inline&#34;&gt;\(\alpha/(\alpha+\beta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the posterior variance is
&lt;span class=&#34;math display&#34;&gt;\[Var[\theta|y]=\frac{(\alpha+y)(\beta+n-y)}{(\alpha+\beta+n)^2(\alpha+\beta+n+1)}=\frac{E[\theta|y](1-E[\theta|y])}{\alpha+\beta+n+1}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;as &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; become large with fixed &lt;span class=&#34;math inline&#34;&gt;\(\alpha,\beta\)&lt;/span&gt;,
&lt;span class=&#34;math display&#34;&gt;\[E[\theta|y]\approx \frac yn,\ Var[\theta|y]\approx \frac 1n\frac yn(1-\frac yn).\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;conjugate-prior-distributions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conjugate prior distributions&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt;: If &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F}\)&lt;/span&gt; is a class of sampling distribution &lt;span class=&#34;math inline&#34;&gt;\(p(y|\theta)\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{P}\)&lt;/span&gt; is a class of prior distributions for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, then the class &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{P}\)&lt;/span&gt; is &lt;em&gt;conjugate&lt;/em&gt; for &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F}\)&lt;/span&gt; if
&lt;span class=&#34;math display&#34;&gt;\[p(\theta|y)\in \mathcal{P} \text{ for all } p(\cdot|\theta)\in\mathcal{F} \text{ and }p(\cdot)\in\mathcal{P}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Advantages of conjugate prior distributions&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;computational convenience&lt;/li&gt;
&lt;li&gt;can be interpreted as additional data&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;exponential-families&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exponential families&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt;: The class &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F}\)&lt;/span&gt; is an &lt;em&gt;exponential family&lt;/em&gt; if all its members have the form
&lt;span class=&#34;math display&#34;&gt;\[p(y_i|\theta)=f(y_i)g(\theta)\exp[\phi(\theta)^\top u(y_i)].\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(f(\cdot)\ge 0\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\phi(\theta)\)&lt;/span&gt; is called the &lt;code&gt;natural parameter&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For iid samples, we have
&lt;span class=&#34;math display&#34;&gt;\[p(y|\theta)=\left(\prod_{i=1}^n f(y_i)\right)g(\theta)^n\exp\left[\phi(\theta)^\top \sum_{i=1}^nu(y_i)\right]
\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[p(y|\theta)\propto g(\theta)^n\exp[\phi(\theta)^\top t(y)]\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;where &lt;span class=&#34;math inline&#34;&gt;\(t(y)=\sum_{i=1}^nu(y_i)\)&lt;/span&gt; (i.e., a &lt;em&gt;sufficient statistic&lt;/em&gt; for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;conjugate-prior-distribution-for-exponential-families&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conjugate prior distribution for exponential families&lt;/h2&gt;
&lt;p&gt;If the prior distribution is specified as
&lt;span class=&#34;math display&#34;&gt;\[p(\theta)\propto g(\theta)^\eta \exp[\phi(\theta)^\top \nu],\]&lt;/span&gt;
then the posterior density is
&lt;span class=&#34;math display&#34;&gt;\[p(\theta|y)\propto g(\theta)^{\eta+n} \exp[\phi(\theta)^\top (\nu+t(y))].\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A list of exponential families&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;binomial distributions&lt;/li&gt;
&lt;li&gt;normal distributions&lt;/li&gt;
&lt;li&gt;exponential distributions&lt;/li&gt;
&lt;li&gt;possion distributions&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;example-probability-of-a-girl-birth-given-placenta-previa&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example: Probability of a girl birth given placenta previa&lt;/h2&gt;
&lt;p&gt;An early study concerning the sex of placenta previa births in Germany found that of a total of 980 births, 437 were female.&lt;/p&gt;
&lt;p&gt;How much evidence does this provide for the claim that the proportion of female births in the population of placenta previa births is less than &lt;strong&gt;0.485&lt;/strong&gt;, the proportion of female births in the general population?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;using a uniform prior: the posterior is &lt;span class=&#34;math inline&#34;&gt;\(Beta(438,544)\)&lt;/span&gt;. The central &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; posterior interval is &lt;span class=&#34;math inline&#34;&gt;\([0.415,0.477]\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;using conjugate prior &lt;span class=&#34;math inline&#34;&gt;\(Beta(\alpha,\beta)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;using nonconjugate prior&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;different-conjugate-prior-distributions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Different conjugate prior distributions&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;span class=&#34;math inline&#34;&gt;\(\alpha/(\alpha+\beta)\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class=&#34;math inline&#34;&gt;\(\alpha+\beta\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;posterior median&lt;/th&gt;
&lt;th&gt;&lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; posterior interval&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;0.500&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;0.446&lt;/td&gt;
&lt;td&gt;[0.415, 0.477]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;0.485&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;0.446&lt;/td&gt;
&lt;td&gt;[0.415, 0.477]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;0.485&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0.446&lt;/td&gt;
&lt;td&gt;[0.415, 0.477]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;0.485&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;0.446&lt;/td&gt;
&lt;td&gt;[0.415, 0.477]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;0.485&lt;/td&gt;
&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;0.447&lt;/td&gt;
&lt;td&gt;[0.416, 0.478]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;0.485&lt;/td&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;td&gt;0.450&lt;/td&gt;
&lt;td&gt;[0.420, 0.479]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;0.485&lt;/td&gt;
&lt;td&gt;200&lt;/td&gt;
&lt;td&gt;0.453&lt;/td&gt;
&lt;td&gt;[0.424, 0.481]&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;Posterior inferences based on a large sample are not sensitive to the prior distribution.&lt;/li&gt;
&lt;li&gt;All the &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; posterior intervals exclude the prior mean.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-effect-of-prior-distributions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The effect of prior distributions&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;bprior.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-a-nonconjugate-prior-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using a nonconjugate prior distribution&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;nonconjugate.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; posterior interval is [0.419, 0.480]&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;normal-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;2.2 Normal models&lt;/h1&gt;
&lt;div id=&#34;estimating-a-normal-mean-with-known-variance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Estimating a normal mean with known variance&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Likelihood function&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(y|\theta) = \prod_{i=1}^n \frac 1{\sqrt{2\pi}\sigma}e^{-\frac{(y_i-\theta)^2}{2\sigma^2}}\propto e^{-\frac{n\theta^2}{2\sigma^2}}e^{\frac{n\theta\bar y}{\sigma^2}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conjugate prior&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\theta\sim N(\mu_0,\tau_0^2)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Posterior distribution&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(\theta|y)\propto e^{-\frac{n\theta^2}{2\sigma^2}}e^{\frac{n\theta\bar y}{\sigma^2}}e^{-\frac{\theta^2}{2\tau_0^2}}e^{\frac{\mu_0\theta}{\tau_0^2}}=N(\mu_n,\tau_n^2)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where
&lt;span class=&#34;math display&#34;&gt;\[\mu_n=\frac{\frac 1{\tau_0^2}\mu_0+\frac n{\sigma^2}\bar y}{\frac 1{\tau_0^2}+\frac n{\sigma^2}},\ \frac1{\tau_n^2}=\frac{1}{\tau_0^2}+\frac n{\sigma^2}.\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;comments&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comments&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;the inverse of the variance plays a prominet role and is called the &lt;em&gt;precision&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;posterior precision = prior precision + data precision&lt;/li&gt;
&lt;li&gt;the posterior mean is expressed as a weighted average of the prior mean &lt;span class=&#34;math inline&#34;&gt;\(\mu_0\)&lt;/span&gt; and the sample mean &lt;span class=&#34;math inline&#34;&gt;\(\bar y\)&lt;/span&gt;, with weights proportional to the precisions.&lt;/li&gt;
&lt;li&gt;what happens if &lt;span class=&#34;math inline&#34;&gt;\(n\to \infty\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\tau_0^2\)&lt;/span&gt; fixed? data info. dominated!&lt;/li&gt;
&lt;li&gt;what happens if &lt;span class=&#34;math inline&#34;&gt;\(\tau_0\to \infty\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; fixed? This would result from assuming &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)\)&lt;/span&gt; is proportional to a constant for &lt;span class=&#34;math inline&#34;&gt;\(\theta\in(-\infty,\infty)\)&lt;/span&gt;. (improper prior, serves as an noninformative prior)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;normal-distribution-with-known-mean-but-unknown-variance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Normal distribution with known mean but unknown variance&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Likelihood function&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(y|\sigma^2)=\prod_{i=1}^n \frac 1{\sqrt{2\pi}\sigma}e^{-\frac{(y_i-\theta)^2}{2\sigma^2}}\propto \sigma^{-n}\exp\left[-\frac{n}{2\sigma^2}\nu\right]\]&lt;/span&gt;
where the sufficient statistic is
&lt;span class=&#34;math display&#34;&gt;\[\nu=\frac 1n\sum_{i=1}^n(y_i-\theta)^2.\]&lt;/span&gt;
&lt;strong&gt;Conjugate prior density&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(\sigma^2)\propto (\sigma^2)^{-(\alpha+1)}e^{-\beta/\sigma^2},\]&lt;/span&gt;
where the hyperparameters is &lt;span class=&#34;math inline&#34;&gt;\((\alpha,\beta)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We may take &lt;span class=&#34;math inline&#34;&gt;\(\theta\sim \text{Inv-}\chi^2(\nu_0,\sigma^2_0)\)&lt;/span&gt; as a prior (i.e., &lt;span class=&#34;math inline&#34;&gt;\(\theta\stackrel d {=}\sigma_0^2\nu_0/\chi^2_{\nu_0}\)&lt;/span&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;normal-distribution-with-known-mean-but-unknown-variance-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Normal distribution with known mean but unknown variance&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Prior density&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(\sigma^2)= \text{Inv-}\chi^2(\nu_0,\sigma^2_0)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Posterior density&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(\sigma^2|y)=\text{Inv-}\chi^2\left(\nu_0+n,\frac{\nu_0\sigma_0^2+n\nu}{\nu_0+n}\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;degree of freedom = sum of the prior and data&lt;/li&gt;
&lt;li&gt;&lt;p&gt;scale = weighted average of the prior and data&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;if &lt;span class=&#34;math inline&#34;&gt;\(\nu_0=0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(p(\sigma^2|y)=\text{Inv-}\chi^2(n,\nu)\)&lt;/span&gt;, as effectively taking &lt;span class=&#34;math inline&#34;&gt;\(p(\sigma^2)\propto 1/\sigma^2\)&lt;/span&gt; (improper prior, serves as an noninformative prior)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;poisson-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;2.3 Poisson models&lt;/h1&gt;
&lt;div id=&#34;poisson-models-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Poisson models&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;: The Possion distribution arises naturally in the study of data taking the form of counts.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;number of customer on the queue over an unit time&lt;/li&gt;
&lt;li&gt;epidemiology – the incidence of diseases&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Likelihood function&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(y|\theta) = \prod_{i=1}^n\frac{\theta^{y_i}e^{-\theta}}{y_i!}\propto \theta^{t(y)}e^{-n\theta}\propto e^{-n\theta}e^{t(y)\log \theta}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(t(y)=\sum_{i=1}^n y_i\)&lt;/span&gt; is the sufficient statistic&lt;/li&gt;
&lt;li&gt;the natural parameter is &lt;span class=&#34;math inline&#34;&gt;\(\log \theta\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Conjugate prior&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(\theta)\propto e^{-\eta\theta}e^{\nu\log \theta}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So we may choose &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)=Gamma(\alpha,\beta)\propto \theta^{\alpha-1}e^{-\beta\theta}\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;poisson-models-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Poisson models&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Prior density&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)=Gamma(\alpha,\beta)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Posterior density&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(p(\theta|y)=Gamma(\alpha+n\bar y,\beta+n)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Marginal density&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(y_i)=C_{\alpha+y_i-1}^{y_i} \left(\frac{\beta}{\beta+1}\right)^\alpha\left(\frac{1}{\beta+1}\right)^{y_i}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(y_i\sim \text{Neg-bin}(\alpha,\beta)\)&lt;/span&gt;, i.e., the negative binomial distribution&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;possion-models-an-extension&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Possion models: an extension&lt;/h2&gt;
&lt;p&gt;In many applications, it is convenient to extend the Possion model for data pionts &lt;span class=&#34;math inline&#34;&gt;\(y_1,\dots,y_n\)&lt;/span&gt; to the form
&lt;span class=&#34;math display&#34;&gt;\[y_i\sim Poission(x_i\theta),\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the values &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; are known positive values of an explanatory variable &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, called the &lt;em&gt;exposure&lt;/em&gt; of the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;th unit&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is unknown, called the &lt;em&gt;rate&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Prior density&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)=Gamma(\alpha,\beta)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Posterior density&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(\theta|y)=Gamma(\alpha+\sum_{i=1}^ny_i,\beta+\sum_{i=1}^nx_i)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;: Bayesian inference for the cancer death rates (p.48)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exponential-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;2.4 Exponential models&lt;/h1&gt;
&lt;div id=&#34;exponential-models-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exponential models&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;: The expoential distribution is commonly used to model ‘waiting times’ and other continuous, poisitive, real-valued random variables. It has a ‘memoryless’ property that makes it a natural model for survival or lifetime data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Likelihood function&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(y|\theta) = \prod_{i=1}^n\theta \exp(-y_i\theta)= \theta^{n}e^{-n\bar y \theta}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Prior density&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)=Gamma(\alpha,\beta)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Posterior density&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(\theta|y)=Gamma(\alpha+n,\beta+n\bar y)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Population&lt;/th&gt;
&lt;th&gt;Parameter&lt;/th&gt;
&lt;th&gt;Conjugate prior&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Binomial&lt;/td&gt;
&lt;td&gt;probability of success&lt;/td&gt;
&lt;td&gt;Beta dist.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Possion&lt;/td&gt;
&lt;td&gt;mean&lt;/td&gt;
&lt;td&gt;Gamma dist.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Exponential&lt;/td&gt;
&lt;td&gt;inverse mean&lt;/td&gt;
&lt;td&gt;Gamma dist.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Normal (known variance)&lt;/td&gt;
&lt;td&gt;mean&lt;/td&gt;
&lt;td&gt;Normal dist.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Normal (known mean)&lt;/td&gt;
&lt;td&gt;variance&lt;/td&gt;
&lt;td&gt;Inv-Gamma dist.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;end-notes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;End notes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;two kinds of prior distributions: uniform (noninformative) and conjugate (informative)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;some other noninformative prior distributions: Jeffreys’ prior etc. See pp.52-56&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;noninformative prior are often useful when it does not seem to be worth the effort to quantify one’s real prior knowledge as a probability distribution&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;when using conjugate prior, it remains to choose the hyperparameters; see Chapter 5 for hierarchical models&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 3: Introduction to multiparameter models</title>
      <link>/course/bchap03/</link>
      <pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/course/bchap03/</guid>
      <description>&lt;div id=&#34;nuisance-parameters&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Nuisance parameters&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;there are more than one unknown or unobservable parameters&lt;/li&gt;
&lt;li&gt;conclusions will often be drawn about one, or only a few parameters at a time&lt;/li&gt;
&lt;li&gt;&lt;p&gt;there is no interest in making inferences about many of the unknown parameters – &lt;em&gt;nuisance parameters&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;suppose &lt;span class=&#34;math inline&#34;&gt;\(\theta=(\theta_1,\theta_2)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;interest centers only on &lt;span class=&#34;math inline&#34;&gt;\(\theta_1\)&lt;/span&gt;; &lt;span class=&#34;math inline&#34;&gt;\(\theta_2\)&lt;/span&gt; is a ‘nuisance’ parameter.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the joint posterior density:
&lt;span class=&#34;math display&#34;&gt;\[p(\theta_1,\theta_2|y)\propto p(y|\theta_1,\theta_2)p(\theta_1,\theta_2)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the marginal posterior density:
&lt;span class=&#34;math display&#34;&gt;\[p(\theta_1|y)=\int p(\theta_1,\theta_2|y)d\theta_2=\int p(\theta_1|\theta_2,y)p(\theta_2|y)d\theta_2\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;normal-data-with-a-noninformative-prior-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Normal data with a noninformative prior distribution&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Likelihood function&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(y|\mu,\sigma^2)=\prod_{i=1}^n \frac 1{\sqrt{2\pi}\sigma}e^{-\frac{(y_i-\mu)^2}{2\sigma^2}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Noninformative prior distribution&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(\mu,\sigma^2)\propto (\sigma^2)^{-1}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Posterior distribution&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(\mu,\sigma^2|y)\propto \sigma^{-n-2}e^{-\frac{\sum_{i=1}^n(y_i-\mu)^2}{2\sigma^2}}=\sigma^{-n-2}e^{-\frac{(n-1)s^2+n(\bar y-\mu)^2}{2\sigma^2}}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(s^2=\frac 1{n-1}\sum_{i=1}^n(y_i-\bar y)^2\)&lt;/span&gt; is the sample variance&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;normal-data-with-a-noninformative-prior-distribution-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Normal data with a noninformative prior distribution&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Conditional posterior distribution&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(\mu|\sigma^2,y)\sim N(\bar y,\sigma^2/n)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Marginal posterior distribution &lt;span class=&#34;math inline&#34;&gt;\(p(\sigma^2|y)\)&lt;/span&gt;&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(\sigma^2|y)\propto \int \sigma^{-n-2}e^{-\frac{(n-1)s^2+n(\bar y-\mu)^2}{2\sigma^2}} d\mu=(\sigma^2)^{-\frac{n+1}2}e^{-\frac{(n-1)s^2}{2\sigma^2}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma^2|y\sim \text{Inv-}\chi^2(n-1,s^2)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;normal-data-with-a-noninformative-prior-distribution-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Normal data with a noninformative prior distribution&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Marginal posterior distribution &lt;span class=&#34;math inline&#34;&gt;\(p(\mu|y)\)&lt;/span&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(\mu|y)\propto \int_0^\infty \sigma^{-n-2}e^{-\frac{(n-1)s^2+n(\bar y-\mu)^2}{2\sigma^2}} d\sigma^2\propto \left[1+\frac{n(\mu-\bar y)^2}{(n-1)s^2}\right]^{-\frac n2}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mu|y\sim t_{n-1}(\bar y,s^2/n),\ \frac{\mu-\bar y}{s/\sqrt{n}}\Big|y\sim t_{n-1}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Posterior predictive distribution for a future observation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\tilde y|y \sim t_{n-1}(\bar y,(1+1/n)s^2)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-estimating-the-speed-of-light&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example: Estimating the speed of light&lt;/h2&gt;
&lt;p&gt;Simon Newcomb set up an experiment in 1882 to measure the speed of light. Newcom measured the amount of time rquired for light to travel a distance of 7442 meters (66 measurements, from Stigler (1977), the data are recorded as deviations from 24800 nanoseconds).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(n=66,\ \bar y = 26.2,\ s = 10.8\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((\mu-26.2)/(10.8/\sqrt{66})|y\sim t_{65}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; central posterior interval for &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(26.2\pm 10.8t_{65,0.975}/\sqrt{66}=[23.6,28.8]\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;the speed of light is 299792458 m/s, so the true value for &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(23.8\)&lt;/span&gt; nanoseconds&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;example-estimating-the-speed-of-light-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example: Estimating the speed of light&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;newcomb.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;normal-data-with-a-conjugate-prior-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Normal data with a conjugate prior distribution&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Prior distribution&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[\mu|\sigma^2\sim N(\mu_0,\sigma^2/\kappa_0),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma^2\sim \text{Inv-}\chi^2(\nu_0,\sigma^2).\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(\mu,\sigma^2)\propto \sigma^{-1}(\sigma^2)^{-(\nu_0/2+1)}\exp\left(-\frac 1{2\sigma^2}[\nu_0\sigma^2+\kappa_0(\mu_0-\mu)^2]\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;denoted by &lt;span class=&#34;math inline&#34;&gt;\(\text{N-Inv-}\chi^2(\mu_0,\sigma^2_0/\kappa_0;\nu_0,\sigma_0^2)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;normal-data-with-a-conjugate-prior-distribution-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Normal data with a conjugate prior distribution&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Posterior distribution&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mu,\sigma^2|y\sim \text{N-Inv-}\chi^2(\mu_n,\sigma^2_n/\kappa_n;\nu_n,\sigma_n^2)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{cases}
\mu_n &amp;amp;= \frac{\kappa_0}{\kappa_0+n}\mu_0+\frac{n}{\kappa_0+n}\bar y\\
\kappa_n &amp;amp;= \kappa_0+n\\
\nu_n&amp;amp;=\nu_0+n\\
\nu_n\sigma_n^2 &amp;amp;= \nu_0\sigma_0^2+(n-1)s^2+\frac{\kappa_0n}{\kappa_0+n}(\bar y-\mu_0)^2
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mu|\sigma^2,y\sim N(\mu_n,\sigma^2/\kappa_n)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2|y\sim \text{Inv-}\chi^2(\nu_n,\sigma_n^2)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mu|y\sim t_{\nu_n}(\mu_n,\sigma_n^2/\kappa_n)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;multinormal-model-for-categorical-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multinormal model for categorical data&lt;/h2&gt;
&lt;p&gt;The multinomial sampling distribution is used to describe data for which each observation is one of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; possible outcomes. If &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is the vector of counts of the number of observations of each outcome, then
&lt;span class=&#34;math display&#34;&gt;\[p(y|\theta)\propto \prod_{j=1}^k\theta_j^{y_j},\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\sum_{j=1}^k\theta_j=1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conjugate prior&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(\theta|\alpha)\propto \prod_{j=1}^k\theta_j^{\alpha_j-1}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dirichlet distribution&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Posterior distribution&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(\alpha|\theta)\propto \prod_{j=1}^k\theta_j^{y_j+\alpha_j-1}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multivariate-normal-model-with-known-variance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multivariate normal model with known variance&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Likelihood function&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(y_1,\dots,y_n|\mu,\Sigma)\propto |\Sigma|^{-n/2}\exp\left(-\frac 12\sum_{i=1}^n(y_i-\mu)^\top\Sigma^{-1}(y_i-\mu)\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conjuate prior&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\mu\sim N(\mu_0,\Lambda_0)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Posterior distribution&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\mu|y\sim N(\mu_n,\Lambda_n)\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mu_n=(\Lambda_n^{-1}+n\Sigma^{-1})^{-1}(\Lambda_0^{-1}\mu_0+n\Sigma^{-1}\bar y)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Lambda_n^{-1} = \Lambda_n^{-1}+n\Sigma^{-1}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;multivariate-normal-model-with-unknown-mean-and-variance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multivariate normal model with unknown mean and variance&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Prior distribution&lt;/strong&gt;: the normal-inverse-Wishart &lt;span class=&#34;math inline&#34;&gt;\((\mu_0,\Lambda_0/\kappa_0;\nu_0,\Lambda_0)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\Sigma\sim \text{Inv-Wishart}_{\nu_0}(\Lambda_0^{-1})\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\mu|\Sigma\sim N(\mu_0,\Sigma/\kappa_0)\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[p(\mu,\Sigma)\propto |\Sigma|^{-\frac{\nu_0+d}{2}-1}\exp\left(-\frac{1}{2}tr(\Lambda_0\Sigma^{-1})-\frac {\kappa_0}2(\mu-\mu_0)^\top\Sigma^{-1}(\mu-\mu_0)\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Posterior distribution&lt;/strong&gt;: the normal-inverse-Wishart &lt;span class=&#34;math inline&#34;&gt;\((\mu_n,\Lambda_n/\kappa_n;\nu_0,\Lambda_n)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{cases}
\mu_n &amp;amp;= \frac{\kappa_0}{\kappa_0+n}\mu_0+\frac{n}{\kappa_0+n}\bar y\\
\kappa_n &amp;amp;= \kappa_0+n\\
\nu_n&amp;amp;=\nu_0+n\\
\Lambda_n &amp;amp;= \Lambda_0+\sum_{i=1}^n(y_i-\bar y)(y_i-\bar y)^\top+\frac{\kappa_0n}{\kappa_0+n}(\bar y-\mu_0)(\bar y-\mu_0)^\top
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 3: Asymptotics and connections to non-Bayesian approaches</title>
      <link>/course/bchap04/</link>
      <pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/course/bchap04/</guid>
      <description>&lt;div id=&#34;large-sample-theory&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Large-sample theory&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Assumptions and notations&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;true distribution: &lt;span class=&#34;math inline&#34;&gt;\(y_i\stackrel {iid}{\sim} f(\cdot)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\theta\in\Theta\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;prior distribution: &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;model distribution: &lt;span class=&#34;math inline&#34;&gt;\(p(y_i|\theta)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Kullback-Leibler divergence&lt;/em&gt;: a measure of ‘discrepancy’ between the model and the true distribution
&lt;span class=&#34;math display&#34;&gt;\[KL(\theta)= E\left[\log\left(\frac{f(y_i)}{p(y_i|\theta)}\right)\right]=\int \log\left(\frac{f(y_i)}{p(y_i|\theta)}\right)f(y_i)dy_i\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\theta_0\)&lt;/span&gt;: the &lt;strong&gt;unique minimizer&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(KL(\theta)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;if &lt;span class=&#34;math inline&#34;&gt;\(f(y_i) = p(y_i|\theta)\)&lt;/span&gt; then &lt;span class=&#34;math inline&#34;&gt;\(\theta=\theta_0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;convergence-of-the-posterior-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Convergence of the posterior distribution&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Discrete parmeter space&lt;/strong&gt;: If the parameter space &lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt; is finite and &lt;span class=&#34;math inline&#34;&gt;\(P(\theta=\theta_0)&amp;gt;0\)&lt;/span&gt;, then
&lt;span class=&#34;math display&#34;&gt;\[P(\theta=\theta_0|y)\to 1\text{ as }n\to \infty,\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\theta_0=\arg_{\theta\in\Theta} KL(\theta)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Continuous parmeter space&lt;/strong&gt;: If &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is defined on a compace set &lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is a neighborhood of &lt;span class=&#34;math inline&#34;&gt;\(\theta_0\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(P(\theta\in A)&amp;gt;0\)&lt;/span&gt;, then
&lt;span class=&#34;math display&#34;&gt;\[P(\theta\in A|y)\to 1\text{ as }n\to \infty,\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\theta_0=\arg_{\theta\in\Theta} KL(\theta)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;See the proofs in Appendix B.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;normal-approximations-to-the-posterior-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Normal approximations to the posterior distribution&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta\)&lt;/span&gt;: the posterior mode&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Taylor series expansion of &lt;span class=&#34;math inline&#34;&gt;\(\log p(\theta|y)\)&lt;/span&gt; gives
&lt;span class=&#34;math display&#34;&gt;\[\log(\theta|y) = \log p(\hat \theta|y)-\frac 12 (\theta-\hat\theta)^\top I(\hat \theta) (\theta-\hat\theta) + \cdots \]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(I(\theta)\)&lt;/span&gt; is the &lt;em&gt;observed&lt;/em&gt; information
&lt;span class=&#34;math display&#34;&gt;\[I(\theta)=-\frac{d^2}{d\theta^2}\log p(\theta|y)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Normal approximation: &lt;span class=&#34;math inline&#34;&gt;\(p(\theta|y)\approx N(\hat\theta,[I(\hat\theta)]^{-1})\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Fisher information&lt;/em&gt;:
&lt;span class=&#34;math display&#34;&gt;\[J(\theta)=-E_f\left[\frac{d^2}{d\theta^2}\log p(y_j|\theta)\right]\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;convergence-of-the-posterior-distribution-to-normality&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Convergence of the posterior distribution to normality&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Under some regularity conditions (notably that &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; not be on the boundary of &lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt;), as &lt;span class=&#34;math inline&#34;&gt;\(n\to \infty\)&lt;/span&gt;, the posterior distribution of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; approaches normality with mean &lt;span class=&#34;math inline&#34;&gt;\(\theta_0\)&lt;/span&gt; and variance &lt;span class=&#34;math inline&#34;&gt;\([nJ(\theta_0)]^{-1}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\theta_0=\arg_{\theta\in\Theta} KL(\theta)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; is the Fisher information.&lt;/p&gt;
&lt;p&gt;Oberved that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat\theta\to \theta_0\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(n\to \infty\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(I(\hat\theta)=-\frac{d^2}{d\theta^2}\log p(\hat\theta)-\sum_{i=1}^n\frac{d^2}{d\theta^2}\log p(y_i|\hat\theta)\approx nJ(\theta_0)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(J(\theta_0)=\frac{d^2}{d\theta^2} KL(\theta_0)&amp;gt;0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;counterexamples-to-the-theorems&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Counterexamples to the theorems&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;underidentified models: &lt;span class=&#34;math inline&#34;&gt;\(p(y|\theta)\)&lt;/span&gt; is equal for a range of values of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;nonindentified parameters: for example, consider the model,
&lt;span class=&#34;math display&#34;&gt;\[\left(
\begin{matrix}
u\\
v
\end{matrix}
\right)\sim N \left( \left(\begin{matrix}
0\\
0
\end{matrix}
\right),\left(\begin{matrix}
1&amp;amp;\rho\\
\rho &amp;amp; 1
\end{matrix}
\right)\right)\]&lt;/span&gt;
only one of &lt;span class=&#34;math inline&#34;&gt;\(u,v\)&lt;/span&gt; is observed from each pair &lt;span class=&#34;math inline&#34;&gt;\((u,v)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;number of parameters increasing with sample sizes: new latent parameters with each data point&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;point-estimation-consistency-and-efficiency&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Point estimation, consistency, and efficiency&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;point estimations&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;posterior mode &lt;span class=&#34;math inline&#34;&gt;\(\hat\theta(y)=\arg \max_{\theta\in\Theta} p(\theta|y)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;posterior mean &lt;span class=&#34;math inline&#34;&gt;\(\hat\theta(y)=E[\theta|y]=\int \theta p(\theta|y)d \theta\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;posterior median &lt;span class=&#34;math inline&#34;&gt;\(\hat\theta(y)=F^{-1}_{\theta|y}(0.5)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;consistency&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\hat\theta(y)\to \theta_0\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(n\to \infty\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;asymptotic unbiasedness&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(E[\hat\theta|\theta_0]\to\theta_0\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(n\to \infty\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;efficiency&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[\text{eff}(\hat\theta)=\frac{\inf_T E[(T(y)-\theta_0)^2|\theta_0]}{E[(\hat\theta-\theta_0)^2|\theta_0]}\le 1\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;asymptotically efficient&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\text{eff}(\hat\theta)\to 1\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(n\to \infty\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 5: Hierarchial models</title>
      <link>/course/bchap05/</link>
      <pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/course/bchap05/</guid>
      <description>&lt;div id=&#34;introduction-to-hierarchial-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction to hierarchial models&lt;/h2&gt;
&lt;p&gt;Many statistical applications involve multiple parameters (say, &lt;span class=&#34;math inline&#34;&gt;\(\theta_1,\dots,\theta_J\)&lt;/span&gt;) that can be regarded as related or connected in some way by the structure of the problem.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;for the group &lt;span class=&#34;math inline&#34;&gt;\(j\in 1{:}J\)&lt;/span&gt;, we have the observed data &lt;span class=&#34;math inline&#34;&gt;\(y_{ij}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(i=1,\dots,n_j\)&lt;/span&gt; from the population distribution with unknown parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;we use a prior distribution in which the &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt;’s are viewed as a sample from a common &lt;em&gt;population distribution&lt;/em&gt;, say &lt;span class=&#34;math inline&#34;&gt;\(p(\theta|\phi)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; is known as &lt;em&gt;hyperparameters&lt;/em&gt;. Assume that &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt; are iid, i.e.,
&lt;span class=&#34;math display&#34;&gt;\[p(\theta|\phi)=\prod_{j=1}^Jp(\theta_j|\phi)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;hierarchical-model-for-rats-experiment&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hierarchical model for Rats experiment&lt;/h2&gt;
&lt;p&gt;The experiment is used to estimate the probability &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; of tumor in a population of female laboratory rats of type ‘F344’ that receive a zero dose of the drug. The data show that 4 out of 14 rats developed a kind of tumor.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;assume a binomial model for the number of tumors&lt;/li&gt;
&lt;li&gt;select a prior from the conjugate family, i.e., &lt;span class=&#34;math inline&#34;&gt;\(\theta\sim Beta(\alpha,\beta)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;the posterior is therefore &lt;span class=&#34;math inline&#34;&gt;\(Beta(\alpha+1,\beta+10)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The question is how to determine the hyperparameters &lt;span class=&#34;math inline&#34;&gt;\(\phi=(\alpha,\beta)\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;historical data are available on previous experiments on similar groups of rats: in the jth historical experiments, let the number of rats with tumors be &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt; and the total number of rats be &lt;span class=&#34;math inline&#34;&gt;\(n_j\)&lt;/span&gt;, the parameters for the populations are &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(j=1,\dots,70\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;for current experiment, let &lt;span class=&#34;math inline&#34;&gt;\(y_{71},n_{71},\theta_{71}\)&lt;/span&gt; be the associated notations.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;historical-data-for-the-70-historical-experiments&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Historical data for the 70 historical experiments&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  1  1  2
## [24]  2  2  2  2  2  2  2  2  1  5  2  5  3  2  7  7  3  3  2  9 10  4  4
## [47]  4  4  4  4  4 10  4  4  4  5 11 12  5  5  6  5  6  6  6  6 16 15 15
## [70]  9  4&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 20 20 20 20 20 20 20 19 19 19 19 18 18 17 20 20 20 20 19 19 18 18 25
## [24] 24 23 20 20 20 20 20 20 10 49 19 46 27 17 49 47 20 20 13 48 50 20 20
## [47] 20 20 20 20 20 48 19 19 19 22 46 49 20 20 23 19 22 20 20 20 52 46 47
## [70] 24 14&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;viewed-as-separate-models-using-uniform-priors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Viewed as separate models using uniform priors&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;separate_model.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;viewed-as-a-pooled-model-using-uniform-prior&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Viewed as a pooled model using uniform prior&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;pool_model.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-the-historical-data-to-estimate-the-hyperparameters&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using the historical data to estimate the hyperparameters&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the sample mean and standard deviation of the 70 values &lt;span class=&#34;math inline&#34;&gt;\(y_i/n_i\)&lt;/span&gt; are 0.136 and 0.103&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;let &lt;span class=&#34;math inline&#34;&gt;\(E[\theta]=\frac{\alpha}{\alpha+\beta}=0.136\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Var[\theta]=\frac{E[\theta](1-E[\theta])}{\alpha+\beta+1}=0.103\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\alpha}=1.4,\ \hat{\beta}=8.6\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;for the current exeriment, the posterior for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(Beta(5.4,18.6)\)&lt;/span&gt;, posterior mean is &lt;span class=&#34;math inline&#34;&gt;\(0.223\)&lt;/span&gt;, standard deviation is 0.083.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are several logical and practical problems with the approach of directly estimating a prior distribution from existing data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the data will be used twice for inference about the first 70 experiments – overestimate our precision&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the point estimate for &lt;span class=&#34;math inline&#34;&gt;\(\alpha,\beta\)&lt;/span&gt; seems arbitrary that necessarily ignores some posterior uncertainty&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;this is not the logic of Bayesian inference&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-full-bayesian-treatment-of-the-hierarchical-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The full Bayesian treatment of the hierarchical model&lt;/h2&gt;
&lt;p&gt;Suppose the hyperparameters &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; has its own prior distribution &lt;span class=&#34;math inline&#34;&gt;\(p(\phi)\)&lt;/span&gt;, which is called &lt;em&gt;hyperprior distribution&lt;/em&gt;. The appropriate Bayesian posterior distribution is of the vector &lt;span class=&#34;math inline&#34;&gt;\((\phi,\theta)\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the joint prior distribution is
&lt;span class=&#34;math display&#34;&gt;\[p(\phi,\theta)=p(\phi)p(\theta|\phi)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the joint posterior distribution is
&lt;span class=&#34;math display&#34;&gt;\[p(\phi,\theta|y)\propto p(\phi,\theta)p(y|\phi,\theta)=p(\phi)p(\theta|\phi)p(y|\theta)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Previously, we assumed &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; was known, which is unrealistic; now we include the uncertainty in &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; in the model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fully-bayesian-analysis-of-conjugate-hierarchical-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fully Bayesian analysis of conjugate hierarchical models&lt;/h2&gt;
&lt;p&gt;Consider the setting in which &lt;span class=&#34;math inline&#34;&gt;\(p(\theta|\phi)\)&lt;/span&gt; is conjugate to the likelihood &lt;span class=&#34;math inline&#34;&gt;\(p(y|\theta)\)&lt;/span&gt;. For this case, it is easy to determine analytically &lt;span class=&#34;math display&#34;&gt;\[p(\theta|\phi,y)\propto p(\theta|\phi)p(y|\theta)\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the joint posterior density:
&lt;span class=&#34;math display&#34;&gt;\[p(\phi,\theta|y)\propto p(\phi)p(\theta|\phi)p(y|\theta)\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;the marginal posterior density &lt;span class=&#34;math inline&#34;&gt;\(p(\phi|y)\)&lt;/span&gt; can be computed via
&lt;span class=&#34;math display&#34;&gt;\[p(\phi|y)=\int p(\phi,\theta|y)d \theta\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{or }p(\phi|y)=\frac{p(\phi,\theta|y)}{p(\theta|\phi,y)}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;application-to-the-model-for-rat-tumors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Application to the model for rat tumors&lt;/h2&gt;
&lt;p&gt;The binomial model:
&lt;span class=&#34;math display&#34;&gt;\[y_j\sim Bin(n_j,\theta_j),\ j=1,\dots,J=71\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt; are assumed to be independent samples from a beta distribution:
&lt;span class=&#34;math display&#34;&gt;\[\theta_j\sim Beta(\alpha,\beta)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The joint posterior density is
&lt;span class=&#34;math display&#34;&gt;\[p(\theta,\alpha,\beta|y)\propto p(\alpha,\beta)p(\theta|\alpha,\beta)p(y|\theta)\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\propto p(\alpha,\beta)\prod_{j=1}^J\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\theta_j^{\alpha-1}(1-\theta_j)^{\beta-1}\prod_{j=1}^J\theta_j^{y_j}(1-\theta_j)^{n_j-y_j}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(\theta|\alpha,\beta,y)=\prod_{j=1}^J\frac{\Gamma(\alpha+\beta+n_j)}{\Gamma(\alpha+y_j)\Gamma(\beta+n_j-y_j)}\theta_j^{\alpha+y_i-1}(1-\theta_j)^{\beta+n_j-y_j-1}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;application-to-the-model-for-rat-tumors-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Application to the model for rat tumors&lt;/h2&gt;
&lt;p&gt;The marginal posterior density:
&lt;span class=&#34;math display&#34;&gt;\[p(\alpha,\beta|y)\propto p(\alpha,\beta)\prod_{j=1}^J\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\frac{\Gamma(\alpha+y_j)\Gamma(\beta+n_j-y_j)}{\Gamma(\alpha+\beta+n_j)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Choosing a noninformative hyperprior distribution:
&lt;span class=&#34;math display&#34;&gt;\[p(\alpha,\beta)\propto (\alpha+\beta)^{-5/2}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This implies that &lt;span class=&#34;math inline&#34;&gt;\((\alpha/(\alpha+\beta),(\alpha+\beta)^{-1/2})\)&lt;/span&gt; is uniformly distributed.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the prior mean is &lt;span class=&#34;math inline&#34;&gt;\(\alpha/(\alpha+\beta)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;the prior variance is approximately &lt;span class=&#34;math inline&#34;&gt;\((\alpha+\beta)^{-1}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-of-the-marginal-posterior-density&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plot of the marginal posterior density&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;alphabeta.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;compare-the-separate-model-and-hierarchical-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Compare the separate model and hierarchical model&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;hier_sep.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hierarchical-model-based-on-normal-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hierarchical model based on normal distribution&lt;/h2&gt;
&lt;p&gt;Consider &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; independent experiments, with experiment &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; estimating &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt; form &lt;span class=&#34;math inline&#34;&gt;\(n_j\)&lt;/span&gt; independent distributed data points &lt;span class=&#34;math inline&#34;&gt;\(y_{ij}\)&lt;/span&gt;, each with known error variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;, that is
&lt;span class=&#34;math display&#34;&gt;\[y_{ij}|\theta_j\stackrel{iid}{\sim} N(\theta_j,\sigma^2), \text{ for }i=1,\dots,n_j;\ j=1,\dots,J\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;denote the sample mean of each group &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; as
&lt;span class=&#34;math display&#34;&gt;\[\bar{y}_{\cdot j}=\frac 1{n_j}\sum_{i=1}^{n_j}y_{ij}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;let &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_j=\sigma^2/n_j\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the likelihood for each &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[\bar{y}_{\cdot j}|\theta_j\sim N(\theta_j,\sigma_j^2)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;for the convenience of conjugacy, assume the paramerters &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt; are drawn from a normal distribution with hyperparameters &lt;span class=&#34;math inline&#34;&gt;\(\mu,\tau\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(\theta_1,\dots,\theta_J|\mu,\tau)=\prod_{j=1}^J N(\theta_j|\mu,\tau^2)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;assign noninformative uniform hyperprior density to &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(\mu,\tau)=p(\mu|\tau)p(\tau)\propto p(\tau)\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;prior distribution for &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(p(\tau)\propto 1\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the joint posterior density is
&lt;span class=&#34;math display&#34;&gt;\[p(\theta,\mu,\tau|y)\propto p(\mu,\tau)p(\theta|\mu,\tau)p(y|\theta)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(\theta,\mu,\tau|y)\propto p(\mu,\tau)\prod_{j=1}^J N(\theta_j|\mu,\tau^2)\prod_{j=1}^JN(\bar{y}_{\cdot j}|\theta_j,\sigma_j^2)\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the conditional posterior distirbution:
&lt;span class=&#34;math display&#34;&gt;\[\theta_j|\mu,\tau,y\sim N(\hat{\theta}_j,V_j)\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where
&lt;span class=&#34;math display&#34;&gt;\[\hat{\theta}_j=\frac{\frac 1{\sigma^2}\bar{y}_{\cdot j}+\frac 1{\tau^2}\mu}{\frac 1{\sigma^2}+\frac 1{\tau^2}},\ V_j=\frac{1}{\frac 1{\sigma^2}+\frac 1{\tau^2}}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the marginal posterior density can be computed in a simple way
&lt;span class=&#34;math display&#34;&gt;\[p(\mu,\tau|y)\propto p(\mu,\tau)p(y|\mu,\tau)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bar{y}_{\cdot j}|\mu,\tau\sim N(\mu,\sigma_j^2+\tau^2)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(\mu,\tau|y)\propto p(\mu,\tau)\prod_{j=1}^JN(\bar{y}_{\cdot j}|\mu,\sigma_j^2+\tau^2)\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;posterior distribution of &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\mu|\tau,y\sim N(\hat{\mu},V_{\mu})\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where
&lt;span class=&#34;math display&#34;&gt;\[\hat{\mu}=\frac{\sum_{j=1}^J \frac 1{\sigma_j^2+\tau^2}\bar{y}_{\cdot j}}{\sum_{j=1}^J \frac 1{\sigma_j^2+\tau^2}},\ V_{\mu}^{-1}=\sum_{j=1}^J \frac 1{\sigma_j^2+\tau^2}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;posterior distribution of &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(\tau|y)=\frac{p(\mu,\tau|y)}{p(\mu|\tau,y)}\propto \frac{p(\tau)\prod_{j=1}^JN(\bar{y}_{\cdot j}|\mu,\sigma_j^2+\tau^2)}{N(\mu|\hat{\mu},V_{\mu})}\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(\tau|y)\propto p(\tau)V_{\mu}^{1/2}\prod_{j=1}^J(\sigma_j^2+\tau^2)^{-1/2}\exp\left(-\frac{(\bar{y}_{\cdot j}-\hat{\mu})^2}{2(\sigma_j^2+\tau^2)}\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-parallel-experiments-in-eight-schools&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example: parallel experiments in eight schools&lt;/h2&gt;
&lt;p&gt;A study was performanced for the Educational Testing Service to analyze the effects of special coaching programs on test scores. Seperate randomized experiments were performed to estimate the effects of coaching programs for the SAT-V (Verbal).&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;33%&#34; /&gt;
&lt;col width=&#34;33%&#34; /&gt;
&lt;col width=&#34;33%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;School&lt;/th&gt;
&lt;th&gt;Estiamted treatment effect &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;Standard error of effect estimate &lt;span class=&#34;math inline&#34;&gt;\(\sigma_j\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;A&lt;/td&gt;
&lt;td&gt;28&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;B&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;C&lt;/td&gt;
&lt;td&gt;-3&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;D&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;E&lt;/td&gt;
&lt;td&gt;-1&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;F&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;G&lt;/td&gt;
&lt;td&gt;18&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;H&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;18&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;comparisons&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comparisons&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;8schools.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-the-posterior-summaries&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plot the posterior summaries&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;8schools2.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Quasi-Monte Carlo in ABC</title>
      <link>/course/qmc-abc/</link>
      <pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/course/qmc-abc/</guid>
      <description>&lt;div id=&#34;ingredients-for-abc&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ingredients for ABC&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;summary statistic &lt;span class=&#34;math inline&#34;&gt;\(S(y):\mathbb{R}^n\to \mathbb{R}^d\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;kernel function &lt;span class=&#34;math inline&#34;&gt;\(k(u_1,\dots,u_d)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;bandwidth &lt;span class=&#34;math inline&#34;&gt;\(h&amp;gt;0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;proposal density &lt;span class=&#34;math inline&#34;&gt;\(g(\theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ABC approximation&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\pi_{ABC}(\theta|s_{obs})\propto \pi(\theta)\int \pi(s|\theta)K((s-s_{obs})/h) d s\to \pi(\theta|s_{obs})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(\pi(\theta|s_{obs})\approx \pi(\theta|y)\)&lt;/span&gt;, then ABC density is a proper approximation of the posterior &lt;span class=&#34;math inline&#34;&gt;\(\pi(\theta|y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;abc-convergence-rates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ABC convergence rates&lt;/h2&gt;
&lt;p&gt;Consider the estimation of &lt;span class=&#34;math inline&#34;&gt;\(\mu=E[a(\theta)|s_{obs}]\)&lt;/span&gt;. The acceptance-rejection (AR) based ABC estimate is given by
&lt;span class=&#34;math display&#34;&gt;\[\hat\mu=\frac{1}{N}\sum_{i=1}^Na(\theta^{(i)}).\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Under regular conditions, ABC bias is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mathrm{bias}=|E_{ABC}[a(\theta)|s_{obs}]-\mu|=O(h^2),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and the acceptance probability is &lt;span class=&#34;math inline&#34;&gt;\(R = O(h^{d}),\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and Monte Carlo variance is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mathrm{variance}=\frac{\sigma^2_{ABC}}{N}=O\left(\frac{1}{C h^d}\right),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; is the complexity. Overall, the MSE is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mathrm{MSE} = \mathrm{bias}^2+\mathrm{variance} = O(h^4)+O\left(\frac{1}{Ch^d}\right).\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For a given &lt;span class=&#34;math inline&#34;&gt;\(C&amp;gt;0\)&lt;/span&gt;,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the optimal &lt;span class=&#34;math inline&#34;&gt;\(h^*=O(C^{-1/(4+d)})\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the optimal &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{MSE}^*=O(C^{-4/(d+4)})\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This reveals that ABC suffers from the &lt;strong&gt;curse of dimensionality&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;improving-the-sampling-efficiency&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Improving the sampling efficiency&lt;/h2&gt;
&lt;p&gt;A possible way to improve ABC efficiency is to accelerate the Monte Carlo. Monte Carlo error is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{MC error}=\frac{\sigma}{\sqrt{N}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Some variance reduction techniques are proposed to reduce &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;, such as&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;importance sampling&lt;/li&gt;
&lt;li&gt;antithetic variates&lt;/li&gt;
&lt;li&gt;control variates&lt;/li&gt;
&lt;li&gt;hybrid strategies&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;On the other hand, quasi-Monte Carlo is used to &lt;strong&gt;improve the rate of convergence&lt;/strong&gt; (&lt;span class=&#34;math inline&#34;&gt;\(1/\sqrt{N}\)&lt;/span&gt;) rather than the constant &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;quasi-monte-carlo-a-review&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Quasi-Monte Carlo: A review&lt;/h2&gt;
&lt;p&gt;As a start-up setting, let’s consider an intergal over the unit cube &lt;span class=&#34;math inline&#34;&gt;\([0,1]^d\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mu=\int_{[0,1]^d} f(u_1,\dots,u_d)du_1\cdots du_d.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;MC estimate is the average of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; iid samples:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat\mu_N = \frac 1 N\sum_{i=1}^N f(u^{(i)}),\ u^{(i)}\stackrel{iid}{\sim} U[0,1]^d.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;QMC estimate has the same form but uses deterministic sequences&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat\mu_N = \frac 1 N\sum_{i=1}^N f(u^{(i)}),\ u^{(i)}\in[0,1]^d.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The sequences are clearly constructed with better uniformness, which are known as &lt;strong&gt;low discrepancy sequences (LDSs)&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Halton (1960)&lt;/li&gt;
&lt;li&gt;Sobol’ (1967)&lt;/li&gt;
&lt;li&gt;Faure (1982)&lt;/li&gt;
&lt;li&gt;Niderreiter (1992)&lt;/li&gt;
&lt;li&gt;Lattice rules&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Koksma-Hlawka inequality&lt;/strong&gt; gives&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
|\hat\mu_N-\mu|\le V_{\mathrm{HK}}(f)D^*(u^{(1)},...,u^{(N)})
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(V_{\mathrm{HK}}(f)\)&lt;/span&gt; is the variation in the sense of Hardy and Krause&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(D^*\)&lt;/span&gt; is the star discrepancy of the points&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(V_{\mathrm{HK}}(f)&amp;lt;\infty\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\{u^{(1)},...,u^{(N)}\}\)&lt;/span&gt; is a LDS, then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{QMC error}=O(N^{-1}(\log N)^d)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;QMC can achieve higher-order rate of convergence, but needs higher smoothness conditions; see Dick (Ann. Stat., 2011).&lt;/p&gt;
&lt;p&gt;Some mathematical softwares such as &lt;code&gt;Matlab&lt;/code&gt;, &lt;code&gt;R&lt;/code&gt; include some common generators of LDSs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#  install.packages(&amp;quot;randtoolbox&amp;quot;)
set.seed(7)
library(&amp;quot;randtoolbox&amp;quot;)
par(mfrow=c(2,2))
qmc = sobol(1024,2)
plot(qmc[1:256,],pch=19,xlab=&amp;quot;x&amp;quot;,ylab=&amp;quot;y&amp;quot;,main=&amp;quot;Sobol&amp;#39; points: N = 256&amp;quot;)
plot(qmc,pch=19,xlab=&amp;quot;x&amp;quot;,ylab=&amp;quot;y&amp;quot;,main=&amp;quot;Sobol&amp;#39; points: N = 1024&amp;quot;)
mc = matrix(runif(2048),ncol = 2)
plot(mc[1:256,],pch=19,xlab=&amp;quot;x&amp;quot;,ylab=&amp;quot;y&amp;quot;,main=&amp;quot;MC points: N = 256&amp;quot;)
plot(mc,pch=19,xlab=&amp;quot;x&amp;quot;,ylab=&amp;quot;y&amp;quot;,main=&amp;quot;MC points: N = 1024&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/course/qmc-abc_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 1&lt;/h2&gt;
&lt;p&gt;Consider the integral:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mu = \int_{[0,1]^d} \sum_{i=1}^d x_i^2 dx =\frac{d}{3}.\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;myfun = function(x){
  #rowSums(x^2)
  apply(x-0.5,1,prod)
}

m = 16
N = 2^m
d = 8
#trueval = d/3
trueval = 0
qmc = as.matrix(sobol(n=N,dim=d),ncol=d)
fsum = cumsum(myfun(qmc))
ns = 1:N
fmean = fsum[ns]/ns
par(mar=c(4,4,2,1),mfrow=c(2,1))
tt = paste0(&amp;quot;QMC: d = &amp;quot;,d)
plot(ns,fmean,xlab=&amp;quot;N&amp;quot;,ylab=&amp;quot;Mean&amp;quot;,typ=&amp;quot;l&amp;quot;,main=tt)
abline(h=trueval,lty=5,col=&amp;quot;red&amp;quot;)
ns = 2^(0:m)
fmean = fsum[ns]/ns
err = abs(fmean-trueval)
plot(ns,err,xlab=&amp;quot;N&amp;quot;,ylab=&amp;quot;Error&amp;quot;,typ=&amp;quot;b&amp;quot;,log=&amp;quot;xy&amp;quot;,main=tt)
r = 1
lines(ns[c(3,m)], c(err[3],err[3]*(ns[3]/ns[m])^r),col=&amp;quot;red&amp;quot;,lty=5)
legend(500,err[2],legend = c(&amp;quot;QMC errors&amp;quot;,paste0(&amp;quot;N^{&amp;quot;,-r,&amp;quot;}&amp;quot;)),lty = c(1,5),
       col=c(&amp;quot;black&amp;quot;,&amp;quot;red&amp;quot;),pch=c(1,NA),cex=1.2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/course/qmc-abc_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;randomized-qmc&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Randomized QMC&lt;/h2&gt;
&lt;p&gt;In practice, we use randomized QMC (RQMC), which yields an unbiased estimate.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;random-shift, see Cranley and Patterson (1976)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;scrambled nets, see Owen (1995,1997,1998)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Survey in L’Ecuyer and Lemieux (2005)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(7)
library(&amp;quot;randtoolbox&amp;quot;)
par(mfrow=c(2,2))
qmc = sobol(1024,2)
rqmc = sobol(1024,2,scrambling=1)
plot(qmc[1:256,],pch=19,xlab=&amp;quot;x&amp;quot;,ylab=&amp;quot;y&amp;quot;,main=&amp;quot;Sobol&amp;#39; points: N = 256&amp;quot;)
plot(qmc,pch=19,xlab=&amp;quot;x&amp;quot;,ylab=&amp;quot;y&amp;quot;,main=&amp;quot;Sobol&amp;#39; points: N = 1024&amp;quot;)
mc = matrix(runif(2048),ncol = 2)
plot(rqmc[1:256,],pch=19,xlab=&amp;quot;x&amp;quot;,ylab=&amp;quot;y&amp;quot;,main=&amp;quot;RQMC points: N = 256&amp;quot;)
plot(rqmc,pch=19,xlab=&amp;quot;x&amp;quot;,ylab=&amp;quot;y&amp;quot;,main=&amp;quot;RQMC points: N = 1024&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/course/qmc-abc_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;myfun = function(x){
  d = ncol(x)
  #(rowSums(x)&amp;gt;d/2)-0.5
  rowSums(qnorm(x))
  #apply(x-0.5,1,prod)
  
}

m = 16
N = 2^m
d = 8
trueval = 0
R = 100
ns = 2^(0:m)
fmean = matrix(0,m+1,R)
tmp = sobol(N,d)##initialization
for(i in 1:R)
{
  rqmc = as.matrix(sobol(N,d,scrambling=1,init=FALSE),ncol=d)
  fsum = cumsum(myfun(rqmc))
  fmean[,i] = fsum[ns]/ns
}

par(mar=c(4,4,2,1),mfrow=c(2,1))
tt = paste0(&amp;quot;RQMC: d = &amp;quot;,d)
plot(ns,rowMeans(fmean),xlab=&amp;quot;N&amp;quot;,ylab=&amp;quot;Mean&amp;quot;,typ=&amp;quot;b&amp;quot;,main=tt)
abline(h=trueval,lty=5,col=&amp;quot;red&amp;quot;)

rmse = apply(fmean,1,sd)

plot(ns,rmse,xlab=&amp;quot;N&amp;quot;,ylab=&amp;quot;rmse&amp;quot;,typ=&amp;quot;b&amp;quot;,log=&amp;quot;xy&amp;quot;,main=tt)
r = 1
lines(ns[c(3,m)], c(rmse[3],rmse[3]*(ns[3]/ns[m])^r),col=&amp;quot;red&amp;quot;,lty=5)
legend(500,rmse[2],legend = c(&amp;quot;RQMC errors&amp;quot;,paste0(&amp;quot;N^{&amp;quot;,-r,&amp;quot;}&amp;quot;)),lty = c(1,5),
       col=c(&amp;quot;black&amp;quot;,&amp;quot;red&amp;quot;),pch=c(1,NA),cex=1.2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/course/qmc-abc_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-qmc-in-abc&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using QMC in ABC&lt;/h2&gt;
&lt;p&gt;The univariate g-and-k distribution is a flexible unimodal distribution that
is able to describe data with significant amounts of skewness and kurtosis. Its density function has no closed form, but
is alternatively defined through its quantile function as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Q(q|A,B,g,k)=A+B\left[1+c\frac{1-\exp\{-gz(q)\}}{1+\exp\{-gz(q)\}}\right](1+z(q)^2)^kz(q)\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(c=0.8,\ B&amp;gt;0, k&amp;gt;-1/2\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(z(q)=\Phi^{-1}(q)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;if &lt;span class=&#34;math inline&#34;&gt;\(g=k=0\)&lt;/span&gt;, it is the normal density&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(y_{obs}\)&lt;/span&gt; of length &lt;span class=&#34;math inline&#34;&gt;\(1000\)&lt;/span&gt; is generated from the &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt;-and-&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; distribution with parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta_0=(3,1,2,0.5)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;prior density&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\pi(\theta) = \pi(A)\pi(B)\pi(g)\pi(k) = N(1,5)\times N(0.25,2) \times U(0,10) \times U(0,1)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Summary statistic (Drovandi and Pettitt, 2011): &lt;span class=&#34;math inline&#34;&gt;\(S(y) = (S_A,S_B,S_g,S_k)\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(S_A=E_4\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(S_B=E_6-E_2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(S_g=(E_6+E_2-2E_4)/S_B\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(S_k = (E_7-E_5+E_3-E_1)/S_B\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(E_1\le E_2 \le \cdots \le E_8\)&lt;/span&gt; are the octiles of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(100)
theta0 = c(3,1,2,0.5)

gkmodel &amp;lt;- function(theta,n,z=NA){
  if(is.na(z)[1]){
    z = rnorm(n)
  }
  y = theta[1] + theta[2]*(1+0.8*(1-exp(-theta[3]*z))/
        (1+exp(-theta[3]*z)))*(1+z^2)^theta[4]*z
  return(matrix(y,n,1))
}
n = 1e3
yobs = gkmodel(theta0,n)
## prior density
gkprior &amp;lt;- function(n,u=NA){
  if(is.na(u)[1]){
    u = matrix(runif(n*4),n,4)
  }
  A = qnorm(u[,1])*sqrt(5)+1
  B = qnorm(u[,2])*sqrt(2)+.25
  g = u[,3]*10
  k = u[,4]
  return(cbind(A,B,g,k))
}
gksummary &amp;lt;- function(y){
  sorty = sort(y)
  n = length(y)
  q = sorty[ceiling(n/8*(1:8))]
  s = c(q[4],
        q[6]-q[2],
        (q[6]+q[2]-2*q[4])/(q[6]-q[2]),
        (q[7]-q[5]+q[3]-q[1])/(q[6]-q[2]))
  return(s)
}
ytmp1 = matrix(0,2000,4)
for(i in 1:2000){
  ytmp1[i,] = gksummary(gkmodel(gkprior(1),n))
}
Sigma = var(ytmp1)
invsig = solve(Sigma)
N = 1e5
ytmp = rep(0,N)
theta = matrix(0,N,4)
stmp = rep(0,N)
sobs = gksummary(yobs)
#qmc = sobol(N+1,n+4)
#qmc = qmc[-1,]##initialization
for(i in 1:N){
  theta[i,] = gkprior(1) # matrix(qmc[i,1:4],1,4)
  ypro = gkmodel(theta[i,],n) # qnorm(matrix(qmc[i,-(1:4)],1,n))
  spro = gksummary(ypro)
  diff = matrix(spro-sobs,1,4)
  ytmp[i] = sqrt(sum((ypro-yobs)^2))
  stmp[i] = sqrt(diff%*%invsig%*%t(diff))
}
ysort = sort(ytmp)
ssort = sort(stmp)
effN = N*5e-3
hy = ysort[effN]
hs = ssort[effN]
## draw pairwise scatterplots
theta1 = theta[ytmp&amp;lt;=hy,]
theta2 = theta[stmp&amp;lt;=hs,]
theta = rbind(theta1,theta2,theta0)
colnames(theta) = c(&amp;quot;A&amp;quot;,&amp;quot;B&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;k&amp;quot;)
pairs(theta,pch=c(rep(20,effN*2),24),
      col=c(rep(&amp;quot;grey&amp;quot;,effN),rep(&amp;quot;black&amp;quot;,effN),&amp;quot;red&amp;quot;),cex=1)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>R密度估计</title>
      <link>/course/ex2/</link>
      <pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/course/ex2/</guid>
      <description>&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;案例：身高数据&lt;/h2&gt;
&lt;p&gt;数据来源于R的包&lt;code&gt;dslabs&lt;/code&gt;，第一次使用时需要安装该包，命令为&lt;code&gt;install.packages(&amp;quot;dslabs&amp;quot;)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;参考资料： &lt;a href=&#34;https://simplystatistics.org/2018/01/22/the-dslabs-package-provides-datasets-for-teaching-data-science/&#34;&gt;Some datasets for teaching data science&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;直方图&lt;/h3&gt;
&lt;p&gt;直方图的R命令为：&lt;code&gt;hist(...)&lt;/code&gt;, 查看帮助&lt;code&gt;?hist&lt;/code&gt;看具体参数含义&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;核估计&lt;/h3&gt;
&lt;p&gt;核估计的R命令为：&lt;code&gt;density(...)&lt;/code&gt;, 查看帮助&lt;code&gt;?density&lt;/code&gt;看具体参数含义。注意该命令只是给出估计值的数据，不能直接画图，如果要画图则需要调用画图函数，如&lt;code&gt;plot(...)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;以下代码展示所有身高数据的直方图与和核估计。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;if(!require(dslabs))
  install.packages(&amp;quot;dslabs&amp;quot;)
attach(heights) #此命令用于使用该包里面的身高数据heights
par(mar=c(2,2,1,1)) #调整图形边距
#直方图
hist(height,breaks=10,ylim=c(0,.115),col = &amp;quot;lightblue&amp;quot;, border = &amp;quot;pink&amp;quot;,freq=FALSE,main=&amp;quot;Histogram vs. Kernel density&amp;quot;)
#添加核估计数据
lines(density(height,from = 50,to=85),col=&amp;quot;red&amp;quot;,lwd=2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/course/ex2_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;下面代码比较男生和女生数据的核估计&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;female_height = height[sex==&amp;quot;Female&amp;quot;]#提取女生数据
male_height = height[sex==&amp;quot;Male&amp;quot;]#提取男生数据
par(mar=c(2,2,1,1))
#画男生数据
plot(density(male_height,from = 50,to=85),col=&amp;quot;red&amp;quot;,lwd=2,ylim=c(0,.14),main=&amp;quot;Male vs. Female&amp;quot;)
#添加女生数据
lines(density(female_height,from = 50,to=85),col=&amp;quot;blue&amp;quot;,lwd=2)
#画出图例说明
legend(74,0.12,legend = c(&amp;quot;Male&amp;quot;,&amp;quot;Female&amp;quot;),lty = c(1,1),col=c(&amp;quot;red&amp;quot;,&amp;quot;blue&amp;quot;),lwd=c(2,2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/course/ex2_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;结论：身高数据可以近似看成正态分布，而且男生、女生两个总体的均值有差异，男生身高平均水平大于女生身高的平均水平。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>R画图技巧</title>
      <link>/course/ex1/</link>
      <pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/course/ex1/</guid>
      <description>&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;预备工作&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;R软件&lt;/code&gt;下载: &lt;a href=&#34;https://www.r-project.org/&#34; class=&#34;uri&#34;&gt;https://www.r-project.org/&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;RStudio编辑器&lt;/code&gt;下载: &lt;a href=&#34;https://www.rstudio.com/&#34; class=&#34;uri&#34;&gt;https://www.rstudio.com/&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;例1：密度函数画图&lt;/h2&gt;
&lt;p&gt;画图的主要命令&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;为&lt;code&gt;plot(x,y,...)&lt;/code&gt;，里面各种参数的含义可查看帮助文档&lt;code&gt;help(plot)&lt;/code&gt;或者&lt;code&gt;? plot&lt;/code&gt;. 下面以画&lt;strong&gt;伽马分布&lt;/strong&gt;的密度为例。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;所有的图像都是离散点拼接起来，首先要确定横坐标，可用&lt;code&gt;seq(from = a, to = b, length=n)&lt;/code&gt;生成&lt;span class=&#34;math inline&#34;&gt;\([a,b]\)&lt;/span&gt;间的&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;个等分点。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;接着根据横坐标的值计算相应的密度函数值，常用分布的密度函数在R种有现成的函数（使用命令&lt;code&gt;? distribution&lt;/code&gt;查看常用的分布），直接调用即可。比如&lt;strong&gt;伽马分布&lt;/strong&gt;的密度为&lt;code&gt;dgamma(x,shape=alpha,rate = lambda)&lt;/code&gt;, 详情查看帮助文档&lt;code&gt;? dgamma&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;下面为三组参数下的画图代码（可复制到一个空白的R文件中保存运行）&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x = seq(0.001,10,length = 10000) #生成横坐标值
lambda = 0.5
alpha = 1
y = dgamma(x,shape=alpha,rate = lambda) #计算相应的密度值
par(mai=c(0.9,0.9,0.3,0.1),cex=1.1) #调整图像边缘空白处大小，初学者可不用设置
plot(x,y,type=&amp;quot;l&amp;quot;,ylab = &amp;quot;f(x)&amp;quot;,col=&amp;quot;blue&amp;quot;,cex.lab=1.2)

#画第二组参数的图像
alpha = 2
y = dgamma(x,shape=alpha,rate = lambda)
lines(x,y,col=&amp;quot;red&amp;quot;) #此次通过lines命令画第二组参数的图，若用plot命令则输出一副新的图像，而不是在上一幅图基础上叠加

#画第三组参数的图像
alpha = 3
y = dgamma(x,shape=alpha,rate = lambda)
lines(x,y,col=&amp;quot;green&amp;quot;)

#画出相应的标注，即图中的小矩形
expr1 = expression(alpha==1) #此命令用于希腊字母的转化
expr2 = expression(alpha==2)
expr3 = expression(alpha==3)
legend(6,0.5,legend=c(expr1,expr2,expr3),col=c(&amp;quot;blue&amp;quot;,&amp;quot;red&amp;quot;,&amp;quot;green&amp;quot;),lty = c(1,1,1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/course/ex1_files/figure-html/gammaplot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;例2：经验分布函数画图&lt;/h2&gt;
&lt;p&gt;画经验分布函数主要用到命令&lt;code&gt;ecdf(x)&lt;/code&gt;. 下面的例子为标准正态样本的经验分布图。R中提供了生成常见分布的样本的命令（使用命令&lt;code&gt;? distribution&lt;/code&gt;查看常用的分布）。如生成正态分布&lt;span class=&#34;math inline&#34;&gt;\(N(a,b^2)\)&lt;/span&gt;的&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;个样本代码为&lt;code&gt;rnorm(n, mean = a, sd = b)&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x = rnorm(100) #生成100个标准正态的样本
Fn1 = ecdf(x[1:10]) #计算前10个样本对应的经验分布函数
Fn2 = ecdf(x[1:100]) #计算前100个样本对应的经验分布函数

#计算标准正态分布函数
t = seq(-3,3,by=0.01) #横坐标
y = pnorm(t) #相应的分布函数值

#mfrow表示生成两行一列的图，后面的两个参数用于调整页边距，初学者可不用设置
#最终输出一幅图，包含两幅子图
par(mfrow=c(2,1),mgp=c(1.5,0.8,0),mar=.1+c(3,3,2,1)) 

# 第一幅子图
plot(Fn1,verticals=TRUE,do.points=FALSE,main=&amp;quot;n=10&amp;quot;,xlim=c(-3,3)) #画经验分布函数
lines(t,y,col=&amp;quot;red&amp;quot;) #画真实的正态分布函数图像

# 第二幅子图
plot(Fn2,verticals=TRUE,do.points=FALSE,main=&amp;quot;n=100&amp;quot;,xlim=c(-3,3)) #画经验分布函数
lines(t,y,col=&amp;quot;red&amp;quot;) #画真实的正态分布函数图像&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/course/ex1_files/figure-html/ecdf-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;课后练习&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;安装相应的软件&lt;/li&gt;
&lt;li&gt;参考上面两个例子，学会使用&lt;code&gt;plot(...)&lt;/code&gt;画相关的图形，了解该命令里面参数的作用。&lt;/li&gt;
&lt;li&gt;参考例1，画不同参数下&lt;strong&gt;贝塔分布&lt;/strong&gt;的密度函数；关键的命令查看帮助&lt;code&gt;? beta&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;参考例2，比较其他分布（查看R中的常用分布&lt;code&gt;?distribution&lt;/code&gt;）的经验分布函数与真实的分布函数，并观察他们的差距是否随着样本量的增加而减小。&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;一些建议&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;充分利用帮助文档&lt;/strong&gt;。我们不可能记住所有命令的使用方式，使用帮助文档是一种高效的学习途径，此外帮助文档末尾还提供一些参考例子，有助于理解命令的使用方式。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;充分利用网上资源&lt;/strong&gt;。编程过程中如果遇到问题，可以通过度娘等方式搜索寻找答案，现在的很多技术博客提供很多有价值的资源。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;要学会偷懒&lt;/strong&gt;。在编写一种算法之前，首先要去了解R软件中有没有现成的命令。如果有现成的，则只需学会如何运用即可。通过不断地积累，工作效率会大大提高。&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;更高级的画图方式见&lt;code&gt;ggplot2&lt;/code&gt;, 初学者可先忽略&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>期末试卷</title>
      <link>/course/testa/</link>
      <pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/course/testa/</guid>
      <description>&lt;p&gt;Part I: Each problem is worth 3 points.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(X_1,X_2,\dots,X_6\)&lt;/span&gt; be a simple random sample taken from &lt;span class=&#34;math inline&#34;&gt;\(N(0,2^2)\)&lt;/span&gt;. Denote
&lt;span class=&#34;math display&#34;&gt;\[Y = (X_1+X_2)^2+(X_3+X_4)^2+(X_5+X_6)^2.\]&lt;/span&gt;
If &lt;span class=&#34;math inline&#34;&gt;\(kY\sim \chi^2(3)\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(k=\)&lt;/span&gt; &lt;!-- $1/8$ --&gt;?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(X_1,X_2,X_3\)&lt;/span&gt; be a simple random sample taken from &lt;span class=&#34;math inline&#34;&gt;\(N(\mu,\sigma^2)\)&lt;/span&gt;. If &lt;span class=&#34;math inline&#34;&gt;\(\hat\mu = \frac{1}{2} X_1+cX_2+\frac{1}{6}X_3\)&lt;/span&gt; is an unibased estimate of &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(c=\)&lt;/span&gt; &lt;!-- $1/3$--&gt;?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(X_1,X_2,X_3\)&lt;/span&gt; be a simple random sample taken from &lt;span class=&#34;math inline&#34;&gt;\(B(1,p)\)&lt;/span&gt;. For testing the hypothesis &lt;span class=&#34;math inline&#34;&gt;\(H_0:p=1/2\ vs.\ H_1:p=3/4\)&lt;/span&gt;, we use a rejection region:
&lt;span class=&#34;math display&#34;&gt;\[W=\{(x_1,x_2,x_3):x_1+x_2+x_3\ge 2\}.\]&lt;/span&gt;
The power of the test is &lt;!-- 27/32 --&gt;?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt; be a simple random sample taken from &lt;span class=&#34;math inline&#34;&gt;\(N(\mu,1)\)&lt;/span&gt;, and let &lt;span class=&#34;math inline&#34;&gt;\(S_n^2=\frac 1n\sum_{i=1}^n(X_i-\bar X)^2\)&lt;/span&gt; be the sample variance. Then &lt;span class=&#34;math inline&#34;&gt;\(Var[S_n^2]=\)&lt;/span&gt; &lt;!-- $2(n-1)/n^2$ --&gt;?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If the usual &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; confidence interval for the mean of normal population was &lt;span class=&#34;math inline&#34;&gt;\([0.12,0.22]\)&lt;/span&gt;, the method of moments estimate of the mean would be &lt;!-- 0.17 --&gt;?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;p&gt;Part II: Multiple Choice Problems (one or more than one items may be true). Each problem is worth 3 points.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta,\lambda,\alpha,\beta\)&lt;/span&gt; are unknown in the following densities. Which of the following probability distributions belong to the exponential family? ( &lt;!-- BC --&gt; )&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A. &lt;span class=&#34;math inline&#34;&gt;\(f(x;\theta,\lambda) = \frac \theta\lambda\left(\frac{x}{\lambda}\right)^{\theta-1}e^{-(x/\lambda)^\theta}1\{x&amp;gt; 0\}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;B. &lt;span class=&#34;math inline&#34;&gt;\(f(x;\alpha,\beta) = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1}1\{0&amp;lt;x&amp;lt;1\}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\Gamma(\cdot)\)&lt;/span&gt; is the gamma function.&lt;/p&gt;
&lt;p&gt;C. &lt;span class=&#34;math inline&#34;&gt;\(f(x;\lambda) = \frac{\lambda^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\lambda x}1\{x&amp;gt; 0\}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;D. &lt;span class=&#34;math inline&#34;&gt;\(f(x;\theta) = \frac{2}{\sqrt{2\pi}}e^{-\frac{(x-\theta)^2}{2}}1\{x\ge \theta\}\)&lt;/span&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt; be the simple random sample taken from the normal distribution &lt;span class=&#34;math inline&#34;&gt;\(N(\mu,\sigma^2)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\mu,\sigma^2\)&lt;/span&gt; are unknown parameters. Which of the following are sufficient statistics for &lt;span class=&#34;math inline&#34;&gt;\(\theta=(\mu,\sigma^2)\)&lt;/span&gt;? ( &lt;!-- AB --&gt; )&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A. &lt;span class=&#34;math inline&#34;&gt;\(T_1 = (X_1,\dots,X_n)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;B. &lt;span class=&#34;math inline&#34;&gt;\(T_2 = (\sum_{i=1}^n X_i,\sum_{i=1}^n X_i^2)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;C. &lt;span class=&#34;math inline&#34;&gt;\(T_3 = (\sum_{i=1}^n |X_i|,\sum_{i=1}^n X_i^2)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;D. &lt;span class=&#34;math inline&#34;&gt;\(T_4 = \frac{1}{n}\sum_{i=1}^n X_i\)&lt;/span&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Which of the following statements are true? ( &lt;!-- BC --&gt; )&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A. If the &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-value is 0.05, the corresponding test will be rejected at the significance level 0.03.&lt;/p&gt;
&lt;p&gt;B. If a test rejects at significance level 0.05, then the &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-value is less than or equal to 0.05.&lt;/p&gt;
&lt;p&gt;C. If the significance level of a test is decreased, the power of the test would be expected to decrease.&lt;/p&gt;
&lt;p&gt;D. A type II error occurs when the test statistic falls in the rejection region of the test and the null is true.&lt;/p&gt;
&lt;hr /&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\hat\beta_0,\hat\beta_1\)&lt;/span&gt; be the least squares etstimators for the simple linear model &lt;span class=&#34;math inline&#34;&gt;\(y_i = \beta_0+\beta_1x_i+\epsilon_i,\ i=1,\dots,n\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_i\stackrel{iid}{\sim} N(0,\sigma^2)\)&lt;/span&gt;. Which of the following statements are true? ( &lt;!-- BCD --&gt; )&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A. &lt;span class=&#34;math inline&#34;&gt;\(\hat\beta_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat\beta_1\)&lt;/span&gt; are independent.&lt;/p&gt;
&lt;p&gt;B. &lt;span class=&#34;math inline&#34;&gt;\(\hat\beta_0-\hat\beta_1\)&lt;/span&gt; is normally distributed.&lt;/p&gt;
&lt;p&gt;C. The more spread out the &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; are the better we can estimate the slope &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;D. &lt;span class=&#34;math inline&#34;&gt;\(\bar y = \hat\beta_0+\hat\beta_1 \bar x\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\bar x = \frac 1 n\sum_{i=1}^n x_i,\ \bar y = \frac 1 n\sum_{i=1}^n y_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt; be a simple random sample taken from &lt;span class=&#34;math inline&#34;&gt;\(N(2,3^2)\)&lt;/span&gt;, and let &lt;span class=&#34;math inline&#34;&gt;\(\bar X\)&lt;/span&gt; be the sample mean. Which of the following are true? ( &lt;!-- D --&gt; )&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A. &lt;span class=&#34;math inline&#34;&gt;\(\frac{\bar X -2}{3/\sqrt{n}}\sim t(n)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;B. &lt;span class=&#34;math inline&#34;&gt;\(\frac 1 9\sum_{i=1}^n (X_i-2)^2\sim F(n,1)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;C. &lt;span class=&#34;math inline&#34;&gt;\(\frac{\bar X-2}{\sqrt{3}/\sqrt{n}}\sim N(0,1)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;D. &lt;span class=&#34;math inline&#34;&gt;\(\frac 1 9\sum_{i=1}^n(X_i-2)^2\sim \chi^2(n)\)&lt;/span&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Part III. (12 points)&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt; be a simple random sample taken from the density&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f(x;\theta)=\frac{2x}{\theta^2},\quad 0\le x\le \theta.\]&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Find an expression for &lt;span class=&#34;math inline&#34;&gt;\(\hat\theta_L\)&lt;/span&gt;, the maximum likelihood estimator (MLE) for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Find an expression for &lt;span class=&#34;math inline&#34;&gt;\(\hat\theta_M\)&lt;/span&gt;, the method of moments estimator for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For the two estimators &lt;span class=&#34;math inline&#34;&gt;\(\hat\theta_L\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat\theta_M\)&lt;/span&gt;, which one is more efficient in terms of mean squared error (MSE)?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- 
`Solution`:

1. The likelihood function is

$$L(\theta) = \prod_{i=1}^n f(x_i;\theta) = \frac{2^n}{\theta^n}\left(\prod_{i=1}^n x_i\right) 1\{x_{(n)}\le \theta\}.$$

To maximize $L(\theta)$, we need to choose $\theta\ge x_{(n)}$ so that
$L(\theta) = A\theta^{-n}$, where $A=2^n\prod_{i=1}^n x_i$ does not depend on $\theta$. So the MLE is $\hat\theta_L = X_{(n)}$.

2. First, compute the first order moment:

$$E[X] = \int_0^\theta xf(x;\theta)dx = \int_0^\theta \frac{2x^2}{\theta^2}dx=\frac{2\theta}{3}.$$

This implies that $\theta = 3E[X]/2$. The method of moments estimator $\hat\theta_M=3\bar X/2$.

3. The density for $X_{(n)}$ is given by

$$f_{X_{(n)}}(x;\theta) = nF^{n-1}(x)f(x;\theta)=n\frac{x^{2(n-1)}}{\theta^{2(n-1)}}\frac{2x}{\theta^2}=\frac{2nx^{2n-1}}{\theta^{2n}},\quad 0\le x\le \theta.$$

The first and second order moments for $X_{(n)}$ are

$$E[X_{(n)}] = \int_0^\theta \frac{2nx^{2n}}{\theta^{2n}}dx = \frac{2n\theta}{2n+1},$$

$$E[X_{(n)}^2] = \int_0^\theta \frac{2nx^{2n+1}}{\theta^{2n}}dx = \frac{n\theta^2}{n+1}.$$

The MSE for $\hat\theta_L$ is given by

$$
\begin{align}
MSE(\hat\theta_L)&amp;=E[(\hat\theta_L-\theta)^2]=E[X_{(n)}^2]-2\theta E[X_{(n)}]+\theta^2\\
&amp;=\frac{n\theta^2}{n+1}-\frac{4n\theta^2}{2n+1}+\theta^2\\
&amp;=\frac{\theta^2}{(n+1)(2n+1)}.
\end{align}
$$
The second  order moment for $X$ is

$$E[X^2] = \int_{0}^\theta \frac{2x^3}{\theta^2}dx=\frac{\theta^2}{2}.$$

The MSE for $\hat\theta_M$ is given by

$$
\begin{align}
MSE(\hat\theta_M)&amp;=Var[\hat\theta_M]=\frac{9Var[X]}{4n}\\
&amp;=\frac{9}{4n}(E[X^2]-E[X]^2)\\
&amp;=\frac{9}{4n}\left(\frac{\theta^2}{2}-\frac{4\theta^2}{9}\right)= \frac{\theta^2}{8n}.
\end{align}
$$

It is easy to see that when $n\ge 3$, $MSE(\hat\theta_L)&lt;MSE(\hat\theta_M)$; otherwise, $MSE(\hat\theta_L)&gt;MSE(\hat\theta_M)$.
--&gt;
&lt;hr /&gt;
&lt;p&gt;Part IV. (10 points)&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt; be a simple random sample taken from an exponential distribution &lt;span class=&#34;math inline&#34;&gt;\(Exp(\lambda)\)&lt;/span&gt;, whose density is given by
&lt;span class=&#34;math display&#34;&gt;\[f(x;\lambda) = \lambda e^{-\lambda x}1\{x\ge 0\},\ \lambda&amp;gt;0.\]&lt;/span&gt;
Derive a likelihood ratio test of the hypothesis
&lt;span class=&#34;math display&#34;&gt;\[H_0:\lambda=1\ vs.\ H_1:\lambda=2.\]&lt;/span&gt;
What is the definition of uniformly most powerful (UMP)? Is the test UMP against the alternative &lt;span class=&#34;math inline&#34;&gt;\(H_1:\lambda&amp;gt;1\)&lt;/span&gt;?&lt;/p&gt;
&lt;!-- 
`Solution`: The likelihood function is 
$$L(\lambda)=\prod_{i=1}^n (\lambda e^{-\lambda x_i}) = \lambda^ne^{-\lambda n\bar x}.$$


The likelihood ratio is given by
$$\lambda(\vec x)= \frac{L(2)}{L(1)}=\frac{2^ne^{-2 n\bar x}}{e^{- n\bar x}}=2^ne^{-n\bar x}.$$

Choose the test statistic $T(\vec x) = 2n\bar x$. When $\lambda=1$, $T(\vec X)\sim \chi^2(2n)$. Also, 
$\lambda(\vec x) = 2^ne^{-T(\vec x)/2}.$ The rejection region is of the form $W=\{T(\vec x)&lt;C\}$. We thus have $C=\chi_{\alpha}^2(2n)$.

A rejection region $W$ is said to be UMP if for any rejection region $W&#39;$ with the type I error probability is no more than $\alpha$, the power of the test associated with $W&#39;$ is no larger than that of the rejection region $W$.

Consider the test of the hypothesis 
$$H_0:\lambda=1\ vs.\ H_1:\lambda=\lambda_0&gt;1.$$
Following the same procedure above, the likelihood ratio test gives the same rejection region W. So the test derived before is also UMP for the alternative $H_1:\lambda&gt;1$ by using the N-P lemma. 
--&gt;
&lt;hr /&gt;
&lt;p&gt;Part V.&lt;/p&gt;
&lt;p&gt;A medical researcher believes that women typically
have lower serum cholesterol (血清胆固醇) than men. To test this
hypothesis, he took a sample of 476 men between the ages
of nineteen and forty-four and found their mean serum
cholesterol to be 189.0 mg/dl with a sample standard deviation
of 34.2. A group of 592 women in the same age range
averaged 177.2 mg/dl and had a sample standard deviation
of 33.3. Is the lower average for the women statistically
significant? Set the significant level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; =0.05. What assumptions are made when conducting the test? (&lt;span class=&#34;math inline&#34;&gt;\(u_{0.95}=1.644854\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(t_{0.95}(1066)=1.646284\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(t_{0.95}(1068)=1.646282\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(u_{0.975}=1.959964\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(t_{0.975}(1066)=1.962192\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(t_{0.975}(1068)=1.962188\)&lt;/span&gt;)&lt;/p&gt;
&lt;!-- 
`Solution`: Let $X_i$ be the serum cholesterol for men, $i=1,\dots,n=476$, let $Y_j$ be be the serum cholesterol for women, $j=1,\dots,m=592$. We now have $\bar x = 189.0$, $s_{1n}=34.2$, $\bar y = 177.2$, $s_{2m}=33.3$. Suppose that $X_i\stackrel{iid}{\sim} N(\mu_1,\sigma^2)$ and $Y_i\stackrel{iid}{\sim} N(\mu_2,\sigma^2)$. We are testing

$$H_0:\mu_1\le \mu_2,\ vs.\ H_1:\mu_1&gt;\mu_2.$$

We use the t-test. The test statistic is 

$$T = \frac{\bar X-\bar Y}{S_w\sqrt{\frac 1 n+\frac 1 m}},$$
where $S_w^2 = (nS_{1n}^2+mS_{2m}^2)/(n+m-2)$. The rejection region is given by $W = \{T&gt;t_{1-\alpha}(n+m-2)\}$.
The observed test statistic is $$t=\frac{189.0-177.2}{33.74\sqrt{\frac 1 {476}+\frac 1 {592}}}=5.68&gt;t_{0.95}(1066)=1.65.$$

We therefore reject the null. The lower average for the women is statistically significant.

The assumptions are

1. normally distributed for both groups
2. the two grouds are independent
3. their variances are the same
--&gt;
&lt;hr /&gt;
&lt;p&gt;Part VI:&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt; be a simple random sample taken from the uniform distribution &lt;span class=&#34;math inline&#34;&gt;\(U(\theta,0)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\theta&amp;lt;0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;(a). Derive a &lt;span class=&#34;math inline&#34;&gt;\(100(1-\alpha)\%\)&lt;/span&gt; confidence interval for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;(b). There is a duality between confidence intervals and
hypothesis tests. Use the result in part (a) to derive a test at significant level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; of the hypothesis
&lt;span class=&#34;math display&#34;&gt;\[H_0: \theta = \theta_0\ vs.\ H_1:\theta \neq \theta_0,\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\theta_0&amp;lt;0\)&lt;/span&gt; is fixed.&lt;/p&gt;
&lt;!-- 
`Solution`: Let $G = X_{(1)}/\theta$. The CDF for $G$
is given by

$$F_G(x) = P(G\le x) = P(X_{(1)}/\theta\le x) = P(X_{(1)}\ge \theta x) = \prod_{i=1}^nP(X_i\ge \theta x) = x^n,\ 0&lt; x&lt; 1.$$

Let$a,b\in \mathbb{R}$ such that $P(a\le G\le b)=1-\alpha$. Then the CI for $\theta$ is

$$CI=\left[\frac{X_{(1)}}{a},\frac{X_{(1)}}{b}\right].$$

For simplicity, we take $a,b$ such that $P(G\le a) = P(G\ge b) = \alpha/2$. This implies $a = (\alpha/2)^{1/n},\ b= (1-\alpha/2)^{1/n}$. 

Or you can take $P(G\le a) = \alpha,P(G\le b)=1$ so that $a=\alpha^{1/n}, b=1$.

You can also use other statistics, such as $G=X_{n}/\theta$ or $G=-2\log(\sum_{i=1}^n X_i/\theta)$. The answer is not unique.

Form part (a), we have

$$P_{\theta}\left(\theta\in CI\right) = 1-\alpha\ \forall\theta&lt;0.$$

We therefore choose the rejection region

$$W = \{\theta_0\notin CI\}.$$

It is easy to see that $P_{\theta_0}(\theta_0\notin CI) = \alpha$. 

--&gt;
&lt;hr /&gt;
&lt;p&gt;Part VII:&lt;/p&gt;
&lt;p&gt;Consider the linear model
&lt;span class=&#34;math display&#34;&gt;\[y_i=\beta_0+\beta_1x_i+\epsilon_i,\ \epsilon_i\stackrel{iid}{\sim} N(0,\sigma^2),\ i=1,\dots,n.\]&lt;/span&gt;
Suppose that all the fixed &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; are not equal and &lt;span class=&#34;math inline&#34;&gt;\(n\ge 3\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;(a). Derive a maximum likelihood estimator (MLE) &lt;span class=&#34;math inline&#34;&gt;\(\hat\sigma_L^2\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;(b). Let &lt;span class=&#34;math inline&#34;&gt;\(T_k=k\hat\sigma_L^2\)&lt;/span&gt; be an estimate of &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;. Find a &lt;span class=&#34;math inline&#34;&gt;\(k\in \mathbb{R}\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(T_k\)&lt;/span&gt; is an unbiased estimate of &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;.
Show that the unbiased estimate is not the optimal choice by taking account of mean squared error (MSE), and
the most efficient &lt;span class=&#34;math inline&#34;&gt;\(T_k\)&lt;/span&gt; takes place at &lt;span class=&#34;math inline&#34;&gt;\(k=1\)&lt;/span&gt;, i.e., the MLE &lt;span class=&#34;math inline&#34;&gt;\(\hat\sigma_L^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;!-- 
`Solution`: It is easy to see that $$\hat\sigma_L^2=\frac{S_e^2}{n},$$
where $S_e^2 = \frac 1n\sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)^2$, and $\hat\beta_0,\hat\beta_1$ are the LSE for $\beta_0,\beta_1$.
It is known that $S_e^2/\sigma^2\sim \chi^2(n-2)$. This gives
$E[S_e^2]=(n-2)\sigma^2$ and $Var[S_e^2] = 2(n-2)\sigma^4$.

As a result, $E[T_k] = kE[S_e^2/n]=\frac{k(n-2)}{n}\sigma^2$. If $T_k$ is unbiased, then $k = n/(n-2)$.
On the other hand, $$Var[T_k] = \frac{k^2}{n^2}Var[S_e^2] = \frac{2(n-2)k^2}{n^2}\sigma^4.$$

The MSE of $T_k$ is given by

$$M(k) = E[(T_k-\sigma^2)^2] = (E[T_k]-\sigma^2)^2+Var[T_k]=\frac{(n-2)(k-1)^2+2}{n}\sigma^4$$

whose minimum takes place at $k=1$.
--&gt;
&lt;hr /&gt;
&lt;p&gt;Part VIII:&lt;/p&gt;
&lt;p&gt;Consider the multiple linear regression model
&lt;span class=&#34;math display&#34;&gt;\[y_i = \beta_0+\beta_1 x_{i1}+\beta_2x_{i2}+\dots+\beta_{p-1}x_{i,p-1} +\epsilon_i,\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(i=1,\dots,n\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n&amp;gt;p\ge 2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;(a). Find the least squares estimates (LSE) of &lt;span class=&#34;math inline&#34;&gt;\(\beta_0,\dots,\beta_{p-1}\)&lt;/span&gt; via the matrix formalism. What assumptions are required for ensuring a unique solution of the LSE?&lt;/p&gt;
&lt;p&gt;(b). Show that the the residuals sum to zero. Are the standard assumptions of &lt;span class=&#34;math inline&#34;&gt;\(E[\epsilon_i]=0\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\dots,n\)&lt;/span&gt; required to establish the statement?&lt;/p&gt;
&lt;p&gt;(c). Suppose that &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_i\stackrel{iid}{\sim} N(0,\sigma^2)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\sigma&amp;gt;0\)&lt;/span&gt; is an unknown parameter. Define &lt;span class=&#34;math inline&#34;&gt;\(\alpha = \sum_{i=1}^{p-1} \beta_i^2\)&lt;/span&gt;. Use the generalized likelihood ratio method to test the hypothesis&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0: \alpha = 0\ vs.\ H_1:\alpha&amp;gt;0.\]&lt;/span&gt;
If the coefficient of determination &lt;span class=&#34;math inline&#34;&gt;\(R^2=0.95\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(p = 3\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n=13\)&lt;/span&gt;, is the null rejected at the significant level &lt;span class=&#34;math inline&#34;&gt;\(\alpha =0.05\)&lt;/span&gt;? (&lt;span class=&#34;math inline&#34;&gt;\(F_{0.95}(2,10)=4.10,F_{0.95}(3,10)=3.71,t_{0.95}(10)=1.81\)&lt;/span&gt;)&lt;/p&gt;
&lt;!-- 
`Solution`: 

(a). The model is $Y=X\beta+\epsilon$, and the LSE is $\hat\beta = (X^\top X)^{-1} X^\top Y$. It is requried that
that $\text{rank} (X) = p$.

(b). $\hat\epsilon = Y-X\hat \beta = Y-X(X^\top X)^{-1} X^\top Y=(I_n-P)Y$

$$\hat \epsilon^\top X = Y^\top (I_n-P)X = 0.$$

As a result, we have $\hat \epsilon^\top 1 = \sum_{i=1}^n \hat\epsilon_i=0$. We do not require any assumption on $\epsilon_i$.

(C). The test statistic is 

$$F =\frac{S_R^2/(p-1)}{S_e^2/(n-p)}=\frac{R^2/(p-1)}{(1-R^2)/(n-p)}=\frac{0.95/2}{(1-0.95)/10}=95&gt;F_{0.95}(2,10)=4.1.$$
We therefore reject the null.

--&gt;
</description>
    </item>
    
    <item>
      <title>第一次作业</title>
      <link>/course/homework1/</link>
      <pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/course/homework1/</guid>
      <description>&lt;p&gt;（1）韦布尔分布(Weibull distribution)族
&lt;span class=&#34;math display&#34;&gt;\[p(x)=\frac k\lambda\left(\frac{x}{\lambda}\right)^{k-1}e^{-(x/\lambda)^k}1\{x\ge 0\},k&amp;gt;0,\lambda&amp;gt;0\]&lt;/span&gt;
是不是指数型分布族？&lt;!-- **答案：B** --&gt;&lt;/p&gt;
&lt;p&gt;A. 是&lt;/p&gt;
&lt;p&gt;B. 不是&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;注意：这里的未知参数有两个，分别是&lt;span class=&#34;math inline&#34;&gt;\(k,\lambda\)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr /&gt;
&lt;p&gt;（2）从均值为&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, 方差为&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;的总体中随机抽取样本量为&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;的样本&lt;span class=&#34;math inline&#34;&gt;\(x_1,\dots,x_n\)&lt;/span&gt;, 其中&lt;span class=&#34;math inline&#34;&gt;\(\mu,\sigma^2\)&lt;/span&gt;均未知，指出下列样本函数中哪些为统计量（ ）。&lt;!-- **答案：ADE** --&gt;&lt;/p&gt;
&lt;p&gt;A. &lt;span class=&#34;math inline&#34;&gt;\(T_1=x_1+x_2\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;B. &lt;span class=&#34;math inline&#34;&gt;\(T_2=x_1+x_2-2\mu\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;C. &lt;span class=&#34;math inline&#34;&gt;\(T_3=(x_1-\mu)/\sigma\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;D. &lt;span class=&#34;math inline&#34;&gt;\(T_4=(\bar x-10)/5\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;E. &lt;span class=&#34;math inline&#34;&gt;\(T_5=\frac 1 n\sum_{i=1}^n(x_i-S_n)^2\)&lt;/span&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;（3）设&lt;span class=&#34;math inline&#34;&gt;\(\bar x_n,s_n^2\)&lt;/span&gt;表示样本&lt;span class=&#34;math inline&#34;&gt;\(x_1,\dots,x_n\)&lt;/span&gt;的样本均值与样本方差。已知&lt;span class=&#34;math display&#34;&gt;\[n=15,\bar x_{n}=168, s_n=11.43, x_{n+1}=170.\]&lt;/span&gt; 求&lt;span class=&#34;math inline&#34;&gt;\(\bar x_{n+1},s_{n+1}^2\)&lt;/span&gt;，以及修正样本方差&lt;span class=&#34;math inline&#34;&gt;\(s_{n+1}^{*2}\)&lt;/span&gt;.&lt;/p&gt;
&lt;!-- &gt; 答案来自：张华@16统计 

![](1张华-16统计.jpeg)


&gt; 注意：有同学把已知条件$s_n=11.43$看成$s_n^2=11.43$
--&gt;
&lt;hr /&gt;
&lt;p&gt;（4）设&lt;span class=&#34;math inline&#34;&gt;\(X_1\sim Ga(\alpha_1,\lambda)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(X_2\sim Ga(\alpha_2,\lambda)\)&lt;/span&gt;, 且&lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt;与&lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt;独立。证明&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y_1=X_1+X_2\sim Ga(\alpha_1+\alpha_2,\lambda)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y_2=X_1/(X_1+X_2)\sim Beta(\alpha_1,\alpha_2)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y_1\)&lt;/span&gt;与&lt;span class=&#34;math inline&#34;&gt;\(Y_2\)&lt;/span&gt;独立&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- &gt; 答案来自：王博@16统计 

![](2王博-16统计.jpeg)

&gt; 注意：大部分同学每一问都分别给出证明；实际上只需在求第三问时算出他们的联合密度函数即可，容易观察出联合密度函数是“可分离”的。

--&gt;
&lt;hr /&gt;
&lt;p&gt;（5）设&lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt;是来自某连续总体的一个样本，总体的分布函数&lt;span class=&#34;math inline&#34;&gt;\(F(x)\)&lt;/span&gt;是连续严增函数，证明：统计量&lt;span class=&#34;math inline&#34;&gt;\(T=-2\sum_{i=1}^n \ln F(X_i)\sim \chi^2(2n)\)&lt;/span&gt;.&lt;/p&gt;
&lt;!-- &gt; 答案来自：刘霏@16信管 

![](3刘霏-16信管.jpeg)
--&gt;
&lt;hr /&gt;
&lt;p&gt;（6）从正态总体&lt;span class=&#34;math inline&#34;&gt;\(N(52,6.3^2)\)&lt;/span&gt;中随机抽取容量为36的样本。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;求样本均值&lt;span class=&#34;math inline&#34;&gt;\(\bar X\)&lt;/span&gt;的分布；&lt;/li&gt;
&lt;li&gt;求&lt;span class=&#34;math inline&#34;&gt;\(\bar X\)&lt;/span&gt;落在区间&lt;span class=&#34;math inline&#34;&gt;\((50.8,53.8)\)&lt;/span&gt;内的概率；&lt;/li&gt;
&lt;li&gt;若要以&lt;span class=&#34;math inline&#34;&gt;\(99\%\)&lt;/span&gt;的概率保证&lt;span class=&#34;math inline&#34;&gt;\(|\bar X-52|&amp;lt;2\)&lt;/span&gt;, 试问样本量至少应取多少？&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- &gt; 答案来自：甘桃菁@16信计 

![](4.jpeg)

--&gt;
</description>
    </item>
    
    <item>
      <title>第一章：绪论</title>
      <link>/course/chap01/</link>
      <pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/course/chap01/</guid>
      <description>&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;目录&lt;/h2&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;1.1 数理统计是一门什么样的学科？&lt;/h3&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;1.2 统计学的发展简史&lt;/h3&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;1.3 基本概念&lt;/h3&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;1.4 抽样分布&lt;/h3&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;1.5 充分统计量&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1.1 数理统计是一门什么样的学科？&lt;/h2&gt;
&lt;p&gt;它使用概率论和其它数学方法，研究怎样收集（通过试验和观察）带有随机误差的数据，并在设定的模型（称为统计模型）之下，对这种数据进行分析（称为统计分析），以对所研究的问题作出推断（称为统计推断）。
由于所收集的统计数据（资料）只能反映事物的局部特征，数理统计的任务就在于从统计资料所反映的局部特征以概率论作为理论基础去推断事物的整体特征。&lt;/p&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;本质：由局部（有限样本）推断整体（总体）&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;数据&lt;/li&gt;
&lt;li&gt;模型&lt;/li&gt;
&lt;li&gt;推断&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;数据是什么？&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;data.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;模型是什么？&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;刻画实际问题&lt;/li&gt;
&lt;li&gt;能够进行统计分析&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;essentially-all-models-are-wrong-but-some-are-useful.-george-box&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Essentially, all models are wrong, but some are useful. —— &lt;a href=&#34;https://en.wikipedia.org/wiki/All_models_are_wrong&#34;&gt;George Box&lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;什么样的推断？&lt;/h2&gt;
&lt;p&gt;由样本到总体的推理称为&lt;strong&gt;统计推断&lt;/strong&gt;，有两种基本形式：&lt;/p&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;参数估计&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;模型中未知参数&lt;/li&gt;
&lt;li&gt;与“业务相关”的未知量&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;假设检验&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;判断命题的真假&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;案例&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;zhihu.jpg&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;案例&lt;/h2&gt;
&lt;p&gt;问题1：OPPO手机充电五分钟通话时间为多少？（参数估计）&lt;/p&gt;
&lt;p&gt;问题2：“OPPO手机充电五分钟通话2小时”是否可信？（假设检验）&lt;/p&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;一般步骤&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;数据收集：用&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;部手机进行测试，记录通话时间&lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;模型假定：假设通话时间&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;服从正态分布&lt;span class=&#34;math inline&#34;&gt;\(N(\mu,\sigma^2)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;数据分析：通过观测数据&lt;span class=&#34;math inline&#34;&gt;\(x_1,\dots,x_n\)&lt;/span&gt;作出统计推断&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1.2 统计学的发展简史&lt;/h2&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;第一个时期（萌芽阶段）&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;20世纪以前，描述性统计&lt;/li&gt;
&lt;li&gt;代表性人物：高斯(C. F. Gauss, 1777-1855), 皮尔逊(K. Pearson, 1857-1936)等&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;第二个时期（蓬勃发展阶段）&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;20世纪初至第二次世界大战&lt;/li&gt;
&lt;li&gt;代表性人物：费希尔(R. A. Fisher, 1890-1962), 奈曼(J. Neyman, 1894-1981), 小皮尔逊(E. S. Pearson, 1895-1980), 许宝騄(1910-1970)等&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;第三个时期（理论与应用高速发展）&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;二战后至今，得益于计算机的发展，统计方法渗透许多学科&lt;/li&gt;
&lt;li&gt;贝叶斯学派的兴起&lt;/li&gt;
&lt;li&gt;大数据时代与人工智能的发展&lt;/li&gt;
&lt;li&gt;现代统计依赖强大的计算能力&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;频率学派与贝叶斯学派&lt;/h2&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;频率学派（传统学派）&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;频率学派认为样本信息来自总体，仅通过研究&lt;strong&gt;样本信息&lt;/strong&gt;可以对&lt;strong&gt;总体信息&lt;/strong&gt;做出合理的推断和估计，并且样本越多，就越准确。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;代表性人物：费希尔 (R. A. Fisher, 1890-1962)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;贝叶斯学派&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;起源于英国学者贝叶斯(T. Bayes, 1702-1761)在1763年发表的著名论文《论有关机遇问题的求解》&lt;/li&gt;
&lt;li&gt;最基本观点：任何一个未知量都可以看作是随机的，应该用一个概率分布去描述未知参数，而不是频率派认为的固定值。这种信息称为&lt;strong&gt;先验信息&lt;/strong&gt;，是主观信息。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Good (1973)评价道：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“主观主义者直抒他们的判断，而客观主义者以假设来掩盖其判断，并以此享受科学客观性的荣耀。”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;贝叶斯公式&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;bigbang_bayes.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;贝叶斯统计的发展&lt;/h2&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;应用领域&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;自然语言处理：计算机翻译语言、识别语音、认识文字和海量文献的检索&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;南京市长江大桥欢迎您!&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;人工智能、无人驾驶&lt;/li&gt;
&lt;li&gt;垃圾短信、垃圾邮件识别&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;贝叶斯决策&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;如何在一个陌生的地方找餐馆吃饭？&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## 贝叶斯统计课程(研究生课程)

本课程不涉及贝叶斯统计内容，欢迎对贝叶斯统计感兴趣的同学参加以下课程。

### 教材

- **贝叶斯数据分析（第三版）**, A. Gelman等，机械工业出版社
[https://item.jd.com/11886268.html](https://item.jd.com/11886268.html)

### 上课时间
- 1-12周，周一下午第5—8节，共48学时

### 上课地点
- 四号楼4135
--&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;统计学专业&lt;/h2&gt;
&lt;p&gt;统计学的应用涉及金融、经济、社会学、工程学、环境等多个领域，从而形成的相应的研究分支。其特点是多学科交叉、实用为主。&lt;/p&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;统计学专业包含理论统计和应用统计两方面&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;理论统计：模型选择，非参统计方法，贝叶斯统计，时间序列与生存分析，高维数据分析与机器学习，数据挖掘等等。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;应用统计：目前发展最为突出的是生物统计，金融统计等等。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;统计学专业&lt;/h2&gt;
&lt;p&gt;统计学经过漫长的发展，尤其是计算机的大量应用，目前包括但不限于下面这些分支（或者交叉领域）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;理论研究：概率论（比如Stochastic Process），计算统计理论（比如Asymptotic Theory，在CS系的Computational Theory下面）等&lt;/li&gt;
&lt;li&gt;统计模型（在前人基础上继续发展各种Regression Model，Stratification，Clustering，Blocking，classification等等）、各种Test的发展（比如Time Series，Likelihood Ratio Test, Wald test, Permutation test 等）&lt;/li&gt;
&lt;li&gt;计算统计方法的发展（比如Monte Carlo Simulation，Bootstrap）&lt;/li&gt;
&lt;li&gt;数据采集（Census，Survey和Clinical Trial等）&lt;/li&gt;
&lt;li&gt;生物统计（比如Longitudinal Analysis，Spatial Analysis）&lt;/li&gt;
&lt;li&gt;Machine Learning
Data Mining&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;目前最火热的学科都是跟计算机结合比较紧密的。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;statistician-salaries-in-the-united-states&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Statistician Salaries in the United States&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;salary.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1.3 基本概念&lt;/h2&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;(1) 总体&lt;/h3&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;(2) 样本&lt;/h3&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;(3) 分布族&lt;/h3&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;(4) 统计量与估计量&lt;/h3&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;(5) 经验分布函数&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;总体&lt;/h2&gt;
&lt;p&gt;我们把研究对象的全体（包括有形的和潜在的）称作&lt;strong&gt;总体&lt;/strong&gt;，其中每个成员称为&lt;strong&gt;个体&lt;/strong&gt;。常用随机变量&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;来刻画一个总体（或者总体的特征值）。&lt;/p&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;例&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;网上购物居民占全市居民的比例&lt;/li&gt;
&lt;li&gt;过去一年内网购居民的购物次数&lt;/li&gt;
&lt;li&gt;某品牌灯泡的寿命&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;注&lt;/strong&gt;：总体&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;的分布函数&lt;span class=&#34;math inline&#34;&gt;\(F(x)\)&lt;/span&gt;未知或者部分未知，统计学的核心任务就是要对总体进行观测，并对所得数据推断总体的分布信息。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;样本&lt;/h2&gt;
&lt;p&gt;研究总体可分为&lt;strong&gt;普查&lt;/strong&gt;和&lt;strong&gt;抽样&lt;/strong&gt;这两种方法。&lt;/p&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;普查（全数检查）&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;对总体中的每个个体进行观察，如我国每十年一次的人口普查&lt;/li&gt;
&lt;li&gt;缺点：费用高、时间长、不适合破坏性试验&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;抽样&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;从总体中抽取若干个体进行观察，用所获得数据对总体进行统计推断&lt;/li&gt;
&lt;li&gt;优点：费用低、时间短&lt;/li&gt;
&lt;li&gt;抽取的部分组成的集合&lt;span class=&#34;math inline&#34;&gt;\((X_1,\dots,X_n)\)&lt;/span&gt;称为&lt;strong&gt;样本&lt;/strong&gt;，&lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt;称为&lt;strong&gt;样品&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;样品个数&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;称为&lt;strong&gt;样本量或者样本容量&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;简单随机抽样&lt;/h2&gt;
&lt;p&gt;简单随机抽样满足以下两个特征：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;随机性：每个个体都有相同的机会选中（有放回随机抽取/独立重复观测），即&lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt;与&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;同分布&lt;/li&gt;
&lt;li&gt;独立性：每个样本的选取是独立的&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这种方式得到的样本称为&lt;strong&gt;简单随机样本（简称样本）&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt;独立同分布(independent and identically distributed, iid)&lt;/li&gt;
&lt;li&gt;本课程所研究的均为简单随机样本&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;样本具有两重性&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;抽取之前无法预知它们的数值，故&lt;span class=&#34;math inline&#34;&gt;\((X_1,\dots,X_n)\)&lt;/span&gt;为&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;维随机向量&lt;/li&gt;
&lt;li&gt;抽取后样本为具体的数，用小写字母&lt;span class=&#34;math inline&#34;&gt;\((x_1,\dots,x_n)\)&lt;/span&gt;表示，称为&lt;strong&gt;样本观测值&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;注：所有的统计分析都是基于随机变量，统计推断结论基于样本观测值（数据）。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;案例：&lt;/h2&gt;
&lt;p&gt;“二战”期间，为了加强对战机的防护，英美军方调查了作战后幸存飞机上弹痕的分布，决定哪里弹痕多就加强哪里，你支持这种做法吗？&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;plane.jpg&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;2018ii&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;案例：2018年高考全国II卷作文&lt;/h2&gt;
&lt;p&gt;2018年高考全国II卷（适用地区: 内蒙古、黑龙江、辽宁、吉林、重庆、陕西、甘肃、宁夏、青海、新疆、西藏、海南）作文题目如下:&lt;/p&gt;
&lt;p&gt;“二战”期间，为了加强对战机的防护，英美军方调查了作战后幸存飞机上弹痕的分布，决定哪里弹痕多就加强哪里，然而统计学家瓦尔德(Abrahom Wald, 1902–1950)力排众议，指出更应该注意弹痕少的部位，因为这些部位受到重创的战机，很难有机会返航，而这部分数据被忽略了。事实证明沃德是正确的。&lt;/p&gt;
&lt;p&gt;要求: 综合材料内容及含义，选好角度，确定立意，明确文体，自拟标题; 不要套作，不得抄袭; 不少于800字。&lt;/p&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;这就是所谓的“幸存者偏见”&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;概率分布族&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;模型假定&lt;/strong&gt;：总体&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;分布&lt;span class=&#34;math inline&#34;&gt;\(F(x)\)&lt;/span&gt;属于某个分布族&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F}\)&lt;/span&gt;. 分为以下三类：&lt;/p&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;参数族&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F}\)&lt;/span&gt;中的分布的一般数学形式已知，但包含若干未知参数&lt;span class=&#34;math inline&#34;&gt;\(\theta=(\theta_1,\dots,\theta_m)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F}:=\{F_\theta,\theta\in\Theta\}\)&lt;/span&gt;, 其中&lt;span class=&#34;math inline&#34;&gt;\(\Theta\subset \mathbb{R}^m\)&lt;/span&gt;称为参数空间。&lt;/li&gt;
&lt;li&gt;该模型为&lt;strong&gt;参数统计问题&lt;/strong&gt;，&lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt;为模型的维数&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(m=1\)&lt;/span&gt;为单参数统计问题，&lt;span class=&#34;math inline&#34;&gt;\(m&amp;gt;1\)&lt;/span&gt;为多参数统计问题&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;非参数族&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;当&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F}\)&lt;/span&gt;中的分布不能通过有限个未知参数来刻画&lt;/li&gt;
&lt;li&gt;该模型为&lt;strong&gt;非参数统计问题&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;半参数族&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F}\)&lt;/span&gt;中的分布有一部分可以用参数刻画，一部分则不可以。&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;常用的参数族&lt;/h2&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;离散型&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;二项分布族&lt;span class=&#34;math inline&#34;&gt;\(\{b(n,p);0&amp;lt;p&amp;lt;1\}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;几何分布族&lt;span class=&#34;math inline&#34;&gt;\(\{Ge(p);0&amp;lt;p&amp;lt;1\}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;泊松分布族&lt;span class=&#34;math inline&#34;&gt;\(\{P(\lambda);\lambda&amp;gt;0\}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;连续型&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;正态分布族&lt;span class=&#34;math inline&#34;&gt;\(\{N(\mu,\sigma^2);-\infty&amp;lt;\mu&amp;lt;\infty,\sigma&amp;gt;0\}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;均匀分布族&lt;span class=&#34;math inline&#34;&gt;\(\{U(a,b);-\infty&amp;lt;a&amp;lt;b&amp;lt;\infty\}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;指数分布族&lt;span class=&#34;math inline&#34;&gt;\(\{Exp(\lambda);\lambda&amp;gt;0\}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;伽玛分布族&lt;/h2&gt;
&lt;p&gt;伽玛分布族&lt;span class=&#34;math inline&#34;&gt;\(\{Ga(\alpha,\lambda),\alpha&amp;gt;0,\lambda&amp;gt;0\}\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;为形状参数，&lt;span class=&#34;math inline&#34;&gt;\(1/\lambda\)&lt;/span&gt;为尺度参数&lt;/p&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;密度函数&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f(x) = \frac{\lambda^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\lambda x}1\{x\ge 0\}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;span class=&#34;math inline&#34;&gt;\(\Gamma(\alpha)=\int_0^{+\infty} x^{\alpha-1}e^{-x}dx\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Gamma(1)=1,\Gamma(1/2)=\sqrt{\pi}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Gamma(\alpha+1)=\alpha\Gamma(\alpha)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;当&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;为整数&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;时，&lt;span class=&#34;math inline&#34;&gt;\(\Gamma(n+1)=n!\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;frac-alpha-lambdafrac-alpha-lambda2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;期望：&lt;span class=&#34;math inline&#34;&gt;\(\frac \alpha \lambda\)&lt;/span&gt;；方差：&lt;span class=&#34;math inline&#34;&gt;\(\frac \alpha {\lambda^2}\)&lt;/span&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;两个特例&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\alpha=1\)&lt;/span&gt;时伽玛分布为指数分布，即&lt;span class=&#34;math inline&#34;&gt;\(Ga(\alpha,\lambda)=Exp(\lambda)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\alpha=n/2,\lambda=1/2\)&lt;/span&gt;时伽玛分布为自由度为&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;的卡方分布，即&lt;span class=&#34;math inline&#34;&gt;\(Ga(n/2,1/2)=\chi^2(n)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;伽玛密度函数&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/course/chap01_files/figure-html/gammaplot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;伽玛分布的性质&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;性质1（可加性）&lt;/strong&gt;：设&lt;span class=&#34;math inline&#34;&gt;\(X_1\sim Ga(\alpha_1,\lambda),\ X_2\sim Ga(\alpha_2,\lambda)\)&lt;/span&gt;。如果&lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt;与&lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt;独立，则
&lt;span class=&#34;math display&#34;&gt;\[X_1+X_2\sim Ga(\alpha_1+\alpha_2,\lambda).\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;性质2&lt;/strong&gt;：设&lt;span class=&#34;math inline&#34;&gt;\(X\sim Ga(\alpha,\lambda)\)&lt;/span&gt;,则&lt;span class=&#34;math inline&#34;&gt;\(kX\sim Ga(\alpha,\lambda/k)\)&lt;/span&gt;, 其中&lt;span class=&#34;math inline&#34;&gt;\(k&amp;gt;0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;提示：&lt;span class=&#34;math inline&#34;&gt;\(Ga(\alpha,\lambda)\)&lt;/span&gt;分布的特征函数为&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\phi(t)=E[e^{itX}]=\left(1-\frac{it}\lambda\right)^{-\alpha}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;贝塔分布族&lt;/h2&gt;
&lt;p&gt;贝塔分布族&lt;span class=&#34;math inline&#34;&gt;\(\{Beta(\alpha,\beta),\alpha&amp;gt;0,\beta&amp;gt;0\}\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(\alpha,\beta\)&lt;/span&gt;为形状参数&lt;/p&gt;
&lt;div id=&#34;-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;密度函数&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f(x) = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1}1\{0&amp;lt;x&amp;lt;1\}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fracalphaalphabetafracalpha-betaalphabeta2alphabeta1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;期望：&lt;span class=&#34;math inline&#34;&gt;\(\frac{\alpha}{\alpha+\beta}\)&lt;/span&gt;；方差：&lt;span class=&#34;math inline&#34;&gt;\(\frac{\alpha \beta}{(\alpha+\beta)^2(\alpha+\beta+1)}\)&lt;/span&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;alphabeta1beta11u01&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;特例：当&lt;span class=&#34;math inline&#34;&gt;\(\alpha=\beta=1\)&lt;/span&gt;时，&lt;span class=&#34;math inline&#34;&gt;\(Beta(1,1)=U(0,1)\)&lt;/span&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;适用场合&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;不合格率&lt;/li&gt;
&lt;li&gt;市场占有率&lt;/li&gt;
&lt;li&gt;命中率&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;贝塔密度函数&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;beta.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;指数型分布族&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;定义&lt;/strong&gt;：指数型分布族&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F}=\{f_\theta(x);\theta\in\Theta\}\)&lt;/span&gt;中的分布（分布列或者密度函数）都可以表示成如下形式：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f_\theta(x)=c(\theta)\exp\{\sum_{j=1}^kc_j(\theta)T_j(x)\}h(x)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;为正整数&lt;/li&gt;
&lt;li&gt;分布的支撑与参数&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;无关&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(c(\theta),c_j(\theta)\)&lt;/span&gt;为参数空间&lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt;上的函数&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(h(x)&amp;gt;0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(T_1(x),\dots,T_k(x)\)&lt;/span&gt;线性无关&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;常见的指数型分布族&lt;/h2&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;正态分布族是指数型分布族&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f(x,\mu,\sigma)=\frac 1{\sqrt{2\pi}\sigma}e^{-\mu^2/(2\sigma^2)}\exp\{\frac{\mu}{\sigma^2}x-\frac{1}{2\sigma^2}x^2\}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;二项分布族是指数型分布族&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(X=x) = C_n^x p^x(1-p)^{n-x}=(1-p)^n\exp\{\ln[p/(1-p)]x \}C_n^x\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;伽玛/贝塔分布族是指数型分布族&lt;/h3&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;均匀分布族不是指数型分布族&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;指数型分布族的优点&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;性质&lt;/strong&gt;：如果&lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt;是来自某指数型分布族中某分布的样本，则样本的联合分布还是指数型分布。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(f_\theta(x_1,\dots,x_n)=\prod_{i=1}^np_\theta(x_i)=c(\theta)^n\exp\{\sum_{j=1}^kc_j(\theta)\sum_{i=1}^nT_j(x_i)\}\prod_{i=1}^nh(x_i)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;统计量与估计量&lt;/h2&gt;
&lt;p&gt;样本是总体的反映，但样本所含信息不能直接用于解决我们所要研究的问题，而需要把样本所含的信息进行数学上的加工使其浓缩起来，从而解决我们的问题。为此，数理统计学往往构造一个合适的&lt;strong&gt;依赖于样本的函数&lt;/strong&gt;，我们称之为&lt;strong&gt;统计量&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;定义&lt;/strong&gt;：如果&lt;span class=&#34;math inline&#34;&gt;\((X_1,\dots,X_n)\)&lt;/span&gt;为来自总体的样本，若样本函数&lt;span class=&#34;math display&#34;&gt;\[T=T(X_1,\dots,X_n)\]&lt;/span&gt;中&lt;strong&gt;不含有任何未知参数&lt;/strong&gt;，则称&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;为统计量。统计量的分布称为&lt;strong&gt;抽样分布&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;定义&lt;/strong&gt;：用于估计未知参数的统计量称为&lt;strong&gt;点估计量&lt;/strong&gt;，或者简称&lt;strong&gt;估计量&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;注：这里的未知参数常指以下几种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;分布中所含的未知参数&lt;/li&gt;
&lt;li&gt;分布的数字特征：期望、方差、标准差、分位数等&lt;/li&gt;
&lt;li&gt;某事件的概率&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;例&lt;/h2&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt;为来自&lt;span class=&#34;math inline&#34;&gt;\(X\sim N(\mu,\sigma^2)\)&lt;/span&gt;的样本, 若&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;已知，&lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;未知，判断&lt;span class=&#34;math inline&#34;&gt;\(T_1,T_2\)&lt;/span&gt;是否为统计量。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[T_1 = \frac{\sqrt{n}(\sum_{i=1}^n X_i-\mu)}{\sigma}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[T_2 = \min(X_1,\dots,X_n)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;常见的统计量&lt;/h2&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;样本均值&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\bar{X}=\bar X_n=\frac{1}{n}\sum_{i=1}^n X_i\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;样本方差&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[S_n^2=\frac{1}{n}\sum_{i=1}^n (X_i-\bar{X})^2\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;修正样本方差&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[S_n^{*2}=\frac{1}{n-1}\sum_{i=1}^n (X_i-\bar{X})^2=\frac{n}{n-1}S_n^2\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;样本标准差&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[S_n=\sqrt{S_n^2}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;常见的统计量&lt;/h2&gt;
&lt;div id=&#34;k&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;样本&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;阶原点矩&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\overline{X^k}=\frac{1}{n}\sum_{i=1}^n X_i^k\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;k&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;样本&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;阶中心矩&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{1}{n}\sum_{i=1}^n (X_i-\bar X)^k\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;顺序统计量&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X_{(1)}\le X_{(2)}\le \dots\le X_{(n)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;span class=&#34;math inline&#34;&gt;\(X_{(1)}=\min\{X_1,\dots,X_n\}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(X_{(n)}=\max\{X_1,\dots,X_n\}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(X_{(k)}\)&lt;/span&gt;为&lt;span class=&#34;math inline&#34;&gt;\({X_1,\dots,X_n}\)&lt;/span&gt;的递增排序的第&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;位。&lt;span class=&#34;math inline&#34;&gt;\(X_{(n)}-X_{(1)}\)&lt;/span&gt;样本极差。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;样本中位数&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\tilde{X}=
        \begin{cases}
        X_{(\frac{n+1}{2})},\ &amp;amp;\text{$n$为奇数}\\
        (X_{(\frac{n}{2})}+X_{(\frac{n}{2}+1)})/2,\ &amp;amp;\text{$n$为偶数}
        \end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;经验分布函数&lt;/h2&gt;
&lt;p&gt;通过样本的观测值构造一种函数来近似总体的分布函数&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;定义&lt;/strong&gt;：设总体&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;的样本&lt;span class=&#34;math inline&#34;&gt;\((X_1,\dots,X_n)\)&lt;/span&gt;的一次观测值&lt;span class=&#34;math inline&#34;&gt;\((x_1,\dots,x_n)\)&lt;/span&gt;, 并将它们由小到大排列&lt;span class=&#34;math inline&#34;&gt;\(x_{(1)}\le x_{(2)}\le \dots\le x_{(n)}\)&lt;/span&gt;, 经验分布函数(或称样本分布函数)定义为&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
F_n(x) =\frac{1}{n}\sum_{i=1}^n 1\{x_i\le x\} = \begin{cases}
0,&amp;amp;\ x&amp;lt;x_{(1)}\\
1/n,&amp;amp;\ x_{(1)}\le x&amp;lt;x_{(2)}\\
2/n,&amp;amp;\ x_{(2)}\le x&amp;lt;x_{(3)}\\
&amp;amp;\vdots\\
k/n,&amp;amp;\ x_{(k)}\le x&amp;lt;x_{(k+1)}\\
&amp;amp;\vdots\\
1,&amp;amp;\ x&amp;gt;x_{(n)}\\
\end{cases}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;经验分布函数示意图&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/course/chap01_files/figure-html/ecdf-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;经验分布函数的性质&lt;/h2&gt;
&lt;p&gt;固定的&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(F_n(x)\)&lt;/span&gt;表示事件&lt;span class=&#34;math inline&#34;&gt;\(\{X\le x\}\)&lt;/span&gt;的频率，由强大数定律知，
&lt;span class=&#34;math display&#34;&gt;\[F_n(x)\to P(X\le x)=F(x),\]&lt;/span&gt;
即
&lt;span class=&#34;math display&#34;&gt;\[P\left(\lim_{n\to\infty}F_n(x)=F(x)\right)=1.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;格里汶科定理&lt;/strong&gt;(定理4.1, p48)给出更强的结果（几乎处处一致收敛）:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P\left(\lim_{n\to\infty}\sup_{x\in \mathbb{R}}|F_n(x)-F(x)|=0\right)=1.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注&lt;/strong&gt;：由此可见，当&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;相当大时，经验分布函数&lt;span class=&#34;math inline&#34;&gt;\(F_n(x)\)&lt;/span&gt;是母体分布函数&lt;span class=&#34;math inline&#34;&gt;\(F(x)\)&lt;/span&gt;的一个良好近似。数理统计学中一切都以样本为依据，其理由就在于此。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1.4 抽样分布&lt;/h2&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;(1) 样本均值的分布&lt;/h3&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;(2) 正态总体的抽样分布&lt;/h3&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;(3) 顺序统计量的分布&lt;/h3&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;(4) 样本分位数的分布&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;抽样分布&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;定义&lt;/strong&gt;：统计量的概率分布称为抽样分布，分为如下三类：&lt;/p&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;精确抽样分布&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;当总体&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;的分布已知时，如果对任意自然数&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;都能导出统计量&lt;span class=&#34;math inline&#34;&gt;\(T(X_1,\dots,X_n)\)&lt;/span&gt;的分布的显示表达式&lt;/li&gt;
&lt;li&gt;对样本量&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;较小的统计推断问题（小样本问题）特别有用&lt;/li&gt;
&lt;li&gt;精确抽样分布多数是在正态总体下得到&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;渐近抽样分布&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;寻求在样本量&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;无限大时统计量&lt;span class=&#34;math inline&#34;&gt;\(T(X_1,\dots,X_n)\)&lt;/span&gt;的极限分布&lt;/li&gt;
&lt;li&gt;适用于对样本量&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;较大的统计推断问题（大样本问题）&lt;/li&gt;
&lt;li&gt;常用的方法是中心极限定理&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;近似抽样分布&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;寻找一种分布来近似统计量&lt;span class=&#34;math inline&#34;&gt;\(T(X_1,\dots,X_n)\)&lt;/span&gt;的分布&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;样本均值的抽样分布&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;定理&lt;/strong&gt;：设&lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt;为来自总体&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;的样本，&lt;span class=&#34;math inline&#34;&gt;\(\bar X\)&lt;/span&gt;为其样本均值。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果&lt;span class=&#34;math inline&#34;&gt;\(X\sim N(\mu,\sigma^2)\)&lt;/span&gt;，则&lt;span class=&#34;math inline&#34;&gt;\(\bar X\)&lt;/span&gt;的精确分布为&lt;span class=&#34;math inline&#34;&gt;\(N(\mu,\sigma^2/n)\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;如果总体不是正态分布，但&lt;span class=&#34;math inline&#34;&gt;\(E[X]=\mu,Var[X]=\sigma^2\)&lt;/span&gt;存在，则&lt;span class=&#34;math inline&#34;&gt;\(\bar X\)&lt;/span&gt;的渐近分布为
&lt;span class=&#34;math inline&#34;&gt;\(N(\mu,\sigma^2/n)\)&lt;/span&gt;，记为&lt;span class=&#34;math inline&#34;&gt;\(\bar X\stackrel{\cdot}\sim N(\mu,\sigma^2/n)\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;不同总体均值的分布&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;CLT.jpg&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;卡方分布&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;定义&lt;/strong&gt;：设&lt;span class=&#34;math inline&#34;&gt;\(X_i\stackrel{iid}\sim N(0,1),i=1,\dots,n\)&lt;/span&gt;，则称随机变量
&lt;span class=&#34;math display&#34;&gt;\[X = X_1^2+\dots+X_n^2\]&lt;/span&gt;
服从自由度为&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;的卡方分布，记为&lt;span class=&#34;math inline&#34;&gt;\(X\sim \chi^2(n)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;密度函数&lt;/strong&gt;：&lt;span class=&#34;math inline&#34;&gt;\(f(x)=\frac{1}{2^{\frac n2}\Gamma(n/2)}x^{\frac n2-1}e^{-\frac x2} 1\{x&amp;gt;0\}\)&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;exn-varx2n.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;期望和方差：&lt;span class=&#34;math inline&#34;&gt;\(E[X]=n,\ Var[X]=2n\)&lt;/span&gt;.&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;可加性&lt;/strong&gt;：如果&lt;span class=&#34;math inline&#34;&gt;\(X\sim \chi^2(n)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(Y\sim \chi^2(m)\)&lt;/span&gt;且它们独立，则
&lt;span class=&#34;math display&#34;&gt;\[X+Y\sim \chi^2(n+m).\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;中心极限定理：
&lt;span class=&#34;math display&#34;&gt;\[\frac{X-n}{\sqrt{2n}}\stackrel{d}\to N(0,1).\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;卡方分布的密度函数曲线&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;Chi-square_pdf.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;样本方差的抽样分布（正态总体）&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;定理（定理3.3, p38）&lt;/strong&gt;：设&lt;span class=&#34;math inline&#34;&gt;\((X_1,\dots,X_n)\)&lt;/span&gt;为来自总体&lt;span class=&#34;math inline&#34;&gt;\(X\sim N(\mu,\sigma^2)\)&lt;/span&gt;的样本，则&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\sim N(\mu,\sigma^2/n)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{nS_n^2}{\sigma^2}=\frac{\sum_{i=1}^n(X_i-\bar{X})^2}{\sigma^2}\sim \chi^2(n-1)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;样本均值&lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt;与样本方差&lt;span class=&#34;math inline&#34;&gt;\(S_n^2\)&lt;/span&gt;相互独立&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为证明这个定理，我们需要用到多元正态分布的性质。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;引理&lt;/code&gt;：假设&lt;span class=&#34;math inline&#34;&gt;\(X_{1:n}=(X_1,\dots,X_n)^\top \sim N(\mu,\Sigma)\)&lt;/span&gt;，其中&lt;span class=&#34;math inline&#34;&gt;\(\mu=(\mu_1,\dots,\mu_n)^\top\)&lt;/span&gt;为均值向量, &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;为（非奇异）协方差矩阵。对任意可逆矩阵&lt;span class=&#34;math inline&#34;&gt;\(A\in \mathbb{R}^{n\times n}\)&lt;/span&gt;，有&lt;span class=&#34;math display&#34;&gt;\[AX_{1:n}\sim N(A\mu,A\Sigma A^\top).\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;证明&lt;/code&gt;：令&lt;span class=&#34;math inline&#34;&gt;\(Y_{1:n}=(Y_1,\dots,Y_n)^\top=AX_{1:n}\)&lt;/span&gt;，则其CDF为：
&lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
F_Y(y) &amp;amp;= P(Y_{1:n}\le y) = P(AX_{1:n}\le y)\\
&amp;amp;=\int_{Ax\le y}\frac 1{(2\pi)^{n/2}|\Sigma|^{1/2}}e^{-(1/2)(x-\mu)^\top \Sigma^{-1}(x-\mu)}d x\\
&amp;amp;=\int_{z\le y}|A^{-1}|\frac 1{(2\pi)^{n/2}|\Sigma|^{1/2}}e^{-(1/2)(A^{-1}z-\mu)^\top \Sigma^{-1}(A^{-1}z-\mu)}d z\\
&amp;amp;=\int_{z\le y}\frac 1{(2\pi)^{n/2}|A\Sigma A^\top|^{1/2}}e^{-(1/2)(z-A\mu)^\top (A\Sigma A^\top)^{-1}(z-A\mu)}d z.
\end{align}
\]&lt;/span&gt;
这表明&lt;span class=&#34;math inline&#34;&gt;\(Y_{1:n}\)&lt;/span&gt;的pdf为：
&lt;span class=&#34;math display&#34;&gt;\[f_Y(y) = \frac 1{(2\pi)^{n/2}|A\Sigma A^\top|^{1/2}}e^{-(1/2)(y-A\mu)^\top (A\Sigma A^\top)^{-1}(y-A\mu)},\]&lt;/span&gt;
即&lt;span class=&#34;math inline&#34;&gt;\(Y_{1:n}\sim N(A\mu,A\Sigma A^\top)\)&lt;/span&gt;. 证毕。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;定理的证明&lt;/code&gt;：易知&lt;span class=&#34;math inline&#34;&gt;\(X_{1:n}\sim N((\mu,\dots,\mu)^\top,\sigma^2 I_n)\)&lt;/span&gt;. 假设&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;为如下正交矩阵：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
A=\left[
\begin{matrix}
\frac 1{\sqrt{n}} &amp;amp; \frac 1{\sqrt{n}} &amp;amp; \frac 1{\sqrt{n}} &amp;amp; \cdots &amp;amp; \frac1{\sqrt{n}}\\
\frac 1{\sqrt{2\times 1}} &amp;amp; -\frac 1{\sqrt{2\times1}} &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0\\
\frac 1{\sqrt{3\times 2}} &amp;amp; \frac 1{\sqrt{3\times2}} &amp;amp; -\frac 2{\sqrt{3\times2}} &amp;amp; \cdots &amp;amp; 0\\
\vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots \\
\frac 1{\sqrt{n\times (n-1)}} &amp;amp; \frac 1{\sqrt{n\times (n-1)}} &amp;amp; \frac {1}{\sqrt{n\times (n-1)}} &amp;amp; \cdots &amp;amp; -\frac {n-1}{\sqrt{n\times (n-1)}}\\
\end{matrix}
\right].
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;令&lt;span class=&#34;math inline&#34;&gt;\(Y_{1:n}=AX_{1:n}\)&lt;/span&gt;. 由上面引理得，&lt;span class=&#34;math inline&#34;&gt;\(Y_{1:n}\sim N((\sqrt{n} \mu,0,\dots,0)^\top,\sigma^2 I_n)\)&lt;/span&gt;. 注意到，&lt;span class=&#34;math inline&#34;&gt;\(Y_1 = \sqrt{n}\bar X\)&lt;/span&gt;, 所以&lt;span class=&#34;math inline&#34;&gt;\(\bar X=Y_1/\sqrt{n}\sim N(\mu,\sigma^2/n)\)&lt;/span&gt;. 又&lt;span class=&#34;math display&#34;&gt;\[\sum_{i=1}^n Y_i^2 = Y_{1:n}^\top Y_{1:n} = (A X_{1:n})^\top  A X_{1:n}=X_{1:n}^\top X_{1:n}=\sum_{i=1}^n X_i^2.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以，
&lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\frac{nS_n^2}{\sigma^2}&amp;amp;=\frac{\sum_{i=1}^n(X_i-\bar{X})^2}{\sigma^2}=\frac{\sum_{i=1}^nX_i^2-n(\bar X)^2}{\sigma^2}\\
&amp;amp;=\frac{\sum_{i=1}^nY_i^2-Y_1^2}{\sigma^2}=\sum_{i=2}^n(Y_i/\sigma)^2\sim \chi^2(n-1),
\end{align}
\]&lt;/span&gt;
这是因为&lt;span class=&#34;math inline&#34;&gt;\(Y_i\sim N(0,\sigma^2),i=2,\dots,n\)&lt;/span&gt;. 由于&lt;span class=&#34;math inline&#34;&gt;\(Y_i\)&lt;/span&gt;相互独立，&lt;span class=&#34;math inline&#34;&gt;\(\bar X\)&lt;/span&gt;可以用&lt;span class=&#34;math inline&#34;&gt;\(Y_1\)&lt;/span&gt;表示，&lt;span class=&#34;math inline&#34;&gt;\(S_n^2\)&lt;/span&gt;可以用&lt;span class=&#34;math inline&#34;&gt;\(Y_2,\dots,Y_n\)&lt;/span&gt;表示，所以它们独立。证毕。&lt;/p&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;p&gt;研究发现，只有正态总体才有“样本均值与方差独立”这一性质。&lt;/p&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;div id=&#34;t&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;t分布&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;定义&lt;/strong&gt;：设&lt;span class=&#34;math inline&#34;&gt;\(X\sim N(0,1), Y\sim \chi^2(n)\)&lt;/span&gt;, 且它们独立，则称随机变量&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[T = \frac{X}{\sqrt{Y/n}}\]&lt;/span&gt;
服从自由度为&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;的学生氏t分布（简称&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;分布），记为&lt;span class=&#34;math inline&#34;&gt;\(T\sim t(n)\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;密度函数&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
f(x)=
\frac{\Gamma\left(\frac{n+1}2\right)}{\sqrt{n\pi }\Gamma\left(\frac n2\right)}\left(1+\frac{x^2}n\right)^{-\frac{n+1}{2}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;两种特例&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;当&lt;span class=&#34;math inline&#34;&gt;\(n=1\)&lt;/span&gt;时，t分布成为柯西分布.&lt;/li&gt;
&lt;li&gt;可以证明：&lt;span class=&#34;math inline&#34;&gt;\(\lim_{n\to\infty}f(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}\)&lt;/span&gt;.
当&lt;span class=&#34;math inline&#34;&gt;\(n\ge 25\)&lt;/span&gt;时,可以认为t分布与&lt;span class=&#34;math inline&#34;&gt;\(N(0,1)\)&lt;/span&gt;接近。&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;t&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;t分布的密度函数曲线&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;tpdf.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;t&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;t分布的起源&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;tinfo.jpg&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;样本均值与标准差之比的抽样分布&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;定理&lt;/strong&gt;：设&lt;span class=&#34;math inline&#34;&gt;\((X_1,\dots,X_n)\)&lt;/span&gt;为来自总体&lt;span class=&#34;math inline&#34;&gt;\(X\sim N(\mu,\sigma^2)\)&lt;/span&gt;的样本，则&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{\bar{X}-\mu}{S_n/\sqrt{n-1}}=\frac{\bar{X}-\mu}{S_n^*/\sqrt{n}}\sim t(n-1)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;比较：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{\bar{X}-\mu}{\sigma/\sqrt{n}}\sim N(0,1)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pxc&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;尾部概率P(|X|&amp;gt;c)的比较&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;分布&lt;/th&gt;
&lt;th&gt;c=2&lt;/th&gt;
&lt;th&gt;c=2.5&lt;/th&gt;
&lt;th&gt;c=3&lt;/th&gt;
&lt;th&gt;c=3.5&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;X~N(0,1)&lt;/td&gt;
&lt;td&gt;0.0455&lt;/td&gt;
&lt;td&gt;0.0124&lt;/td&gt;
&lt;td&gt;0.0027&lt;/td&gt;
&lt;td&gt;0.000465&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;X~t(4)&lt;/td&gt;
&lt;td&gt;0.1161&lt;/td&gt;
&lt;td&gt;0.0668&lt;/td&gt;
&lt;td&gt;0.0399&lt;/td&gt;
&lt;td&gt;0.0249&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;f&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;F分布&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;定义&lt;/strong&gt;：设&lt;span class=&#34;math inline&#34;&gt;\(X\sim \chi^2(m), Y\sim \chi^2(n)\)&lt;/span&gt;, 且&lt;span class=&#34;math inline&#34;&gt;\(X,Y\)&lt;/span&gt;相互独立，则随机变量&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Z=\frac{X/m}{Y/n}\]&lt;/span&gt;
称为服从第一自由度为&lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt;、第二自由度为&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;的F分布，记&lt;span class=&#34;math inline&#34;&gt;\(Z\sim F(m,n)\)&lt;/span&gt;. 其密度函数为&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f(x)=
\frac{\Gamma((m+n)/2)}{\Gamma(m/2)\Gamma(n/2)}\left(\frac{m}{n}\right)^{m/2}x^{\frac m2-1}(1+mx/n)^{-(m+n)/2} 1\{x&amp;gt;0\}
\]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;f&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;F分布的性质&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Z\sim F(m,n)\)&lt;/span&gt;, 则&lt;span class=&#34;math inline&#34;&gt;\(1/Z\sim F(n,m)\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;如果&lt;span class=&#34;math inline&#34;&gt;\(T\sim t(n)\)&lt;/span&gt;, 则&lt;span class=&#34;math inline&#34;&gt;\(T^2\sim F(1,n)\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;f&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;F分布的密度函数曲线&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;fpdf.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;两个独立正态总体的抽样分布&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;定理&lt;/strong&gt;：设两独立总体&lt;span class=&#34;math inline&#34;&gt;\(X\sim N(\mu_1,\sigma_1^2)\)&lt;/span&gt;,&lt;span class=&#34;math inline&#34;&gt;\(Y\sim N(\mu_2,\sigma_2^2)\)&lt;/span&gt;的样本分别为&lt;span class=&#34;math inline&#34;&gt;\((X_1,\dots,X_m),(Y_1,\dots,Y_n)\)&lt;/span&gt;. 样本方差分别为&lt;span class=&#34;math inline&#34;&gt;\(S_{1m}^2,S_{2n}^2\)&lt;/span&gt;. 则&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{(\bar X-\bar Y)-(\mu_1-\mu_2)}{\sqrt{\sigma_1^2/m+\sigma_2^2/n}}\sim N(0,1).\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{mS_{1m}^2/\sigma_1^2/(m-1)}{nS_{2n}^2/\sigma_2^2/(n-1)}=\frac{S_{1m}^{*2}\sigma_2^2}{S_{2n}^{*2}\sigma_1^2}\sim F(m-1,n-1).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;如果&lt;span class=&#34;math inline&#34;&gt;\(\sigma_1=\sigma_2=\sigma\)&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{(\bar X-\bar Y)-(\mu_1-\mu_2)}{S_w\sqrt{1/m+1/n}}\sim t(m+n-2),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;span class=&#34;math inline&#34;&gt;\(S_w =\sqrt{(mS_{1m}^2+nS_{2n}^2)/(m+n-2)}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;顺序统计量&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;定理&lt;/strong&gt;：若&lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt;独立同分布，分布函数和密度函数分别为&lt;span class=&#34;math inline&#34;&gt;\(F(x),f(x)\)&lt;/span&gt;. 则&lt;span class=&#34;math inline&#34;&gt;\(X_{(1)}=\min(X_1,\dots,X_n)\)&lt;/span&gt;的分布函数和密度函数分别&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{cases}
F_{X_{(1)}}(x) = 1-(1-F(x))^n\\
f_{X_{(1)}}(x) = n(1-F(x))^{n-1}f(x).
\end{cases}
\]&lt;/span&gt;
&lt;span class=&#34;math inline&#34;&gt;\(X_{(n)}=\max(X_1,\dots,X_n)\)&lt;/span&gt;的分布函数和密度函数分别&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{cases}
F_{X_{(n)}}(x) = F(x)^n\\
f_{X_{(n)}}(x) = nF(x)^{n-1}f(x).
\end{cases}
\]&lt;/span&gt;
更一般地，&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f_{X_{(k)}}(x) = \frac{n!}{(n-k)!(k-1)!}F(x)^{k-1}(1-F(x))^{n-k}f(x),k=1,\dots,n.
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;顺序统计量的联合分布&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;定理&lt;/strong&gt;：顺序统计量&lt;span class=&#34;math inline&#34;&gt;\((X_{(i)},X_{(j)})(i&amp;lt;j)\)&lt;/span&gt;的联合密度函数为&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
f_{X_{(i)},X_{(j)}}(x,y) = &amp;amp;\frac{n!}{(i-1)!(j-i-1)!(n-j)!}F(x)^{i-1}\\
&amp;amp;[F(y)-F(x)]^{j-i-1}[1-F(y)]^{n-j}f(x)f(y) 1\{x\le y\}.
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;定理&lt;/strong&gt;：顺序统计量&lt;span class=&#34;math inline&#34;&gt;\((X_{(1)},\dots,X_{(n)})\)&lt;/span&gt;的联合密度函数为&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
f(y_1,\dots,y_n)=
\begin{cases}
n!\prod_{i=1}^nf(y_i),&amp;amp;y_1&amp;lt;y_2&amp;lt;\dots&amp;lt;y_n\\
0,&amp;amp;else
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;分位数&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;定义&lt;/strong&gt;：设&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;的分布函数为&lt;span class=&#34;math inline&#34;&gt;\(F(x)\)&lt;/span&gt;. 对于任意&lt;span class=&#34;math inline&#34;&gt;\(\alpha\in(0,1)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;分位数&lt;span class=&#34;math inline&#34;&gt;\(x_\alpha\)&lt;/span&gt;满足&lt;span class=&#34;math inline&#34;&gt;\(F(x_\alpha)=\alpha\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;标准正态分布分位数记为&lt;span class=&#34;math inline&#34;&gt;\(u_{\alpha}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;分布分位数记为&lt;span class=&#34;math inline&#34;&gt;\(t_{\alpha}(n)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt;分布分位数记为&lt;span class=&#34;math inline&#34;&gt;\(\chi^2_{\alpha}(n)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;分布分位数记为&lt;span class=&#34;math inline&#34;&gt;\(F_{\alpha}(m,n)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;一些说明&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;在分位点表中对于标准正态分布、&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;分布和F分布只能查到&lt;span class=&#34;math inline&#34;&gt;\(\alpha&amp;gt;1/2\)&lt;/span&gt;的分位数，需利用以下对称性间接查&lt;span class=&#34;math inline&#34;&gt;\(\alpha&amp;lt;1/2\)&lt;/span&gt;的分位数：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[u_\alpha=-u_{1-\alpha},\  t_\alpha(n)=-t_{1-\alpha}(n),\ F_{\alpha}(m,n)=\frac{1}{F_{1-\alpha}(n,m)}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;对于&lt;span class=&#34;math inline&#34;&gt;\(t(n)\)&lt;/span&gt;分布，由于当&lt;span class=&#34;math inline&#34;&gt;\(n\to \infty\)&lt;/span&gt;时，其极限分布为&lt;span class=&#34;math inline&#34;&gt;\(N(0,1)\)&lt;/span&gt;, 所以自由度&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;比较大时，&lt;span class=&#34;math inline&#34;&gt;\(t_{\alpha}(n)\approx u_{\alpha}\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;若&lt;span class=&#34;math inline&#34;&gt;\(X\sim \chi^2(n)\)&lt;/span&gt;分布，由于当&lt;span class=&#34;math inline&#34;&gt;\(n\to \infty\)&lt;/span&gt;时，&lt;span class=&#34;math inline&#34;&gt;\((X-n)/\sqrt{2n}\stackrel{d}\to N(0,1)\)&lt;/span&gt;, 所以自由度&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;比较大时，&lt;span class=&#34;math inline&#34;&gt;\(\chi^2_{\alpha}(n)\approx u_{\alpha}\sqrt{2n}+n\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;分位数示意图&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;quantile.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;样本分位数&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;定义&lt;/strong&gt;：设&lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt;为样本，其顺序统计量为&lt;span class=&#34;math inline&#34;&gt;\(X_{(1)},\dots,X_{(n)}\)&lt;/span&gt;.
对给定的&lt;span class=&#34;math inline&#34;&gt;\(0&amp;lt;\alpha&amp;lt;1\)&lt;/span&gt;, 该样本的&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;分位数定义为：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
m_\alpha = \begin{cases}
\frac{1}2[X_{([n\alpha])}+X_{([n\alpha]+1)}],&amp;amp;np\text{是整数}\\
X_{([n\alpha]+1)},&amp;amp;np\text{不是整数}
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;span class=&#34;math inline&#34;&gt;\([a]\)&lt;/span&gt;表示&lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;的整数部分。&lt;/p&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;样本分位数的渐近分布&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;定理&lt;/strong&gt;：设总体的密度为&lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt;,&lt;span class=&#34;math inline&#34;&gt;\(x_\alpha\)&lt;/span&gt;为其&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;分位数，如果&lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt;在&lt;span class=&#34;math inline&#34;&gt;\(x_\alpha\)&lt;/span&gt;处连续，且&lt;span class=&#34;math inline&#34;&gt;\(f(x_\alpha)&amp;gt;0\)&lt;/span&gt;, 则当&lt;span class=&#34;math inline&#34;&gt;\(n\to \infty\)&lt;/span&gt;时，样本的分位数&lt;span class=&#34;math inline&#34;&gt;\(m_\alpha\)&lt;/span&gt;的渐近分布为：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
m_\alpha \stackrel{\cdot}\sim N\left(x_\alpha,\frac{\alpha(1-\alpha)}{nf^2(x_\alpha)}\right).
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;6种杀虫剂的数据&lt;/h2&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:insectsprays&#34;&gt;Table 1: &lt;/span&gt;&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;spray&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;count&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;spray&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;count&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;spray&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;count&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;spray&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;count&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;spray&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;count&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;spray&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;count&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;A&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;D&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;F&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;A&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;D&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;F&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;A&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;20&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;21&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;D&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;F&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;A&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;D&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;F&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;22&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;A&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;D&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;F&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;A&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;D&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;F&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;A&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;D&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;F&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;A&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;23&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;D&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;F&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;A&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;19&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;D&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;F&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;26&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;A&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;20&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;21&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;D&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;F&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;26&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;A&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;D&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;F&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;24&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;A&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;D&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;F&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;箱线图&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/course/chap01_files/figure-html/boxplot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1.5 充分统计量&lt;/h2&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;(1) 充分统计量的定义&lt;/h3&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;(2) 因子分解定理&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;充分统计量&lt;/h2&gt;
&lt;p&gt;目标&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;简化数据&lt;/li&gt;
&lt;li&gt;不损失重要信息&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;定义&lt;/strong&gt;：设有一个分布族&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt;是从某分布&lt;span class=&#34;math inline&#34;&gt;\(F\in\mathcal{F}\)&lt;/span&gt;中抽取的一个样本。&lt;span class=&#34;math inline&#34;&gt;\(T=T(X_1,\dots,X_n)\)&lt;/span&gt;是一个（向量）统计量。若在给定&lt;span class=&#34;math inline&#34;&gt;\(T=t\)&lt;/span&gt;下，样本&lt;span class=&#34;math inline&#34;&gt;\((X_1,\dots,X_n)\)&lt;/span&gt;的条件分布与总体分布&lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;无关，则称&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;为此分布族&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F}\)&lt;/span&gt;的充分统计量。如果&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F}=\{F_\theta;\theta\in\Theta\}\)&lt;/span&gt;是参数分布族，在给定&lt;span class=&#34;math inline&#34;&gt;\(T=t\)&lt;/span&gt;下，样本&lt;span class=&#34;math inline&#34;&gt;\((X_1,\dots,X_n)\)&lt;/span&gt;的条件分布与参数&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;无关，则称&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;为参数&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的充分统计量。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;例1：两点分布&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;例&lt;/strong&gt;：总体&lt;span class=&#34;math inline&#34;&gt;\(X\sim b(1,p),0&amp;lt;p&amp;lt;1\)&lt;/span&gt;. 判断以下两个统计量是否是充分统计量&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(T_1=\sum_{i=1}^nX_i\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(T_2=X_1+X_2\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;例2：几何分布&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;例&lt;/strong&gt;：总体&lt;span class=&#34;math inline&#34;&gt;\(X\sim Ge(p),0&amp;lt;p&amp;lt;1\)&lt;/span&gt;, 证明&lt;span class=&#34;math inline&#34;&gt;\(T=\sum_{i=1}^nX_i\)&lt;/span&gt;是参数&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;的充分统计量。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;3&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;例3：正态分布&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;例&lt;/strong&gt;：总体&lt;span class=&#34;math inline&#34;&gt;\(X\sim N(\mu,\sigma^2)\)&lt;/span&gt;，其中&lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;已知。证明&lt;span class=&#34;math inline&#34;&gt;\(T=\sum_{i=1}^nX_i\)&lt;/span&gt;是参数&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;的充分统计量。&lt;/p&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;引理&lt;/h3&gt;
&lt;p&gt;设总体的密度为&lt;span class=&#34;math inline&#34;&gt;\(f_\theta(x)\)&lt;/span&gt;. 则在给定&lt;span class=&#34;math inline&#34;&gt;\(T=t\)&lt;/span&gt;下，样本的条件密度函数为&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
f_\theta(x_1,\dots,x_n|T=t)=\frac{\prod_{i=1}^nf_\theta(x_i) 1\{T(x_1,\dots,x_n)=t\}}{f^T_\theta(t)},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;span class=&#34;math inline&#34;&gt;\(f^T_\theta(t)\)&lt;/span&gt;为&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;的密度函数。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;思考题&lt;/h2&gt;
&lt;p&gt;顺序统计量&lt;span class=&#34;math inline&#34;&gt;\(X_{(1)},\dots,X_{(n)}\)&lt;/span&gt;是否充分统计量？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;连续分布族&lt;/li&gt;
&lt;li&gt;离散分布族&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;因子分解定理&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;J. Neyman和P. R. Halmos在20世纪40年代提出&lt;/li&gt;
&lt;li&gt;判断充分统计量的法则&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;定理&lt;/strong&gt;：设样本的分布为&lt;span class=&#34;math inline&#34;&gt;\(f_\theta(x_1,\dots,x_n)\)&lt;/span&gt;（在离散总体情况下表示样本的分布列，在连续总体情况下表示样本的密度函数）。则在统计量&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;是充分的当且仅当存在两个函数满足&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(h(x_1,\dots,x_n)\)&lt;/span&gt;非负&lt;/li&gt;
&lt;li&gt;在统计量&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;取值空间上的函数&lt;span class=&#34;math inline&#34;&gt;\(g_\theta(t)\)&lt;/span&gt;, 使得&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
f_\theta(x_1,\dots,x_n) = g_\theta(T(x_1,\dots,x_n))h(x_1,\dots,x_n),\ \forall\theta\in\Theta, \forall x_i
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;因子分解定理的应用&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;例1&lt;/strong&gt;：总体分布为&lt;span class=&#34;math inline&#34;&gt;\(U(0,\theta)\)&lt;/span&gt;, 求参数&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的充分统计量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;例2&lt;/strong&gt;：总体分布为&lt;span class=&#34;math inline&#34;&gt;\(N(\mu,\sigma^2)\)&lt;/span&gt;，求&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;参数&lt;span class=&#34;math inline&#34;&gt;\((\mu,\sigma^2)\)&lt;/span&gt;的充分统计量&lt;/li&gt;
&lt;li&gt;当&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;已知时，&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;的充分统计量&lt;/li&gt;
&lt;li&gt;当&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;已知时，&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;的充分统计量&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;指数型分布族的充分统计量&lt;/h2&gt;
&lt;p&gt;指数型分布族下的样本分布为&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f_\theta(x_1,\dots,x_n)=\prod_{i=1}^np_\theta(x_i)=c(\theta)^n\exp\{\sum_{j=1}^kc_j(\theta)\sum_{i=1}^nT_j(x_i)\}\prod_{i=1}^nh(x_i)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;由因子分解定理知，参数&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的一个充分统计量为&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\left(\sum_{i=1}^nT_1(x_i),\dots,\sum_{i=1}^nT_k(x_i)\right).\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;充分统计量有无穷多个&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;定理&lt;/strong&gt;：如果&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;是充分统计量，且&lt;span class=&#34;math inline&#34;&gt;\(T=\psi(S)\)&lt;/span&gt;, 其中&lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt;是可测函数，&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;是另一个统计量，则&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;也是充分统计量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;例&lt;/strong&gt;：总体分布为&lt;span class=&#34;math inline&#34;&gt;\(N(\mu,\sigma^2)\)&lt;/span&gt;，以下哪些统计量为参数&lt;span class=&#34;math inline&#34;&gt;\((\mu,\sigma^2)\)&lt;/span&gt;的充分统计量&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((\bar X, S_n)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((\bar X, S_n^2)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((\bar X, S_n^*)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((\bar X, S_n^{*2})\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((\sum_{i=1}^n X_i,\sum_{i=1}^n X_i^2)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((\sum_{i=1}^n X_i,\sum_{i=1}^n |X_i|)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((\sum_{i=1}^n X_i,\sum_{i=1}^n |X_i|,\sum_{i=1}^n X_i^2)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;思考&lt;/strong&gt;：哪种最好？&lt;em&gt;最小充分统计量&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>第七次作业</title>
      <link>/course/homework7/</link>
      <pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/course/homework7/</guid>
      <description>&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt; be a sample from an exponential distribution &lt;span class=&#34;math inline&#34;&gt;\(Exp(\lambda)\)&lt;/span&gt;. Given a significance level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;, derive a likelihood ratio test of&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0:\lambda=\lambda_1\ vs.\ H_1:\lambda=\lambda_2,\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1\neq\lambda_2\)&lt;/span&gt;.&lt;/p&gt;
&lt;!-- 
`Solution`: The likelihood function is 
$$L(\lambda)=\prod_{i=1}^n (\lambda e^{-\lambda x_i}) = \lambda^ne^{-\lambda n\bar x}.$$


The likelihood ratio is given by
$$\lambda(\vec x)= \frac{L(\lambda_2)}{L(\lambda_1)}=\frac{\lambda_2^ne^{-\lambda_2 n\bar x}}{\lambda_1^ne^{-\lambda_1 n\bar x}}=(\lambda_2/\lambda_1)^ne^{(\lambda_1-\lambda_2)n\bar x}.$$

Choose the ``test statistic`` $T(\vec x) = 2\lambda_1n\bar x$. When $\lambda=\lambda_1$, $T(\vec X)\sim \chi^2(2n)$. Also, 
$$\lambda(\vec x) = (\lambda_2/\lambda_1)^ne^{(\lambda_1-\lambda_2)T(\vec x)/(2\lambda_1)}.$$


1. Note that $\lambda_1,\lambda_2&gt;0$. If $\lambda_1&gt;\lambda_2$, the rejection region is of the form $W=\{T(\vec x)&gt;C\}$. We thus have $C=\chi_{1-\alpha}^2(2n)$.

2. If $\lambda_1&lt;\lambda_2$, the rejection region is of the form $W=\{T(\vec x)&lt;C\}$. We thus have $C=\chi_{\alpha}^2(2n)$.

--&gt;
&lt;hr /&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt; be a sample from an exponential distribution &lt;span class=&#34;math inline&#34;&gt;\(Exp(\lambda)\)&lt;/span&gt;. Given a significance level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;, derive a UMPU test of&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0:\lambda=\lambda_0\ vs.\ H_1:\lambda\neq\lambda_0.\]&lt;/span&gt;&lt;/p&gt;
&lt;!-- 
`Solution`: Exponential distribution belongs to exponential family of the form $S(\lambda)h(x)e^{Q(\lambda)V(x)}$ with $V(x) = -x$ and $Q(\lambda)=\lambda$. Choose the ``test statistic`` $T(\vec x)=2\lambda_0 n\bar x$.  As a result, the UMPU rejection region has the form
$$W = \{T(\vec x)&lt;C_1 \text{ or } &gt;C_2\},$$
where $C_1,C_2$ satisfy 
$$P_{\lambda_0}(\bar X\in W)=\alpha$$
and
$$E_{\lambda_0}[1\{\vec X\in W\}T(\vec X)]=\alpha E_{\lambda_0}[T(\vec X)].$$
Let $f(x;n)$ be the density of $\chi^2(n)$, that is 
$$f(x;n)=\frac{1}{2^{n/2}\Gamma(n/2)}x^{n/2-1}e^{-x/2}1\{x&gt;0\}.$$
If $\lambda=\lambda_0$, $T(\vec X)\sim \chi^2(2n)$. We thus have
$$\int_{C_1}^{C_2}f(x;2n) d x=1-\alpha,\quad(1)$$
$$\int_{C_1}^{C_2} x f(x;2n)dx = 2n(1-\alpha).$$
The later equality can be expressed as
$$\int_{C_1}^{C_2} \frac{x}{2n} f(x;2n)dx=\int_{C_1}^{C_2} \frac{x}{2n} \frac{1}{2^{n}\Gamma(n)}x^{n-1}e^{-x/2}dx=\int_{C_1}^{C_2} f(x;2n+2)dx=1-\alpha.\quad(2)$$

It is hard to solve the equations (1) and (2). In practice, we may take
$C_1=\chi_{\alpha/2}^2(2n)$ and $C_2=\chi_{1-\alpha/2}^2(2n)$ so that the significance level of the test is $\alpha$. BUT, it is not the exact UMPU test since (2) is not satisfied. If $n$ is large enough, $f(x;2n+2)\approx f(x;2n)$ (see figure below). This implies the resulting rejection region is almost UMPU when $n$ is large.

&lt;img src=&#34;/course/homework7_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;

Of course, you can solve the equations (1) and (2) by numerical algorithms, such as bisection and Newton&#39;s methods. Let $F(x;n)$ be the CDF of $\chi^2(n)$ and $F^{-1}$ denote its inverse. By (1), we have $C_2=F^{-1}(F(C_1;2n)+1-\alpha;2n)$. Substituting  it into (2), we arrive at an equation:
$$F(F^{-1}(F(C_1;2n)+1-\alpha;2n);2n+2)-F(C_1;2n+2)=1-\alpha.$$

We can slove the equation using R function `uniroot`. The code is given below.


```r
myfun &lt;- function(c,n,alpha)
  pchisq(qchisq(pchisq(c,2*n)+1-alpha,2*n),2*n+2)-pchisq(c,2*n+2)-1+alpha

mysolver &lt;- function(n,alpha){
  a = qchisq(alpha/2,2*n)
  b = qchisq(alpha/2,2*n+2)
  ## solve the equation by using the root finding algorithm
  r = uniroot(myfun,n=n,alpha=alpha,interval = c(a,b))
  c1 = r$root
  c2 = qchisq(pchisq(c1,2*n)+1-alpha,2*n)
  err1 = pchisq(c2,2*n+2)-pchisq(c1,2*n+2)-1+alpha #check the error for eq. (2)
  ## the approximate method
  c11 = qchisq(alpha/2,2*n)
  c22 = qchisq(1-alpha/2,2*n)
  err2 = pchisq(c22,2*n+2)-pchisq(c11,2*n+2)-1+alpha #check the error for eq. (2)
  output = data.frame(exact=c(c1,c2,abs(err1)),rough=c(c11,c22,abs(err2)),
                      row.names = c(&#34;C1&#34;,&#34;C2&#34;,&#34;error&#34;))
  return(output)
}
alpha = 0.5
n = 10
output = mysolver(n,alpha)
knitr::kable(output,&#34;html&#34;,caption = &#34;n=10, alpha=0.05&#34;)
```

&lt;table&gt;
&lt;caption&gt;(\#tab:unnamed-chunk-2)n=10, alpha=0.05&lt;/caption&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt;   &lt;/th&gt;
   &lt;th style=&#34;text-align:right;&#34;&gt; exact &lt;/th&gt;
   &lt;th style=&#34;text-align:right;&#34;&gt; rough &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; C1 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 16.00121 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 15.451773 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; C2 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 24.61524 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 23.827692 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; error &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.00000 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.014194 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

For large $n=100$, we have the following results.


```r
n = 100
output = mysolver(n,alpha)
knitr::kable(output,&#34;html&#34;,caption = &#34;n=100, alpha=0.05&#34;)
```

&lt;table&gt;
&lt;caption&gt;(\#tab:unnamed-chunk-3)n=100, alpha=0.05&lt;/caption&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt;   &lt;/th&gt;
   &lt;th style=&#34;text-align:right;&#34;&gt; exact &lt;/th&gt;
   &lt;th style=&#34;text-align:right;&#34;&gt; rough &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; C1 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 186.8010 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 186.171668 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; C2 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 213.8065 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 213.102185 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; error &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.0000 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 0.001428 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

As expected, the error for the rough estimates $C_1=\chi_{\alpha/2}^2(2n)$ and $C_2=\chi_{1-\alpha/2}^2(2n)$ decreases as $n$ goes up. 

--&gt;
&lt;hr /&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt; be a sample from &lt;span class=&#34;math inline&#34;&gt;\(U[0,\theta]\)&lt;/span&gt;. Given a significance level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;, derive a UMP test of&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0:\theta=\theta_0\ vs.\ H_1:\theta&amp;gt;\theta_0.\]&lt;/span&gt;&lt;/p&gt;
&lt;!-- 
`Solution`: Firstly, consider the simple alternative:
$$H_0:\theta=\theta_0\ vs.\ H_1:\theta=\theta_1&gt;\theta_0.$$

The likelihood function is $L(\theta)=\theta^{-n}1\{x_{(n)}\le \theta\}$.
The likelihood ratio for the simple test is 
$$\lambda(\vec x) = \frac{\theta_1^{-n}1\{x_{(n)}\le \theta_1\}}{\theta_0^{-n}1\{x_{(n)}\le \theta_0\}}
=\begin{cases}
(\theta_0/\theta_1)^n,\ &amp;x_{(n)}\le \theta_0\\
\infty,\ &amp;x_{(n)}&gt; \theta_0
\end{cases}$$

We therefore cannot find a $\lambda_0$ such that $P_{\theta_0}(\lambda(\vec X)&gt;\lambda_0)=\alpha$. This implies that the N-P lemma cannot be applied. As we can see, the likelihood ratio is a function of $x_{(n)}$. We thus can use $X_{(n)}$ as the test statistic. A resonable rejection region would be
$W = \{x_{(n)}&gt;C\}$, where $C$ satisfies
$$P_{\theta_0}(X_{(n)}&gt;C)=\alpha.$$

When $\theta=\theta_0$, the order statistic $X_{(n)}/\theta_0$ has a CDF $F(x)=x^n$ (see [Exercise 1](https://hezhijian.netlify.com/post/homework5/)). So we have $C=(1-\alpha)^{1/n}\theta_0$. The rejection region is
$$W = \{\vec x:x_{(n)}&gt;(1-\alpha)^{1/n}\theta_0\}.$$


We next prove that $W$ is UMP. Suppose that there exists a rejection region $W&#39;$ satisfying $P_{\theta_0}(\vec X\in W&#39;)\le \alpha$. Let $A=[0,\theta_0]^n$ be the sample space when $\theta=\theta_0$, and let $B=[0,\theta_1]^n$ be the sample space  when $\theta=\theta_1$. It is clear that $A\subseteq B$ since $\theta_1&gt;\theta_0$. This implies 
$$P_{\theta_0}(\vec X\in W&#39;) = P_{\theta_0}(\vec X\in W&#39;\cap A)=\frac{1}{\theta_0^n}\int_{W&#39;\cap A} 1 d x_1\dots d x_n\le \alpha.$$
Similarly, 
$$P_{\theta_0}(\vec X\in W) = P_{\theta_0}(\vec X\in W\cap A)=\frac{1}{\theta_0^n}\int_{W\cap A} 1 d x_1\dots d x_n= \alpha.$$
Define $\mu(E) = \int_E 1 d x_1\dots d x_n$. So we have $\mu(W\cap A)\ge \mu(W&#39;\cap A)$. 


On the other hand, noticing that $W\cap B=W\cap A+\bar A\cap B$, we thus have

$$
\begin{align}
P_{\theta_1}(\vec X\in W) &amp;=\frac{1}{\theta_1^n}\int_{W\cap B} 1 d x_1\dots d x_n\\&amp;=\frac{1}{\theta_1^n}\int_{W\cap A}1 d x_1\dots d x_n+\frac{1}{\theta_1^n}\int_{\bar A\cap B}1 d x_1\dots d x_n\\
&amp;=\theta_1^{-n}[\mu(W\cap A)+\mu(\bar A\cap B)]\\
&amp;\ge \theta_1^{-n}[\mu(W&#39;\cap A)+\mu(W&#39;\cap\bar A\cap B)\\
&amp;=\theta_1^{-n}\mu(W&#39;\cap B)=P_{\theta_1}(\bar X\in W&#39;).
\end{align}
$$

Therefore, $W$ is UMP rejection region. Since $W$ does not depend on $\theta_1$, it is also the UMP rejection region for the alternative $H_1:\theta&gt;\theta_0$.

For this example, the UMP rejection region is not unique. Following the same procedure above, one can  easily prove that for any
set $W_0\subset A$ with $\mu(W_0)=\alpha\theta_0^n$, $W=W_0\cup \bar A$ is UMP.

--&gt;
&lt;hr /&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(X_1,X_2,X_3,X_4\)&lt;/span&gt; be a sample from &lt;span class=&#34;math inline&#34;&gt;\(N(\theta,1)\)&lt;/span&gt;. Given a significance level &lt;span class=&#34;math inline&#34;&gt;\(\alpha=0.1\)&lt;/span&gt;, derive a UMP test of&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0:\theta\ge 10\ vs.\ H_1:\theta&amp;lt;10.\]&lt;/span&gt;
Calculate the power of the test when &lt;span class=&#34;math inline&#34;&gt;\(\theta=9\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Solution&lt;/code&gt;: The test statistics is &lt;span class=&#34;math inline&#34;&gt;\(T(\vec x)=\frac{\bar x-10}{1/\sqrt{n}}=\sqrt{n}(\bar x-10)=2(\bar x-10)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(n=4\)&lt;/span&gt;. The UMP rejection region has the form &lt;span class=&#34;math inline&#34;&gt;\(W=\{T(\vec x)&amp;lt;C\}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; satisfies
&lt;span class=&#34;math display&#34;&gt;\[P(T(\vec X)&amp;lt;C|\theta=10)=\alpha=0.1\]&lt;/span&gt;
This gives &lt;span class=&#34;math inline&#34;&gt;\(C= u_{0.1}=-u_{0.9}=-1.28\)&lt;/span&gt;. So &lt;span class=&#34;math display&#34;&gt;\[W=\{\vec x|2(\bar x-10)&amp;lt;-1.28\}=\{\vec x|\bar x&amp;lt;9.36\}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The power of the test is
&lt;span class=&#34;math display&#34;&gt;\[P(\bar X&amp;lt;9.36|\theta=9)=P(2(\bar X-9)&amp;lt;0.72|\theta=9)=\Phi(0.72)=0.76.\]&lt;/span&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>第三次作业</title>
      <link>/course/homework3/</link>
      <pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/course/homework3/</guid>
      <description>&lt;p&gt;（课本p.59, 第2题）设&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;的分布密度函数为
&lt;span class=&#34;math display&#34;&gt;\[f(x)=\frac{1}{2\sigma} e^{-|x|/\sigma}\ (\sigma&amp;gt;0),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt;是&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;的样本，求&lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;的最大似然估计。&lt;/p&gt;
&lt;!-- 
`解`: 似然函数为

$$L(\sigma)=\prod_{i=1}^n f(x_i)=\prod_{i=1}^n \left(\frac{1}{2\sigma} e^{-|x_i|/\sigma}\right)=(2\sigma)^{-n}e^{-\sum_{i=1}^n|x_i|/\sigma}$$

对数似然函数为：
$$\ln L(\sigma) = -n \ln (2\sigma)-\left(\sum_{i=1}^n|x_i|\right)/\sigma.$$

对数似然方程为：
$$\frac{d \ln L(\sigma)}{d\sigma}=-\frac{n}{\sigma}+\frac{\sum_{i=1}^n|x_i|}{\sigma^2}=0.$$

其根是$\sigma^* = \frac{\sum_{i=1}^n|x_i|}{n}$. 又
$$\frac{d^2 \ln L(\sigma)}{d\sigma^2}\Bigg|_{\sigma=\sigma^*}=\frac{n}{\sigma^{*2}}-\frac{2\sum_{i=1}^n|x_i|}{\sigma^{*3}}=-\frac{n}{\sigma^{*2}}&lt;0.$$

所以，$\ln L(\sigma)$在$\sigma=\sigma^*$处取得最大值，故$\sigma$的最大似然估计为$\hat{\mu}= \frac{\sum_{i=1}^n|X_i|}{n}$.

&gt; 注意：最终的估计量要用大写字母$X_i$表示，这样才是估计量。用小写字母$x_i$表示的是估计值，是具体的数值，而不是估计量。

--&gt;
&lt;hr /&gt;
&lt;p&gt;（课本p.59, 第3题）设&lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt;是来自&lt;span class=&#34;math inline&#34;&gt;\([\theta,\theta+1]\)&lt;/span&gt;上均匀分布的样本，其中&lt;span class=&#34;math inline&#34;&gt;\(\theta\in\mathbb{R}\)&lt;/span&gt;, 证明&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的最大似然估计不止一个，并求出所有的最大似然估计。&lt;/p&gt;
&lt;!-- 
`证明`：似然函数为

$$L(\theta)=\prod_{i=1}^n f(x_i)=\prod_{i=1}^n 1\{\theta\le x_i\le \theta+1\}=1\{x_{(n)}-1\le\theta\le x_{(1)}\}$$

观察得知，当$\theta\in [x_{(n)}-1,x_{(1)}]$时，似然函数取得最大值$1$. 所以，$\theta$的最大似然估计不止一个，所有的最大似然估计为集合$[X_{(n)}-1,X_{(1)}]$.
--&gt;
&lt;hr /&gt;
&lt;p&gt;（课本p.59, 第4题）设随机变量&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;以均等机会按&lt;span class=&#34;math inline&#34;&gt;\(N(0,1)\)&lt;/span&gt;分布取值和按&lt;span class=&#34;math inline&#34;&gt;\(N(\mu,\sigma^2)\)&lt;/span&gt;分布取值，其中&lt;span class=&#34;math inline&#34;&gt;\(\mu\in \mathbb{R},\sigma^2&amp;gt;0\)&lt;/span&gt;. 这时&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;的分布密度函数为这两个分布的密度的平均，即
&lt;span class=&#34;math display&#34;&gt;\[f(x;\mu,\sigma^2) = \frac 12\frac{1}{\sqrt{2\pi}}e^{-x^2/2}+\frac 12\frac{1}{\sqrt{2\pi}\sigma}e^{-(x-\mu)^2/(2\sigma^2)},\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt;为此混合分布的简单随机样本，证明&lt;span class=&#34;math inline&#34;&gt;\(\mu,\sigma^2\)&lt;/span&gt;不存在最大似然估计。能否通过矩法估计&lt;span class=&#34;math inline&#34;&gt;\(\mu,\sigma^2\)&lt;/span&gt;？&lt;/p&gt;
&lt;!-- 
(1) `证明`：似然函数为
$$L(\mu,\sigma^2)=\prod_{i=1}^n \left(\frac 12\frac{1}{\sqrt{2\pi}}e^{-x_i^2/2}+\frac 12\frac{1}{\sqrt{2\pi}\sigma}e^{-(x_i-\mu)^2/(2\sigma^2)}\right)$$

取$\mu=x_1$, 则有
$$L(x_1,\sigma^2)\ge \frac{1}{2\sqrt{2\pi}\sigma}\prod_{i=2}^n\left(\frac 12\frac{1}{\sqrt{2\pi}}e^{-x_i^2/2}\right)$$

因为$\sigma\to 0$时，上式右端趋于无穷，所以似然函数$L(\mu,\sigma^2)$在$\mathbb{R}\times[0,\infty)$上无界，故最大似然估计不存在。


2. `解`：

$$E[X]=\int_{-\infty}^\infty xf(x;\mu,\sigma^2) dx= \frac \mu2$$

$$E[X^2] = \int_{-\infty}^\infty x^2f(x;\mu,\sigma^2)dx = \frac{1+\sigma^2+\mu^2}{2} $$

所以，
$$\begin{cases}
\mu &amp;= 2E[X]\\
\sigma^2 &amp;= 2E[X^2]-4(E[X])^2-1 = 2Var[X]-2(E[X])^2-1
\end{cases}
$$

矩估计为：
$$\begin{cases}
\hat{\mu} &amp;= 2\bar X\\
\hat{\sigma^2} &amp;= 2S_n^2-2\bar X^2-1
\end{cases}
\text{ 或者 }\begin{cases}
\hat{\mu} &amp;= \frac{2}{n}\sum_{i=1}^n X_i\\
\hat{\sigma^2} &amp;= \frac 2n\sum_{i=1}^n X_i^2-\frac{4}{n^2}(\sum_{i=1}^nX_i)^2-1
\end{cases}
$$
--&gt;
&lt;hr /&gt;
&lt;p&gt;（附加题I，选做）考虑上题的模型。设&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;为一随机变量，&lt;span class=&#34;math inline&#34;&gt;\(Y=1\)&lt;/span&gt;表示&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;来自&lt;span class=&#34;math inline&#34;&gt;\(N(0,1)\)&lt;/span&gt;分布，&lt;span class=&#34;math inline&#34;&gt;\(Y=0\)&lt;/span&gt;表示&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;来自&lt;span class=&#34;math inline&#34;&gt;\(N(\mu,\sigma^2)\)&lt;/span&gt;分布，即&lt;span class=&#34;math inline&#34;&gt;\(Y\sim b(1,0.5)\)&lt;/span&gt;. 假设我们可以观测&lt;span class=&#34;math inline&#34;&gt;\(Y_i\)&lt;/span&gt;的值，基于样本&lt;span class=&#34;math inline&#34;&gt;\((X_i,Y_i),i=1,\dots,n\)&lt;/span&gt;，是否可以求出&lt;span class=&#34;math inline&#34;&gt;\(\mu,\sigma^2\)&lt;/span&gt;的最大似然估计？事实上，&lt;span class=&#34;math inline&#34;&gt;\(Y_i\)&lt;/span&gt;的值不可观测（通常称为潜变量），此时你有没有更好的办法估计&lt;span class=&#34;math inline&#34;&gt;\(\mu,\sigma^2\)&lt;/span&gt;？&lt;/p&gt;
&lt;!-- 
`解`：当$Y_i$可观测时，似然函数为
$$L(\mu,\sigma^2)=\prod_{i=1}^n \left(\frac{y_i}{\sqrt{2\pi}}e^{-x_i^2/2}+\frac{1-y_i}{\sqrt{2\pi}\sigma}e^{-(x_i-\mu)^2/(2\sigma^2)}\right)$$

令$I = \{i=1,\dots,n|y_i=0\}$, 则
$$L(\mu,\sigma^2)=\prod_{i\notin I}\frac{1}{\sqrt{2\pi}}e^{-x_i^2/2}\prod_{i\in I}\frac{1}{\sqrt{2\pi}\sigma}e^{-(x_i-\mu)^2/(2\sigma^2)}$$
因为$\prod_{i\notin I}\frac{1}{\sqrt{2\pi}}e^{-x_i^2/2}$与参数$\mu,\sigma^2$无关, 则只需求出
$$\tilde{L}(\mu,\sigma^2):=\prod_{i\in I}\frac{1}{\sqrt{2\pi}\sigma}e^{-(x_i-\mu)^2/(2\sigma^2)}$$
的最大值点即可。这就等价于求样本为$\{X_i,i\in I\}$时，正态总体的最大似然估计，所以最大似然估计为

$$\hat{\mu}=\frac{1}{|I|}\sum_{i\in I} X_i,\ \hat{\sigma^2} = \frac{1}{|I|} \sum_{i\in I}(X_i-\hat{\mu})^2 $$
考虑到$I$中元素的个数$|I|$可能为0。当$I= \varnothing$时，似然函数不含未知参数，此时估计量可以为任意常数。故最终的估计量可以写成

$$\hat{\mu}=
\begin{cases}
\frac{1}{n-\sum_{i=1}^n Y_i}\sum_{i=1}^n X_i(1-Y_i), &amp; \sum_{i=1}^n Y_i&lt;n\\
c_1, &amp;\sum_{i=1}^n Y_i=n
\end{cases}
$$

$$
\hat{\sigma^2} = 
\begin{cases}
\frac{1}{n-\sum_{i=1}^n Y_i} \sum_{i=1}^n(X_i-\hat{\mu})^2(1-Y_i), &amp; \sum_{i=1}^n Y_i&lt;n\\
c_2, &amp;\sum_{i=1}^n Y_i=n
\end{cases}
$$

其中$c_1\in \mathbb{R}$, $c_2&gt;0$为常数。

当$Y_i$不可观测时，我们可以利用EM算法求出最大似然估计值。参考：

[Andrew Ng&#39;s lecture notes 1](http://cs229.stanford.edu/notes/cs229-notes7b.pdf)

[Andrew Ng&#39;s lecture notes 2](http://cs229.stanford.edu/notes/cs229-notes8.pdf)

--&gt;
&lt;hr /&gt;
&lt;p&gt;（附加题II，选做）若考虑更一般的混合分布：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f(x;\lambda,\mu_1,\sigma_1^2,\mu_2,\sigma_2^2)=\frac{\lambda}{\sqrt{2\pi}\sigma_1}e^{-(x-\mu_1)^2/(2\sigma_1^2)}+\frac{1-\lambda}{\sqrt{2\pi}\sigma_2}e^{-(x-\mu_2)^2/(2\sigma_2^2)}\]&lt;/span&gt;
其中&lt;span class=&#34;math inline&#34;&gt;\(\lambda\in[0,1],\mu_1,\mu_2\in \mathbb{R},\sigma_1^2,\sigma_2^2&amp;gt;0\)&lt;/span&gt;, 你能求出未知参数&lt;span class=&#34;math inline&#34;&gt;\(\lambda,\mu_1,\sigma_1^2,\mu_2,\sigma_2^2\)&lt;/span&gt;的矩估计吗？&lt;/p&gt;
&lt;!-- 

`解`：参考文献: [Estimation in Mixtures of Two Normal Distributions](paper.pdf)

--&gt;
&lt;hr /&gt;
&lt;p&gt;（课本p.59, 第9题）设&lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt;是来自分布密度为
&lt;span class=&#34;math display&#34;&gt;\[f(x;\theta)=\frac{\Gamma(\theta+1)}{\Gamma(\theta)\Gamma(1)}x^{\theta-1}1\{0\le x\le 1\}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;的总体的样本，其中&lt;span class=&#34;math inline&#34;&gt;\(\theta&amp;gt;0\)&lt;/span&gt;, 试用矩法估计&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;!-- 
`解`：

$$E[X]=\int_0^1 x\theta x^{\theta-1}d x=\frac{\theta}{\theta+1}$$

$$\theta = \frac{E[X]}{1-E[X]}$$

所以，矩估计为
$$\hat\theta =\frac{\bar X}{1-\bar X}=\frac{\sum_{i=1}X_i}{n-\sum_{i=1}^nX_i}$$
--&gt;
&lt;hr /&gt;
&lt;p&gt;（课本p.60, 第10题）设&lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt;是来自分布密度为
&lt;span class=&#34;math display&#34;&gt;\[f(x;c,\theta)=\frac{1}{2\theta}1\{c-\theta\le x\le c+\theta\}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;的总体的样本，其中&lt;span class=&#34;math inline&#34;&gt;\(\theta&amp;gt;0,c\in\mathbb{R}\)&lt;/span&gt;, 试用矩法估计&lt;span class=&#34;math inline&#34;&gt;\(c,\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;!-- 

`解`：已知总体$X\sim U[c-\theta,c+\theta]$, 所以

$$E[X]=\frac{(c+\theta)+(c-\theta)}{2}=c$$

$$Var[X] = \frac{(2\theta)^2}{12}=\frac{\theta^2}{3}$$

所以矩估计为：
$$\hat{c}=\bar X=\frac 1n\sum_{i=1}^nX_i,\ \hat{\theta} = \sqrt{3S_n^2}=\sqrt{\frac{3}{n}\sum_{i=1}^n(X_i-\bar X)^2}.$$
--&gt;
</description>
    </item>
    
  </channel>
</rss>
