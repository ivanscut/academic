<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dr. Zhijian He on Dr. Zhijian He</title>
    <link>/</link>
    <description>Recent content in Dr. Zhijian He on Dr. Zhijian He</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Zhijian He &amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +0800</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Chap 04: Linear Regression</title>
      <link>/talk/example-talk/</link>
      <pubDate>Wed, 28 Nov 2018 00:00:00 +0800</pubDate>
      
      <guid>/talk/example-talk/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;chap04.html&#34; target=&#34;_blank&#34;&gt;Slides&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>第九次作业</title>
      <link>/post/homework9/</link>
      <pubDate>Mon, 26 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/homework9/</guid>
      <description>&lt;p&gt;Consider the linear model
&lt;span class=&#34;math display&#34;&gt;\[y_i=\beta_0+\beta_1x_i+\epsilon_i,\ \epsilon_i\stackrel{iid}{\sim} N(0,\sigma^2), i=1,\dots,n.\]&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Derive the maximum likelihood estimators (MLE) for &lt;span class=&#34;math inline&#34;&gt;\(\beta_0,\beta_1\)&lt;/span&gt;. Are they consistent with the least square estimators (LSE)?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Derive the MLE for &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; and look at its unbiasedness.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A very slippery point is whether to treat the &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; as fixed numbers or as random variables. In the class, we treated the predictors &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; as fixed numbers for sake of convenience. Now suppose that the predictors &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; are iid random variables (independent of &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_i\)&lt;/span&gt;) with density &lt;span class=&#34;math inline&#34;&gt;\(f_X(x;\theta)\)&lt;/span&gt; for some parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. Write down the likelihood function for all of our data &lt;span class=&#34;math inline&#34;&gt;\((x_i,y_i),i=1,\dots,n\)&lt;/span&gt;. Derive the MLE for &lt;span class=&#34;math inline&#34;&gt;\(\beta_0,\beta_1\)&lt;/span&gt; and see whether the MLE changes if we work with the setting of random predictors?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;p&gt;Consider the linear model without intercept&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_i  = \beta x_i+\epsilon_i,\ i=1,\dots,n,\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_i\)&lt;/span&gt; are independent with &lt;span class=&#34;math inline&#34;&gt;\(E[\epsilon_i]=0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Var[\epsilon_i]=\sigma^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Write down the least square estimator &lt;span class=&#34;math inline&#34;&gt;\(\hat \beta\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;, and derive an unbiased estiamtor for &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For fixed &lt;span class=&#34;math inline&#34;&gt;\(x_0\)&lt;/span&gt;, let &lt;span class=&#34;math inline&#34;&gt;\(\hat{y}_0=\hat\beta x_0\)&lt;/span&gt;. Work out &lt;span class=&#34;math inline&#34;&gt;\(Var[\hat{y}_0]\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;code&gt;Case study&lt;/code&gt;: Genetic variability is thought to be a key factor in the survival of a species, the idea
being that “diverse” populations should have a better chance of coping with changing
environments. Table below summarizes the results of a study designed to test
that hypothesis experimentally. Two populations of fruit flies (Drosophila serrata)-one that was cross-bred (Strain A) and the other,
in-bred (Strain B)-were put into sealed containers where food and space were kept
to a minimum. Recorded every hundred days were the numbers of Drosophila alive
in each population.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;th&gt;Day number&lt;/th&gt;
&lt;th&gt;Strain A&lt;/th&gt;
&lt;th&gt;Strain B&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Feb 2&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;May 13&lt;/td&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;td&gt;250&lt;/td&gt;
&lt;td&gt;203&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Aug 21&lt;/td&gt;
&lt;td&gt;200&lt;/td&gt;
&lt;td&gt;304&lt;/td&gt;
&lt;td&gt;214&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Nov 29&lt;/td&gt;
&lt;td&gt;300&lt;/td&gt;
&lt;td&gt;403&lt;/td&gt;
&lt;td&gt;295&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Mar 8&lt;/td&gt;
&lt;td&gt;400&lt;/td&gt;
&lt;td&gt;446&lt;/td&gt;
&lt;td&gt;330&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Jun 16&lt;/td&gt;
&lt;td&gt;500&lt;/td&gt;
&lt;td&gt;482&lt;/td&gt;
&lt;td&gt;324&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Plot day numbers versus population sizes for Strain A and Strain B, respectively. Does the plot look linear? If so, please use least squares to figure out the coefficients and
their standard errors, and plot the two regression lines.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1^*\)&lt;/span&gt; be the true slopes (i.e., growth rates) for Strain A and Strain B, respectively. Assume the population sizes for the two strains are independent. Under the same assumptions of &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_i\stackrel{iid}{\sim} N(0,\sigma^2)\)&lt;/span&gt; for both strains, do we have enough evidence here
to reject the null hypothesis that &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\le \beta_1^*\)&lt;/span&gt; (significance level &lt;span class=&#34;math inline&#34;&gt;\(\alpha=0.05\)&lt;/span&gt;)? Or equivalently, do these data support the theory that genetically mixed populations have a
better chance of survival in hostile environments.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Approximate Bayesian Computation</title>
      <link>/post/abc/</link>
      <pubDate>Sun, 18 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/abc/</guid>
      <description>&lt;div id=&#34;bayesian-inference&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bayesian inference&lt;/h2&gt;
&lt;p&gt;Our goal is to estimate an expectation of &lt;span class=&#34;math inline&#34;&gt;\(a(\theta)\)&lt;/span&gt; under the posterior distribution&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\pi(\theta|y_{obs})=\frac{\pi(\theta)p(y_{obs}|\theta)}{p(y_{obs})}\propto \pi(\theta)p(y_{obs}|\theta)\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\pi(\theta)\)&lt;/span&gt; is the prior distribution&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(p(y|\theta)\)&lt;/span&gt; is the likelihood function&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the constant &lt;span class=&#34;math inline&#34;&gt;\(p(y_{obs})\)&lt;/span&gt; is intractable&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Suppose that one can generate samples &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(1)},\dots,\theta^{(N)}\sim \pi(\theta|y_{obs})\)&lt;/span&gt;, then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[E[a(\theta)|y_{obs}]\approx \frac 1 N\sum_{i=1}^N a(\theta^{(i)})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is the basic idea of Monte Carlo (MC).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;acceptance-rejection-ar-methods&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Acceptance rejection (AR) methods&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the target density &lt;span class=&#34;math inline&#34;&gt;\(f(\theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the proposal &lt;span class=&#34;math inline&#34;&gt;\(g(\theta)\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(g(\theta)&amp;gt;0\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(f(\theta)&amp;gt;0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(K\ge \sup_{\theta}\frac{f(\theta)}{g(\theta)}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The algorithm goes below, for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\dots,N\)&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Step 1: generate &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(i)}\sim g(\theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 2: accept &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(i)}\)&lt;/span&gt; with probability &lt;span class=&#34;math inline&#34;&gt;\(r(\theta^{(i)})=\frac{f(\theta^{(i)})}{Kg(\theta^{(i)})}\)&lt;/span&gt;. Else go to Step 1.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To fit into Bayesian inference, let &lt;span class=&#34;math inline&#34;&gt;\(f(\theta)=\pi(\theta)p(y_{obs}|\theta)\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the proposal &lt;span class=&#34;math inline&#34;&gt;\(g(\theta)=\pi(\theta)\)&lt;/span&gt;, the acceptance probability is the likelihood &lt;span class=&#34;math inline&#34;&gt;\(r(\theta)=p(y_{obs}|\theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;generally, the acceptance probablity is
&lt;span class=&#34;math display&#34;&gt;\[r(\theta)=\frac{\pi(\theta)}{Kg(\theta)}p(y_{obs}|\theta),\ K=\sup_{\theta}\frac{\pi(\theta)}{g(\theta)}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;importance-sampling-is&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Importance sampling (IS)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the target density &lt;span class=&#34;math inline&#34;&gt;\(f(\theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the proposal &lt;span class=&#34;math inline&#34;&gt;\(g(\theta)\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(g(\theta)&amp;gt;0\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(f(\theta)&amp;gt;0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The algorithm goes below, for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\dots,N\)&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Step 1: generate &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(i)}\sim g(\theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 2: compute the weight &lt;span class=&#34;math inline&#34;&gt;\(w(\theta^{(i)})=\frac{f(\theta^{(i)})}{g(\theta^{(i)})}\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Return the weight samples &lt;span class=&#34;math inline&#34;&gt;\((w(\theta^{(i)}),\theta^{(i)}), i=1,\dots,N\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To fit into Bayesian inference, let &lt;span class=&#34;math inline&#34;&gt;\(f(\theta)=\pi(\theta)p(y_{obs}|\theta)\)&lt;/span&gt;, then the self-normalized IS estimator is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[E[a(\theta)|y_{obs}]\approx \frac{\frac 1 N\sum_{i=1}^N w(\theta^{(i)})a(\theta^{(i)})}{\frac 1 N\sum_{i=1}^N w(\theta^{(i)})}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mcmc&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;MCMC&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the target density &lt;span class=&#34;math inline&#34;&gt;\(f(\theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the proposal distribution: &lt;span class=&#34;math inline&#34;&gt;\(q(\theta&amp;#39;|\theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The algorithm goes below:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;for &lt;span class=&#34;math inline&#34;&gt;\(i=0\)&lt;/span&gt;, draw a starting poing &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(0)}\sim f_0(x)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\dots,N\)&lt;/span&gt;, sample &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(i)}\sim q(\theta&amp;#39;|\theta^{(i-1)})\)&lt;/span&gt;, accept &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(i)}\)&lt;/span&gt; with probability
&lt;span class=&#34;math display&#34;&gt;\[r(\theta^{(i-1)},\theta^{(i)})=\min\left(\frac{f(\theta^{(i)})q(\theta^{(i-1)}|\theta^{(i)})}{f(\theta^{(i-1)})q(\theta^{(i)}|\theta^{(i-1)})},1\right)\]&lt;/span&gt;
Otherwise, taking &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(i)}=\theta^{(i-1)}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To fit into Bayesian inference, let &lt;span class=&#34;math inline&#34;&gt;\(f(\theta)=\pi(\theta)p(y_{obs}|\theta)\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the acceptance probablity is
&lt;span class=&#34;math display&#34;&gt;\[r(\theta,\theta&amp;#39;)=\min\left(\frac{p(y_{obs}|\theta&amp;#39;)q(\theta|\theta&amp;#39;)}{p(y_{obs}|\theta)q(\theta&amp;#39;|\theta)},1\right)\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;motivation-for-abc&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Motivation for ABC&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Both the AR and MCMC require the evaluation of likelihood function.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;However, for an increasing range of scientific problems, numerical evaluation of the likelihood function is
either &lt;strong&gt;computationally prohibitive&lt;/strong&gt;, or simply &lt;strong&gt;not possible&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Instances when the complete likelihood function is
unavailable can occur when the model density function is only implicitly
defined, for example, through quantile or characteristic functions&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;a-g-and-k-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A g-and-k distribution&lt;/h2&gt;
&lt;p&gt;The univariate g-and-k distribution is a flexible unimodal distribution that
is able to describe data with significant amounts of skewness and kurtosis. Its density function has no closed form, but
is alternatively defined through its quantile function as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Q(q|A,B,g,k)=A+B\left[1+c\frac{1-\exp\{-gz(q)\}}{1+\exp\{-gz(q)\}}\right](1+z(q)^2)^kz(q)\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(c=0.8,\ B&amp;gt;0, k&amp;gt;-1/2\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(z(q)=\Phi^{-1}(q)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;if &lt;span class=&#34;math inline&#34;&gt;\(g=k=0\)&lt;/span&gt;, it is the normal density&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;likelihood-free-rejection-sampling-i&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Likelihood-free rejection sampling I&lt;/h2&gt;
&lt;p&gt;While direct evaluation of the
acceptance probability is not available if the likelihood is computationally intractable,
it is possible to stochastically determine whether or not to accept
or reject a draw from the sampling density, &lt;em&gt;without&lt;/em&gt; numerical evaluation of
the acceptance probability.&lt;/p&gt;
&lt;p&gt;Assume that the data &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; are discrete. The algorithm goes below, for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\dots,N\)&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Step 1: generate &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(i)}\sim g(\theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 2: generate &lt;span class=&#34;math inline&#34;&gt;\(y\sim p(y|\theta^{(i)})\)&lt;/span&gt; from the likelihood&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 3: if &lt;span class=&#34;math inline&#34;&gt;\(y=y_{obs}\)&lt;/span&gt;, then accept &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(i)}\)&lt;/span&gt; with probability &lt;span class=&#34;math inline&#34;&gt;\(r(\theta^{(i)})=\frac{\pi(\theta^{(i)})}{Kg(\theta^{(i)})}\)&lt;/span&gt;. Else go to Step 1.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;stereological-extremes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Stereological extremes&lt;/h2&gt;
&lt;p&gt;Interest is in the distribution of the size of inclusions, microscopically small
particles introduced during the production of steel. The steel strength is
thought to be directly related to the size of the largest inclusion. Commonly,
the sampling of inclusions involves measuring the maximum cross-sectional diameter
of each observed inclusion, &lt;span class=&#34;math inline&#34;&gt;\(y_{obs}=(y_{obs,1},\dots,y_{obs,n})\)&lt;/span&gt;, obtained from a
two-dimensional planar slice through the steel block.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;unobserved true inclusion diameter &lt;span class=&#34;math inline&#34;&gt;\(V_i\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(y_{obs,i}\le V_i\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;inclusions were &lt;strong&gt;spherical&lt;/strong&gt; with diameters &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; follows a generalised Pareto distribution
&lt;span class=&#34;math display&#34;&gt;\[P(V\le v|V&amp;gt;v_0)=1-\max\{1+\xi(v-v_0)/\sigma,0\}^{-1/\xi}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the centres followed a homogeneous Poisson process
with rate &lt;span class=&#34;math inline&#34;&gt;\(\lambda&amp;gt;0\)&lt;/span&gt; in the volume of steel&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the parameter is &lt;span class=&#34;math inline&#34;&gt;\(\theta=(\lambda,\sigma,\xi)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;stereological-extremes-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Stereological extremes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Anderson and Coles (2002) were able to construct a tractable likelihood
function for this model. However, while their model assumptions of a Poisson
process are not unreasonable, the assumption that the inclusions are spherical
is not plausible in practice.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Bortot et al. (2007) generalised this model to a family of &lt;strong&gt;ellipsoidal
inclusions&lt;/strong&gt;. While this model is more realistic than the spherical inclusion
model, there are &lt;strong&gt;analytic and computational difficulties&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Analysis&lt;/h2&gt;
&lt;p&gt;Consider the simple case &lt;span class=&#34;math inline&#34;&gt;\(\sigma=1.5,\xi = 0.1\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; unknown. Let &lt;span class=&#34;math inline&#34;&gt;\(n_{obs}\)&lt;/span&gt; be the observed
number of inclusions, so that &lt;span class=&#34;math inline&#34;&gt;\(p(\theta|y_{obs})=p(\lambda|n_{obs})\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;prior density &lt;span class=&#34;math inline&#34;&gt;\(\pi(\lambda)\sim U(0,100)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the data &lt;span class=&#34;math inline&#34;&gt;\(n_{obs}\in \{92,102,112,122,132\}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Acceptance rates for the right panel are &lt;span class=&#34;math inline&#34;&gt;\(0.5\%,10.5\%,20.5\%\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;extremes.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;likelihood-free-rejection-sampling-ii&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Likelihood-free rejection sampling II&lt;/h2&gt;
&lt;p&gt;For a given &lt;span class=&#34;math inline&#34;&gt;\(h&amp;gt;0\)&lt;/span&gt;, the algorithm goes below, for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\dots,N\)&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Step 1: generate &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(i)}\sim g(\theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 2: generate &lt;span class=&#34;math inline&#34;&gt;\(y\sim p(y|\theta^{(i)})\)&lt;/span&gt; from the likelihood&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 3: if &lt;span class=&#34;math inline&#34;&gt;\(||y- y_{obs}||\le h\)&lt;/span&gt;, then accept &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(i)}\)&lt;/span&gt; with probability &lt;span class=&#34;math inline&#34;&gt;\(r(\theta^{(i)})=\frac{\pi(\theta^{(i)})}{Kg(\theta^{(i)})}\)&lt;/span&gt;. Else go to Step 1.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-g-and-k-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The g-and-k distribution&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(y_{obs}\)&lt;/span&gt; of length &lt;span class=&#34;math inline&#34;&gt;\(1000\)&lt;/span&gt; is generated from the &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt;-and-&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; distribution with parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta_0=(3,1,2,0.5)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;prior density&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\pi(\theta) = \pi(A)\pi(B)\pi(g)\pi(k) = N(1,5)\times N(0.25,2) \times U(0,10) \times U(0,1)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The results&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;gandk.png&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;grey dots: for the AR rule &lt;span class=&#34;math inline&#34;&gt;\(||y-y_{obs}||\le h\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;black dots: for the AR rule &lt;span class=&#34;math inline&#34;&gt;\(||S(y)-S(y_{obs})||\le h\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-use-of-summary-statistic&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The use of summary statistic&lt;/h2&gt;
&lt;p&gt;Summary statistic (Drovandi and Pettitt, 2011): &lt;span class=&#34;math inline&#34;&gt;\(S(y) = (S_A,S_B,S_g,S_k)\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(S_A=E_4\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(S_B=E_6-E_2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(S_g=(E_6+E_2-2E_4)/S_B\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(S_k = (E_7-E_5+E_3-E_1)/S_B\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(E_1\le E_2 \le \cdots \le E_8\)&lt;/span&gt; are the octiles of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;likelihood-free-rejection-sampling-iii&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Likelihood-free rejection sampling III&lt;/h2&gt;
&lt;p&gt;For a given &lt;span class=&#34;math inline&#34;&gt;\(h&amp;gt;0\)&lt;/span&gt;, the algorithm goes below, for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\dots,N\)&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Step 1: generate &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(i)}\sim g(\theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 2: generate &lt;span class=&#34;math inline&#34;&gt;\(y\sim p(y|\theta^{(i)})\)&lt;/span&gt; from the likelihood&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 3: if &lt;span class=&#34;math inline&#34;&gt;\(||S(y)- S(y_{obs})||\le h\)&lt;/span&gt;, then accept &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(i)}\)&lt;/span&gt; with probability &lt;span class=&#34;math inline&#34;&gt;\(r(\theta^{(i)})=\frac{\pi(\theta^{(i)})}{Kg(\theta^{(i)})}\)&lt;/span&gt;. Else go to Step 1.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;NOTE: usually &lt;span class=&#34;math inline&#34;&gt;\(\text{dim}(S(y))\ll\text{dim}(y)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;approximate-bayesian-computation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Approximate Bayesian Computation&lt;/h2&gt;
&lt;p&gt;ABC is a kind of likelihood-free methods developed
for when the likelihood function is computationally intractable or otherwise unavailable.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The likelihood-free rejection algorithm with &lt;span class=&#34;math inline&#34;&gt;\(h=0\)&lt;/span&gt; which is exact (work for discrete cases), is not an ABC algorithm&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For &lt;span class=&#34;math inline&#34;&gt;\(h&amp;gt;0\)&lt;/span&gt;, it is an ABC algorithm as the samples will be drawn from an approximation to the posterior distribution.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The aim of any ABC analysis is to find a practical way of
performing the Bayesian analysis, while keeping the approximation and the
computation to a minimum.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;abc-with-general-kernels&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ABC with general kernels&lt;/h2&gt;
&lt;p&gt;The approximate joint distribution:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\pi_{ABC}(\theta,y|y_{obs})\propto I\{||y-y_{obs}||\le h\}p(y|\theta)\pi(\theta)\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;generalize the AR rule &lt;span class=&#34;math inline&#34;&gt;\(I\{||y-y_{obs}||\le h\}\)&lt;/span&gt; to smoothing kernel function &lt;span class=&#34;math inline&#34;&gt;\(K_h(||y-y_{obs}||)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\pi_{ABC}(\theta,y|y_{obs})\propto K_h(||y-y_{obs}||)p(y|\theta)\pi(\theta)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The kernel: &lt;span class=&#34;math inline&#34;&gt;\(K_h(u)=\frac 1 h K(u/h)\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(K(u)\)&lt;/span&gt; is a symmetric function&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(K(u)\ge 0\)&lt;/span&gt; for all &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\int K(u) d u=1,\ \int uK(u)du=0,\ \int u^2K(u)du&amp;lt;\infty\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt; is the bandwidth&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;common-kernels&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Common kernels&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Uniform &lt;span class=&#34;math inline&#34;&gt;\(\frac 1 2 I\{|u|\le 1\}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Tringular &lt;span class=&#34;math inline&#34;&gt;\((1-|u|)I\{|u|\le 1\}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Epanechnikov &lt;span class=&#34;math inline&#34;&gt;\(\frac 3 4(1-u^2)I\{|u|\le 1\}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Biweight &lt;span class=&#34;math inline&#34;&gt;\(\frac{15}{16}(1-u^2)^3I\{|u|\le 1\}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Gaussian &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{\sqrt{2\pi}}e^{-u^2/2}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;kernels.png&#34; width=&#34;60%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;abc-rejection-sampling-algorithm-i&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ABC Rejection Sampling Algorithm I&lt;/h2&gt;
&lt;p&gt;For a given &lt;span class=&#34;math inline&#34;&gt;\(h&amp;gt;0\)&lt;/span&gt;, the algorithm goes below, for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\dots,N\)&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Step 1: generate &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(i)}\sim g(\theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 2: generate &lt;span class=&#34;math inline&#34;&gt;\(y\sim p(y|\theta^{(i)})\)&lt;/span&gt; from the likelihood&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 3: accept &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(i)}\)&lt;/span&gt; with probability &lt;span class=&#34;math inline&#34;&gt;\(r(\theta^{(i)})=\frac{K_h(||y-y_{obs}||)\pi(\theta^{(i)})}{Kg(\theta^{(i)})}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(K\ge \sup_{\theta}\frac{\pi(\theta)}{g(\theta)}\sup_uK(u)\)&lt;/span&gt;. Else go to Step 1.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;approximate-posterior-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Approximate Posterior Distribution&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\pi_{ABC}(\theta,y|y_{obs})\propto K_h(||y-y_{obs}||)p(y|\theta)\pi(\theta)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The approximate posterior:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\pi_{ABC}(\theta|y_{obs})=\int \pi_{ABC}(\theta,y|y_{obs}) d y\propto \pi(\theta)\int K_h(||y-y_{obs}||)p(y|\theta)dy\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ABC approximation to the true likelihood:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p_{ABC}(y_{obs}|\theta) = \int K_h(||y-y_{obs}||)p(y|\theta)dy.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\pi_{ABC}(\theta|y_{obs})\to \pi(\theta|y_{obs}) \text{ as }h\to 0.\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 1&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;likelihood function &lt;span class=&#34;math inline&#34;&gt;\(p(y|\theta)=\theta e^{-\theta y}\)&lt;/span&gt;, i.e., &lt;span class=&#34;math inline&#34;&gt;\(Exp(\theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;prior &lt;span class=&#34;math inline&#34;&gt;\(\pi(\theta) \propto \theta^{\alpha-1}e^{-\beta\theta}\)&lt;/span&gt;, i.e., &lt;span class=&#34;math inline&#34;&gt;\(Gamma(\alpha,\beta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;posterior density &lt;span class=&#34;math inline&#34;&gt;\(Gamma(\alpha+1,\beta+y_{obs})\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;uniform kernel &lt;span class=&#34;math inline&#34;&gt;\(K_h(u) = \frac 1{2h}1\{|u|\le h\}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p_{ABC}(y_{obs}|\theta) = \int K_h(||y-y_{obs}||)p(y|\theta)dy=\frac 1{2h}e^{-\theta y_{obs}}(e^{\theta h}-e^{-\theta h})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p_{ABC}(y_{obs}|\theta)-p(y_{obs}|\theta)=p(y_{obs}|\theta)\left(\frac{e^{\theta h}-e^{-\theta h}}{2\theta h}-1\right)\approx \frac{h^2\theta^3e^{-\theta y_{obs}}}{6}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\pi_{ABC}(\theta|y_{obs}) = \frac{\theta^{\alpha-1}e^{-\theta (y_{obs}+\beta)}(e^{\theta h}-e^{-\theta h})}{\frac{\Gamma(\alpha)}{(y_{obs}+\beta-h)^\alpha}-\frac{\Gamma(\alpha)}{(y_{obs}+\beta+h)^\alpha}}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plots&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;ex1.png&#34; width=&#34;80%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 2&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;likelihood &lt;span class=&#34;math inline&#34;&gt;\(N(\theta,\sigma_0^2)\)&lt;/span&gt; with known &lt;span class=&#34;math inline&#34;&gt;\(\sigma_0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;prior &lt;span class=&#34;math inline&#34;&gt;\(N(m_0,s_0^2)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Guassian kernel &lt;span class=&#34;math inline&#34;&gt;\(K_h(u)=N(0,h^2)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(p_{ABC}(\bar y_{obs}|\theta)=N(\theta,\sigma^2/n+h^2)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\pi_{ABC}(\theta|y_{obs})=N(\mu_{ABC},\sigma_{ABC}^2)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mu_{ABC} = \frac{\frac{m_0}{s_0^2}+\frac{\bar y_{obs}}{\sigma_0^2/n+h^2}}{\frac{1}{s_0^2}+\frac{1}{\sigma_0^2/n+h^2}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{1}{\sigma_{ABC}^2}=\frac{1}{s_0^2}+\frac{1}{\sigma_0^2/n+h^2}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plots-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plots&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;ex2.png&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Gaussian kernel vs. uniform kernel&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The ABC posterior approximation may
be improved simply by rescaling the posterior variance to remove the term &lt;span class=&#34;math inline&#34;&gt;\(h^2\)&lt;/span&gt; (Drovandi 2012).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-use-of-summary-statistics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Use of Summary Statistics&lt;/h2&gt;
&lt;p&gt;For a given &lt;span class=&#34;math inline&#34;&gt;\(h&amp;gt;0\)&lt;/span&gt;, the algorithm goes below, for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\dots,N\)&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Step 1: generate &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(i)}\sim g(\theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 2: generate &lt;span class=&#34;math inline&#34;&gt;\(y\sim p(y|\theta^{(i)})\)&lt;/span&gt; from the likelihood&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 3: accept &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(i)}\)&lt;/span&gt; with probability &lt;span class=&#34;math inline&#34;&gt;\(r(\theta^{(i)})=\frac{K_h(||S(y)-S(y_{obs})||)\pi(\theta^{(i)})}{Kg(\theta^{(i)})}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(K\ge \sup_{\theta}\frac{\pi(\theta)}{g(\theta)}\sup_uK(u)\)&lt;/span&gt;. Else go to Step 1.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;example-3&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 3&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the model &lt;span class=&#34;math inline&#34;&gt;\(y=(y_1,y_2), y_i\sim B(n,\theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;prior &lt;span class=&#34;math inline&#34;&gt;\(\theta\sim U(0,1)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;three sufficient statistics: &lt;span class=&#34;math inline&#34;&gt;\(s^1=(y_1,y_2)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(s^2 =(y_{(1)},y_{(2)})\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(s^3=y_1+y_2\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Acceptance rates (&lt;span class=&#34;math inline&#34;&gt;\(h=0\)&lt;/span&gt;) are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(r_1=C_n^{y_1}C_n^{y_2} B(y_1+y_2+1,2n-y_1-y_2+1)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(r_2 = [2-1\{y_{(1)}=y_{(2)}\}]r_1\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(r_3 = 1/(2n+1)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;E.g., &lt;span class=&#34;math inline&#34;&gt;\(y_{obs}=(1,2)\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\(n=5\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(p_1\approx 0.038,\ p_2\approx 0.076,\ p_3\approx 0.091\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The most efficient choice is the &lt;strong&gt;minimal sufficient statistic&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This may still be non-viable in practice; low-dimensional sufficient statistic may not exist – Weibull distribution&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;example-2-continued&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 2 (continued)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;likelihood &lt;span class=&#34;math inline&#34;&gt;\(N(\theta,\sigma_0^2)\)&lt;/span&gt; with known &lt;span class=&#34;math inline&#34;&gt;\(\sigma_0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;prior &lt;span class=&#34;math inline&#34;&gt;\(N(m_0,s_0^2)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Guassian kernel &lt;span class=&#34;math inline&#34;&gt;\(K_h(u)=N(0,h^2)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;summary statistic &lt;span class=&#34;math inline&#34;&gt;\(s=\bar{y}_{1{:}n&amp;#39;}=\frac{1}{n&amp;#39;}\sum_{i=1}^{n&amp;#39;}y_i\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(n&amp;#39;&amp;lt;n\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(p_{ABC}(s_{obs}|\theta)=N(\theta,\sigma^2/n&amp;#39;+h^2)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\pi_{ABC}(\theta|y_{obs})=N(\mu_{ABC},\sigma_{ABC}^2)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mu_{ABC} = \frac{\frac{m_0}{s_0^2}+\frac{s_{obs}}{\sigma_0^2/n&amp;#39;+h^2}}{\frac{1}{s_0^2}+\frac{1}{\sigma_0^2/n&amp;#39;+h^2}},\ \frac{1}{\sigma_{ABC}^2}=\frac{1}{s_0^2}+\frac{1}{\sigma_0^2/n&amp;#39;+h^2}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;two sources of error: the degree
of inefficiency of replacing &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(s=S(y)\)&lt;/span&gt; and the matching of the
simulated and observed data through the Gaussian kernel&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-choice-of-summary-statistics-for-abc&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The choice of summary statistics for ABC&lt;/h2&gt;
&lt;p&gt;The choice of summary statistics for an ABC analysis is a critical decision
that directly affects the quality of the posterior approximation.&lt;/p&gt;
&lt;p&gt;Many
approaches for determining these statistics are available, and these are reviewed
in Blum et al. (2013) and Prangle (2019). These methods seek to trade off two aspects of the ABC posterior
approximation that directly result from the choice of summary statistics.&lt;/p&gt;
&lt;p&gt;The dimension of the summary statistic should be large enough so
that it contains as much information about the observed data as possible, but
also low enough so that the curse-of-dimensionality of matching &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(s_{obs}\)&lt;/span&gt;
is avoided.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-practical-issues-with-summary-statistics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Some practical issues with summary statistics&lt;/h2&gt;
&lt;p&gt;It is NOT always viable to continue to add summary statistics to s until the
resulting ABC posterior approximation does not change for the worse.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the model &lt;span class=&#34;math inline&#34;&gt;\(y=(y_1,\dots,y_n), y_i\sim Poission(\lambda)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;prior &lt;span class=&#34;math inline&#34;&gt;\(\lambda \sim Gamma(\alpha,\beta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;posterior &lt;span class=&#34;math inline&#34;&gt;\(\lambda|y\sim Gamma(\alpha+n\bar y,\beta+n)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;data &lt;span class=&#34;math inline&#34;&gt;\(y_{obs}=(0,0,0,0,5)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;summary statistics: &lt;span class=&#34;math inline&#34;&gt;\(s^1=\bar y,\ s^2= v=\sqrt{\frac{1}{n-1}\sum_{i=1}^n(y_i-\bar y)^2}, s^3=(\bar y,v)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;using uniform kernel with &lt;span class=&#34;math inline&#34;&gt;\(h=0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(h=0.3\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\alpha=\beta=1\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;comparison&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comparison&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;ss.png&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;top (&lt;span class=&#34;math inline&#34;&gt;\(h=0\)&lt;/span&gt;), bottom (&lt;span class=&#34;math inline&#34;&gt;\(h=0.3\)&lt;/span&gt;)&lt;/li&gt;
&lt;li&gt;prior &lt;span class=&#34;math inline&#34;&gt;\(Gamma(\alpha,\beta)\)&lt;/span&gt; (dashed lines), target distribution &lt;span class=&#34;math inline&#34;&gt;\(Gamma(\alpha+n\bar y,\beta+n)\)&lt;/span&gt; (solid line)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-choice-of-distance-measure&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The choice of distance measure&lt;/h2&gt;
&lt;p&gt;The distance measure &lt;span class=&#34;math inline&#34;&gt;\(||s-s_{obs}||\)&lt;/span&gt; can also have a substantial impact on ABC algorithm efficiency, and therefore the quality of the posterior approximation.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the most common one is the Euclidean distance&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the Mahalanobis distance (Peters
et al. 2012; Erhardt and Sisson 2016):&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[||s-s_{obs}||=(s-s_{obs})^\top\Sigma^{-1}(s-s_{obs})\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; is the covariance matrix of &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;comparison-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comparison&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;distance.png&#34; width=&#34;60%&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the entries of &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; may dependent&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;implementing a circular acceptance region (implying independence
and identical scales) induces both type I and type II errors.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;example-4&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 4&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the model &lt;span class=&#34;math inline&#34;&gt;\(y_1,\dots,y_{50}\sim N(\theta,1)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;prior &lt;span class=&#34;math inline&#34;&gt;\(\theta\sim U(-5,5)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;summary statistics: &lt;span class=&#34;math inline&#34;&gt;\(s^1=(\bar y_{1{:}40},\bar y_{41{:}50})\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(s^2=(\bar y_{1{:}25}-\bar y_{26{:}50},\bar y_{26{:}50})\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\Sigma^1 = Cov(s^1|\theta) = \left(
\begin{matrix}
1/40 &amp;amp; 0\\
0 &amp;amp; 1/10
\end{matrix}
\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\Sigma^2 = Cov(s^2|\theta) = \left(
\begin{matrix}
2/25 &amp;amp; -1/25\\
-1/25 &amp;amp; 1/25
\end{matrix}
\right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulation&lt;/h2&gt;
&lt;p&gt;Mean Number of Summary StatisticGenerations per Final Accepted Particle (with Standard Errors inParentheses)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;tabdistance.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pilot-analysis-on-the-covariance-matrix&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pilot analysis on the covariance matrix&lt;/h2&gt;
&lt;p&gt;To estimate the covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;, Luciani
et al. (2009) and Erhardt and Sisson (2016) identify some value of &lt;span class=&#34;math inline&#34;&gt;\(\theta=\theta^*\)&lt;/span&gt; in a high posterior density region via a pilot analysis and then estimate
&lt;span class=&#34;math inline&#34;&gt;\(Cov(s|\theta^*)\)&lt;/span&gt; based on repeated draws from &lt;span class=&#34;math inline&#34;&gt;\(p(s|\theta^*)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;abc-is-algorithm&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ABC-IS algorithm&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the target density &lt;span class=&#34;math inline&#34;&gt;\(\pi_{ABC}(\theta|s_{obs})\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the proposal &lt;span class=&#34;math inline&#34;&gt;\(g(\theta)\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(g(\theta)&amp;gt;0\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(f(\theta)&amp;gt;0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For a given &lt;span class=&#34;math inline&#34;&gt;\(h&amp;gt;0\)&lt;/span&gt;, the algorithm goes below, for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\dots,N\)&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Step 1: generate &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(i)}\sim g(\theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 2: generate &lt;span class=&#34;math inline&#34;&gt;\(y\sim p(y|\theta^{(i)})\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 3: compute the weight &lt;span class=&#34;math display&#34;&gt;\[w(\theta^{(i)})=\frac{K_h(||S(y)-S(y_{obs})||)\pi(\theta^{(i)})}{g(\theta^{(i)})}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Return the weight samples &lt;span class=&#34;math inline&#34;&gt;\((w(\theta^{(i)}),\theta^{(i)}), i=1,\dots,N\)&lt;/span&gt;. The self-normalized IS estimator is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[E[a(\theta)|y_{obs}]\approx \frac{\frac 1 N\sum_{i=1}^N w(\theta^{(i)})a(\theta^{(i)})}{\frac 1 N\sum_{i=1}^N w(\theta^{(i)})}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;All models are approximations to the real data-generation process.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;use of summary statistics &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;, the effect of &lt;span class=&#34;math inline&#34;&gt;\(\text{dim}(s)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;use of kernel appriximation &lt;span class=&#34;math inline&#34;&gt;\(K_h\)&lt;/span&gt;, the effect of bandwidth &lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;use of distance measure, the effect of covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Monte Carlo error, the effect of the sample size &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Quasi-Monte Carlo in ABC</title>
      <link>/post/qmc_abc/</link>
      <pubDate>Sun, 18 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/qmc_abc/</guid>
      <description>&lt;div id=&#34;ingredients-for-abc&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ingredients for ABC&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;summary statistic &lt;span class=&#34;math inline&#34;&gt;\(S(y):\mathbb{R}^n\to \mathbb{R}^d\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;kernel function &lt;span class=&#34;math inline&#34;&gt;\(k(u_1,\dots,u_d)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;bandwidth &lt;span class=&#34;math inline&#34;&gt;\(h&amp;gt;0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;proposal density &lt;span class=&#34;math inline&#34;&gt;\(g(\theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ABC approximation&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\pi_{ABC}(\theta|s_{obs})\propto \pi(\theta)\int \pi(s|\theta)K((s-s_{obs})/h) d s\to \pi(\theta|s_{obs})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(\pi(\theta|s_{obs})\approx \pi(\theta|y)\)&lt;/span&gt;, then ABC density is a proper approximation of the posterior &lt;span class=&#34;math inline&#34;&gt;\(\pi(\theta|y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;abc-convergence-rates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ABC convergence rates&lt;/h2&gt;
&lt;p&gt;Consider the estimation of &lt;span class=&#34;math inline&#34;&gt;\(\mu=E[a(\theta)|s_{obs}]\)&lt;/span&gt;. The acceptance-rejection (AR) based ABC estimate is given by
&lt;span class=&#34;math display&#34;&gt;\[\hat\mu=\frac{1}{N}\sum_{i=1}^Na(\theta^{(i)}).\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Under regular conditions, ABC bias is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mathrm{bias}=|E_{ABC}[a(\theta)|s_{obs}]-\mu|=O(h^2),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and the acceptance probability is &lt;span class=&#34;math inline&#34;&gt;\(R = O(h^{d}),\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and Monte Carlo variance is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mathrm{variance}=\frac{\sigma^2_{ABC}}{N}=O\left(\frac{1}{C h^d}\right),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; is the complexity. Overall, the MSE is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mathrm{MSE} = \mathrm{bias}^2+\mathrm{variance} = O(h^4)+O\left(\frac{1}{Ch^d}\right).\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For a given &lt;span class=&#34;math inline&#34;&gt;\(C&amp;gt;0\)&lt;/span&gt;,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the optimal &lt;span class=&#34;math inline&#34;&gt;\(h^*=O(C^{-1/(4+d)})\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the optimal &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{MSE}^*=O(C^{-4/(d+4)})\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This reveals that ABC suffers from the &lt;strong&gt;curse of dimensionality&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;improving-the-sampling-efficiency&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Improving the sampling efficiency&lt;/h2&gt;
&lt;p&gt;A possible way to improve ABC efficiency is to accelerate the Monte Carlo. Monte Carlo error is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{MC error}=\frac{\sigma}{\sqrt{N}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Some variance reduction techniques are proposed to reduce &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;, such as&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;importance sampling&lt;/li&gt;
&lt;li&gt;antithetic variates&lt;/li&gt;
&lt;li&gt;control variates&lt;/li&gt;
&lt;li&gt;hybrid strategies&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;On the other hand, quasi-Monte Carlo is used to &lt;strong&gt;improve the rate of convergence&lt;/strong&gt; (&lt;span class=&#34;math inline&#34;&gt;\(1/\sqrt{N}\)&lt;/span&gt;) rather than the constant &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;quasi-monte-carlo-a-review&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Quasi-Monte Carlo: A review&lt;/h2&gt;
&lt;p&gt;As a start-up setting, let’s consider an intergal over the unit cube &lt;span class=&#34;math inline&#34;&gt;\([0,1]^d\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mu=\int_{[0,1]^d} f(u_1,\dots,u_d)du_1\cdots du_d.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;MC estimate is the average of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; iid samples:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat\mu_N = \frac 1 N\sum_{i=1}^N f(u^{(i)}),\ u^{(i)}\stackrel{iid}{\sim} U[0,1]^d.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;QMC estimate has the same form but uses deterministic sequences&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat\mu_N = \frac 1 N\sum_{i=1}^N f(u^{(i)}),\ u^{(i)}\in[0,1]^d.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The sequences are clearly constructed with better uniformness, which are known as &lt;strong&gt;low discrepancy sequences (LDSs)&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Halton (1960)&lt;/li&gt;
&lt;li&gt;Sobol’ (1967)&lt;/li&gt;
&lt;li&gt;Faure (1982)&lt;/li&gt;
&lt;li&gt;Niderreiter (1992)&lt;/li&gt;
&lt;li&gt;Lattice rules&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Koksma-Hlawka inequality&lt;/strong&gt; gives&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
|\hat\mu_N-\mu|\le V_{\mathrm{HK}}(f)D^*(u^{(1)},...,u^{(N)})
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(V_{\mathrm{HK}}(f)\)&lt;/span&gt; is the variation in the sense of Hardy and Krause&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(D^*\)&lt;/span&gt; is the star discrepancy of the points&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(V_{\mathrm{HK}}(f)&amp;lt;\infty\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\{u^{(1)},...,u^{(N)}\}\)&lt;/span&gt; is a LDS, then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{QMC error}=O(N^{-1}(\log N)^d)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;QMC can achieve higher-order rate of convergence, but needs higher smoothness conditions; see Dick (Ann. Stat., 2011).&lt;/p&gt;
&lt;p&gt;Some mathematical softwares such as &lt;code&gt;Matlab&lt;/code&gt;, &lt;code&gt;R&lt;/code&gt; include some common generators of LDSs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#  install.packages(&amp;quot;randtoolbox&amp;quot;)
set.seed(7)
library(&amp;quot;randtoolbox&amp;quot;)
par(mfrow=c(2,2))
qmc = sobol(1024,2)
plot(qmc[1:256,],pch=19,xlab=&amp;quot;x&amp;quot;,ylab=&amp;quot;y&amp;quot;,main=&amp;quot;Sobol&amp;#39; points: N = 256&amp;quot;)
plot(qmc,pch=19,xlab=&amp;quot;x&amp;quot;,ylab=&amp;quot;y&amp;quot;,main=&amp;quot;Sobol&amp;#39; points: N = 1024&amp;quot;)
mc = matrix(runif(2048),ncol = 2)
plot(mc[1:256,],pch=19,xlab=&amp;quot;x&amp;quot;,ylab=&amp;quot;y&amp;quot;,main=&amp;quot;MC points: N = 256&amp;quot;)
plot(mc,pch=19,xlab=&amp;quot;x&amp;quot;,ylab=&amp;quot;y&amp;quot;,main=&amp;quot;MC points: N = 1024&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/QMC_ABC_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 1&lt;/h2&gt;
&lt;p&gt;Consider the integral:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mu = \int_{[0,1]^d} \sum_{i=1}^d x_i^2 dx =\frac{d}{3}.\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;myfun = function(x){
  #rowSums(x^2)
  apply(x-0.5,1,prod)
}

m = 16
N = 2^m
d = 8
#trueval = d/3
trueval = 0
qmc = as.matrix(sobol(n=N,dim=d),ncol=d)
fsum = cumsum(myfun(qmc))
ns = 1:N
fmean = fsum[ns]/ns
par(mar=c(4,4,2,1),mfrow=c(2,1))
tt = paste0(&amp;quot;QMC: d = &amp;quot;,d)
plot(ns,fmean,xlab=&amp;quot;N&amp;quot;,ylab=&amp;quot;Mean&amp;quot;,typ=&amp;quot;l&amp;quot;,main=tt)
abline(h=trueval,lty=5,col=&amp;quot;red&amp;quot;)
ns = 2^(0:m)
fmean = fsum[ns]/ns
err = abs(fmean-trueval)
plot(ns,err,xlab=&amp;quot;N&amp;quot;,ylab=&amp;quot;Error&amp;quot;,typ=&amp;quot;b&amp;quot;,log=&amp;quot;xy&amp;quot;,main=tt)
r = 1
lines(ns[c(3,m)], c(err[3],err[3]*(ns[3]/ns[m])^r),col=&amp;quot;red&amp;quot;,lty=5)
legend(500,err[2],legend = c(&amp;quot;QMC errors&amp;quot;,paste0(&amp;quot;N^{&amp;quot;,-r,&amp;quot;}&amp;quot;)),lty = c(1,5),
       col=c(&amp;quot;black&amp;quot;,&amp;quot;red&amp;quot;),pch=c(1,NA),cex=1.2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/QMC_ABC_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;randomized-qmc&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Randomized QMC&lt;/h2&gt;
&lt;p&gt;In practice, we use randomized QMC (RQMC), which yields an unbiased estimate.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;random-shift, see Cranley and Patterson (1976)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;scrambled nets, see Owen (1995,1997,1998)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Survey in L’Ecuyer and Lemieux (2005)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(7)
library(&amp;quot;randtoolbox&amp;quot;)
par(mfrow=c(2,2))
qmc = sobol(1024,2)
rqmc = sobol(1024,2,scrambling=1)
plot(qmc[1:256,],pch=19,xlab=&amp;quot;x&amp;quot;,ylab=&amp;quot;y&amp;quot;,main=&amp;quot;Sobol&amp;#39; points: N = 256&amp;quot;)
plot(qmc,pch=19,xlab=&amp;quot;x&amp;quot;,ylab=&amp;quot;y&amp;quot;,main=&amp;quot;Sobol&amp;#39; points: N = 1024&amp;quot;)
mc = matrix(runif(2048),ncol = 2)
plot(rqmc[1:256,],pch=19,xlab=&amp;quot;x&amp;quot;,ylab=&amp;quot;y&amp;quot;,main=&amp;quot;RQMC points: N = 256&amp;quot;)
plot(rqmc,pch=19,xlab=&amp;quot;x&amp;quot;,ylab=&amp;quot;y&amp;quot;,main=&amp;quot;RQMC points: N = 1024&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/QMC_ABC_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;myfun = function(x){
  d = ncol(x)
  #(rowSums(x)&amp;gt;d/2)-0.5
  rowSums(qnorm(x))
  #apply(x-0.5,1,prod)
  
}

m = 16
N = 2^m
d = 8
trueval = 0
R = 100
ns = 2^(0:m)
fmean = matrix(0,m+1,R)
tmp = sobol(N,d)##initialization
for(i in 1:R)
{
  rqmc = as.matrix(sobol(N,d,scrambling=1,init=FALSE),ncol=d)
  fsum = cumsum(myfun(rqmc))
  fmean[,i] = fsum[ns]/ns
}

par(mar=c(4,4,2,1),mfrow=c(2,1))
tt = paste0(&amp;quot;RQMC: d = &amp;quot;,d)
plot(ns,rowMeans(fmean),xlab=&amp;quot;N&amp;quot;,ylab=&amp;quot;Mean&amp;quot;,typ=&amp;quot;b&amp;quot;,main=tt)
abline(h=trueval,lty=5,col=&amp;quot;red&amp;quot;)

rmse = apply(fmean,1,sd)

plot(ns,rmse,xlab=&amp;quot;N&amp;quot;,ylab=&amp;quot;rmse&amp;quot;,typ=&amp;quot;b&amp;quot;,log=&amp;quot;xy&amp;quot;,main=tt)
r = 1
lines(ns[c(3,m)], c(rmse[3],rmse[3]*(ns[3]/ns[m])^r),col=&amp;quot;red&amp;quot;,lty=5)
legend(500,rmse[2],legend = c(&amp;quot;RQMC errors&amp;quot;,paste0(&amp;quot;N^{&amp;quot;,-r,&amp;quot;}&amp;quot;)),lty = c(1,5),
       col=c(&amp;quot;black&amp;quot;,&amp;quot;red&amp;quot;),pch=c(1,NA),cex=1.2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/QMC_ABC_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-qmc-in-abc&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using QMC in ABC&lt;/h2&gt;
&lt;p&gt;The univariate g-and-k distribution is a flexible unimodal distribution that
is able to describe data with significant amounts of skewness and kurtosis. Its density function has no closed form, but
is alternatively defined through its quantile function as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Q(q|A,B,g,k)=A+B\left[1+c\frac{1-\exp\{-gz(q)\}}{1+\exp\{-gz(q)\}}\right](1+z(q)^2)^kz(q)\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(c=0.8,\ B&amp;gt;0, k&amp;gt;-1/2\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(z(q)=\Phi^{-1}(q)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;if &lt;span class=&#34;math inline&#34;&gt;\(g=k=0\)&lt;/span&gt;, it is the normal density&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(y_{obs}\)&lt;/span&gt; of length &lt;span class=&#34;math inline&#34;&gt;\(1000\)&lt;/span&gt; is generated from the &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt;-and-&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; distribution with parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta_0=(3,1,2,0.5)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;prior density&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\pi(\theta) = \pi(A)\pi(B)\pi(g)\pi(k) = N(1,5)\times N(0.25,2) \times U(0,10) \times U(0,1)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Summary statistic (Drovandi and Pettitt, 2011): &lt;span class=&#34;math inline&#34;&gt;\(S(y) = (S_A,S_B,S_g,S_k)\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(S_A=E_4\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(S_B=E_6-E_2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(S_g=(E_6+E_2-2E_4)/S_B\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(S_k = (E_7-E_5+E_3-E_1)/S_B\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(E_1\le E_2 \le \cdots \le E_8\)&lt;/span&gt; are the octiles of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(100)
theta0 = c(3,1,2,0.5)

gkmodel &amp;lt;- function(theta,n,z=NA){
  if(is.na(z)[1]){
    z = rnorm(n)
  }
  y = theta[1] + theta[2]*(1+0.8*(1-exp(-theta[3]*z))/
        (1+exp(-theta[3]*z)))*(1+z^2)^theta[4]*z
  return(matrix(y,n,1))
}
n = 1e3
yobs = gkmodel(theta0,n)
## prior density
gkprior &amp;lt;- function(n,u=NA){
  if(is.na(u)[1]){
    u = matrix(runif(n*4),n,4)
  }
  A = qnorm(u[,1])*sqrt(5)+1
  B = qnorm(u[,2])*sqrt(2)+.25
  g = u[,3]*10
  k = u[,4]
  return(cbind(A,B,g,k))
}
gksummary &amp;lt;- function(y){
  sorty = sort(y)
  n = length(y)
  q = sorty[ceiling(n/8*(1:8))]
  s = c(q[4],
        q[6]-q[2],
        (q[6]+q[2]-2*q[4])/(q[6]-q[2]),
        (q[7]-q[5]+q[3]-q[1])/(q[6]-q[2]))
  return(s)
}
ytmp1 = matrix(0,2000,4)
for(i in 1:2000){
  ytmp1[i,] = gksummary(gkmodel(gkprior(1),n))
}
Sigma = var(ytmp1)
invsig = solve(Sigma)
N = 1e5
ytmp = rep(0,N)
theta = matrix(0,N,4)
stmp = rep(0,N)
sobs = gksummary(yobs)
#qmc = sobol(N+1,n+4)
#qmc = qmc[-1,]##initialization
for(i in 1:N){
  theta[i,] = gkprior(1) # matrix(qmc[i,1:4],1,4)
  ypro = gkmodel(theta[i,],n) # qnorm(matrix(qmc[i,-(1:4)],1,n))
  spro = gksummary(ypro)
  diff = matrix(spro-sobs,1,4)
  ytmp[i] = sqrt(sum((ypro-yobs)^2))
  stmp[i] = sqrt(diff%*%invsig%*%t(diff))
}
ysort = sort(ytmp)
ssort = sort(stmp)
effN = N*5e-3
hy = ysort[effN]
hs = ssort[effN]
## draw pairwise scatterplots
theta1 = theta[ytmp&amp;lt;=hy,]
theta2 = theta[stmp&amp;lt;=hs,]
theta = rbind(theta1,theta2,theta0)
colnames(theta) = c(&amp;quot;A&amp;quot;,&amp;quot;B&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;k&amp;quot;)
pairs(theta,pch=c(rep(20,effN*2),24),
      col=c(rep(&amp;quot;grey&amp;quot;,effN),rep(&amp;quot;black&amp;quot;,effN),&amp;quot;red&amp;quot;),cex=1)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>第四章：线性回归</title>
      <link>/post/chap04/</link>
      <pubDate>Thu, 15 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/chap04/</guid>
      <description>&lt;div id=&#34;simple-linear-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simple linear models&lt;/h2&gt;
&lt;p&gt;The linear model is given by
&lt;span class=&#34;math display&#34;&gt;\[y_i=\beta_0+\beta_1x_i+\epsilon_i,\ i=1,\dots,n.\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\epsilon_i\)&lt;/span&gt; are random (need some assumptions)&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; are &lt;strong&gt;fixed&lt;/strong&gt; (&lt;em&gt;independent/predictor&lt;/em&gt; variable)&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; are random (&lt;em&gt;dependent/response&lt;/em&gt; variable)&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the &lt;em&gt;intercept&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; is the &lt;em&gt;slope&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;least-square-estimators&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Least square estimators&lt;/h3&gt;
&lt;p&gt;Choose &lt;span class=&#34;math inline&#34;&gt;\(\beta_0,\beta_1\)&lt;/span&gt; to minimize
&lt;span class=&#34;math display&#34;&gt;\[Q(\beta_0,\beta_1) = \sum_{i=1}^n(y_i-\beta_0-\beta_1x_i)^2.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The minimizers &lt;span class=&#34;math inline&#34;&gt;\(\hat\beta_0,\hat\beta_1\)&lt;/span&gt; satisfy
&lt;span class=&#34;math display&#34;&gt;\[
\begin{cases}
\frac{\partial Q}{\partial \beta_0} = -2\sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)=0\\
\frac{\partial Q}{\partial \beta_1} = -2\sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)x_i=0
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This gives
&lt;span class=&#34;math display&#34;&gt;\[\hat\beta_1 = \frac{\sum_{i=1}^n(y_i-\bar y)x_i}{\sum_{i=1}^n(x_i-\bar x)x_i},\ \hat\beta_0=\bar y-\hat\beta_1\bar x.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Define &lt;span class=&#34;math display&#34;&gt;\[\ell_{xx} = \sum_{i=1}^n(x_i-\bar x)^2,\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[\ell_{yy} = \sum_{i=1}^n(y_i-\bar y)^2,\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[\ell_{xy} = \sum_{i=1}^n(x_i-\bar x)(y_i-\bar y).\]&lt;/span&gt;
We thus have
&lt;span class=&#34;math display&#34;&gt;\[\hat\beta_1 = \frac{\sum_{i=1}^n(y_i-\bar y)(x_i-\bar x)}{\sum_{i=1}^n(x_i-\bar x)(x_i-\bar x)}=\frac{\ell_{xy}}{\ell_{xx}}=\frac{1}{\ell_{xx}}\sum_{i=1}^n(x_i-\bar x)y_i.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Regression function&lt;/code&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\hat y=\hat\beta_0+\hat\beta_1x\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;expected-values&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Expected values&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;Assumption A1&lt;/code&gt;: &lt;span class=&#34;math inline&#34;&gt;\(E[\epsilon_i]=0,i=1,\dots,n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Theorem 1&lt;/code&gt;: Under Assumption A1, &lt;span class=&#34;math inline&#34;&gt;\(\hat\beta_0,\hat\beta_1\)&lt;/span&gt; are unbiased estimators for &lt;span class=&#34;math inline&#34;&gt;\(\beta_0,\beta_1\)&lt;/span&gt;, respectively.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Proof&lt;/code&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
E[\hat \beta_1] &amp;amp;= \frac{1}{\ell_{xx}}\sum_{i=1}^n(x_i-\bar x)E[y_i]\\
&amp;amp;=\frac{1}{\ell_{xx}}\sum_{i=1}^n(x_i-\bar x)(\beta_0+\beta_1x_i)\\
&amp;amp;=\frac{\beta_0}{\ell_{xx}}\sum_{i=1}^n(x_i-\bar x)+\frac{\beta_1}{\ell_{xx}}\sum_{i=1}^n(x_i-\bar x)x_i\\
&amp;amp;=\frac{\beta_1}{\ell_{xx}}\sum_{i=1}^n(x_i-\bar x)(x_i-\bar x)\\
&amp;amp;=\beta_1
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
E[\hat \beta_0] &amp;amp;= E[\bar y-\hat\beta_1\bar x]=E[\bar y]-\beta_1\bar x=\beta_0+\beta_1\bar x-\beta_1\bar x=\beta_0.
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;variances&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Variances&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;Assumption A2&lt;/code&gt;: &lt;span class=&#34;math inline&#34;&gt;\(Cov(\epsilon_i,\epsilon_j)=\sigma^21\{i=j\}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Theorem 2&lt;/code&gt;: Under Assumption A2, we have
&lt;span class=&#34;math display&#34;&gt;\[Var[\hat\beta_0] = \left(\frac 1n+\frac{\bar x^2}{\ell_{xx}}\right)\sigma^2,\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Var[\hat\beta_1] =\frac{\sigma^2}{\ell_{xx}},\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Cov(\hat\beta_0,\hat\beta_1) = \frac{-\bar x}{\ell_{xx}}\sigma^2.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Proof&lt;/code&gt;: Since &lt;span class=&#34;math inline&#34;&gt;\(Cov(\epsilon_i,\epsilon_j)=0\)&lt;/span&gt; for any &lt;span class=&#34;math inline&#34;&gt;\(i\neq j\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(Cov(y_i,y_j)=0\)&lt;/span&gt;. We thus have
&lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
Var[\hat\beta_1] &amp;amp;= \frac{1}{\ell_{xx}^2}\sum_{i=1}^n(x_i-\bar x)^2Var[y_i]\\
&amp;amp;= \frac{\sigma^2}{\ell_{xx}^2}\sum_{i=1}^n(x_i-\bar x)^2=\frac{\sigma^2}{\ell_{xx}}.
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We next show that &lt;span class=&#34;math inline&#34;&gt;\(Cov(\bar y,\hat \beta_1)=0\)&lt;/span&gt;.
&lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
Cov(\bar y,\hat \beta_1) &amp;amp;= Cov\left(\frac{1}{n}\sum_{i=1}^n y_i,\frac{1}{\ell_{xx}}\sum_{i=1}^n(x_i-\bar x)y_i\right)\\
&amp;amp;=\frac{1}{n\ell_{xx}}Cov\left(\sum_{i=1}^n y_i,\sum_{i=1}^n(x_i-\bar x)y_i\right)\\
&amp;amp;=\frac{1}{n\ell_{xx}}\sum_{i=1}^nCov(y_i,(x_i-\bar x)y_i)\\
&amp;amp;=\frac{1}{n\ell_{xx}}\sum_{i=1}^n(x_i-\bar x)\sigma^2 = 0.
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Var[\hat\beta_0] = Var[\bar y-\hat\beta_1\bar x]=Var[\bar y]+Var[\hat \beta_1\bar x]=\frac{\sigma^2}{n}+\frac{\bar x^2\sigma^2}{\ell_{xx}}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Cov(\hat\beta_0,\hat\beta_1) = Cov(\bar y-\hat\beta_1\bar x,\hat\beta_1) = -\bar x Var[\hat\beta_1]=\frac{-\bar x}{\ell_{xx}}\sigma^2.\]&lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;So bigger &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is better. Get a bigger sample size if you can. Smaller &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; is better. The most interesting one is that bigger &lt;span class=&#34;math inline&#34;&gt;\(\ell_{xx}\)&lt;/span&gt; is better. The more spread out the &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; are the better we can
estimate the slope &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;. When you’re picking the &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt;, if you can spread them out more, then it is more informative.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;estimation-of-sigma2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Estimation of &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;For Assumption A2, it is common that the variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; is unknown.
The next theorem gives an unbiased estimate of &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Definition&lt;/code&gt;: The sum of squared errors (SSE) is defined by
&lt;span class=&#34;math display&#34;&gt;\[S_e^2 = \sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)^2.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Theorem 3&lt;/code&gt;: Let
&lt;span class=&#34;math display&#34;&gt;\[\hat{\sigma}^2 := \frac{Q(\hat \beta_0,\hat\beta_1)}{n-2}=\frac{\sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)^2}{n-2}=\frac{S_e^2}{n-2}.\]&lt;/span&gt;
Under Assumptions A1 and A2, we have &lt;span class=&#34;math inline&#34;&gt;\(E[\hat\sigma^2]=\sigma^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Proof&lt;/code&gt;: Let &lt;span class=&#34;math inline&#34;&gt;\(\hat y_i = \hat\beta_0+\hat\beta_1x_i=\bar y+\hat\beta_1(x_i-\bar x)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
E[Q(\hat \beta_0,\hat\beta_1)] &amp;amp;= \sum_{i=1}^nE[(y_i-\hat y_i)^2]=\sum_{i=1}^nVar[y_i-\hat y_i]+(E[y_i]-E[\hat y_i])^2\\
&amp;amp;=\sum_{i=1}^n[Var[y_i]+Var[\hat y_i]-2Cov(y_i,\hat y_i)].
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
Var[\hat y_i]&amp;amp;= Var[\hat\beta_0+\hat\beta_1x_i]=Var[\bar y+\hat\beta_1(x_i-\bar x)]\\
&amp;amp;=Var[\bar y]+(x_i-\bar x)^2Var[\hat\beta_1]\\
&amp;amp;=\frac{\sigma^2}{n}+\frac{(x_i-\bar x)^2\sigma^2}{\ell_{xx}}.
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
Cov(y_i,\hat y_i)  &amp;amp;= Cov(\beta_0+\beta_1x_i+\epsilon_i, \bar y+\hat\beta_1(x_i-\bar x))\\
&amp;amp;=Cov(\epsilon_i,\bar y)+(x_i-\bar x)Cov(\epsilon_i,\hat\beta_1)\\
&amp;amp;=\frac{\sigma^2}{n}+\frac{(x_i-\bar x)^2\sigma^2}{\ell_{xx}}.
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As a result, we have
&lt;span class=&#34;math display&#34;&gt;\[E[Q(\hat \beta_0,\hat\beta_1)] = \sum_{i=1}^n \left[\sigma^2-\frac{\sigma^2}{n}+\frac{(x_i-\bar x)^2\sigma^2}{\ell_{xx}}\right]=(n-2)\sigma^2.\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;normal-distributions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Normal distributions&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;Assumption B&lt;/code&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_i\stackrel{iid}{\sim}N(0,\sigma^2),i=1,\dots,n\)&lt;/span&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Assumption B includes Assumptions A1 and A2.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;code&gt;Theorem 4&lt;/code&gt;: Under Assumption B, we have&lt;/p&gt;
&lt;p&gt;(1). &lt;span class=&#34;math inline&#34;&gt;\(\hat\beta_0\sim N(\beta_0,(\frac 1n+\frac{\bar x^2}{\ell_{xx}})\sigma^2)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;(2). &lt;span class=&#34;math inline&#34;&gt;\(\hat\beta_1\sim N(\beta_1,\frac{\sigma^2}{\ell_{xx}})\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;(3). &lt;span class=&#34;math inline&#34;&gt;\(\frac{(n-2)\hat\sigma^2}{\sigma^2}=\frac{S_e^2}{\sigma^2}\sim \chi^2(n-2)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;(4). &lt;span class=&#34;math inline&#34;&gt;\(\hat\sigma^2\)&lt;/span&gt; is independent of &lt;span class=&#34;math inline&#34;&gt;\((\hat\beta_0,\hat\beta_1)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Proof&lt;/code&gt;: Under Assumption B, &lt;span class=&#34;math inline&#34;&gt;\(y_i=\beta_0+\beta_1x_i+\epsilon_i\sim N(\beta_0+\beta_1x_i,\sigma^2)\)&lt;/span&gt; independently. Both &lt;span class=&#34;math inline&#34;&gt;\(\hat\beta_0,\hat\beta_1\)&lt;/span&gt; are linear combinations of &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt;s. Consequently, they are normally distributed. We have known their expected values and variances from Theorems 1 and 2. The claims (1) and (2) are thus verified. The proofs of claims (3) and (4) are deferred to the general case.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It is &lt;span class=&#34;math inline&#34;&gt;\(n-2\)&lt;/span&gt; degrees of freedom because we have fit two parameters to the &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; data points.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;confidence-intervals-and-hypothesis-tests&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Confidence intervals and hypothesis tests&lt;/h3&gt;
&lt;p&gt;For known &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; we can make tests and confidence
intervals using
&lt;span class=&#34;math display&#34;&gt;\[\frac{\hat\beta_1-\beta_1}{\sigma/\sqrt{\ell_{xx}}}\sim N(0,1).\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The &lt;span class=&#34;math inline&#34;&gt;\(100(1-\alpha)\%\)&lt;/span&gt; confidence interval for &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; is given by &lt;span class=&#34;math inline&#34;&gt;\(\hat\beta_1\pm u_{1-\alpha/2}\sigma/\sqrt{\ell_{xx}}\)&lt;/span&gt;. For testing
&lt;span class=&#34;math display&#34;&gt;\[H_0:\beta_1=\beta_1^*\ vs.\ H_1:\beta_1\neq\beta_1^*,\]&lt;/span&gt;
we reject &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(|\hat\beta_1-\beta_1^*|&amp;gt;u_{1-\alpha/2}\sigma/\sqrt{\ell_{xx}}\)&lt;/span&gt; with the most popular hypothesized value being &lt;span class=&#34;math inline&#34;&gt;\(\beta_1^*=0\)&lt;/span&gt; (i.e., the regession function is &lt;strong&gt;significant&lt;/strong&gt; or not at significance level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;.)&lt;/p&gt;
&lt;p&gt;In the more realistic setting of unknown &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;, so long as &lt;span class=&#34;math inline&#34;&gt;\(n \ge 3\)&lt;/span&gt;, using claims (2-4) gives
&lt;span class=&#34;math display&#34;&gt;\[\frac{\hat\beta_1-\beta_1}{\hat{\sigma}/\sqrt{\ell_{xx}}}\sim t(n-2).\]&lt;/span&gt;
The &lt;span class=&#34;math inline&#34;&gt;\(100(1-\alpha)\%\)&lt;/span&gt; confidence interval for &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(\hat\beta_1\pm t_{1-\alpha/2}(n-2)\hat{\sigma}/\sqrt{\ell_{xx}}\)&lt;/span&gt;. For testing
&lt;span class=&#34;math display&#34;&gt;\[H_0:\beta_1=\beta_1^*\ vs.\ H_1:\beta_1\neq\beta_1^*,\]&lt;/span&gt;
we reject &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(|\hat\beta_1-\beta_1^*|&amp;gt;t_{1-\alpha/2}(n-2)\hat\sigma/\sqrt{\ell_{xx}}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For drawing inferences about &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;, we can use &lt;span class=&#34;math display&#34;&gt;\[\frac{\hat\beta_0-\beta_0}{\sigma\sqrt{1/n+\bar x^2/\ell_{xx}}}\sim N(0,1),\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\frac{\hat\beta_0-\beta_0}{\hat\sigma\sqrt{1/n+\bar x^2/\ell_{xx}}}\sim t(n-2).\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The &lt;span class=&#34;math inline&#34;&gt;\(100(1-\alpha)\%\)&lt;/span&gt; confidence interval for &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; is
&lt;span class=&#34;math display&#34;&gt;\[\left[\frac{(n-2)\hat\sigma^2}{\chi_{1-\alpha/2}^2(n-2)},\frac{(n-2)\hat\sigma^2}{\chi_{\alpha/2}^2(n-2)}\right]=\left[\frac{S_e^2}{\chi_{1-\alpha/2}^2(n-2)},\frac{S_e^2}{\chi_{\alpha/2}^2(n-2)}\right].\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;case-study-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Case study 1&lt;/h3&gt;
&lt;p&gt;A manufacturer of air conditioning units is having assembly problems due to
the failure of a connecting rod (连接杆) to meet finished-weight specifications. Too many
rods are being completely tooled, then rejected as overweight. To reduce that
cost, the company’s quality-control department wants to quantify the relationship
between the weight of the &lt;strong&gt;finished rod&lt;/strong&gt;, &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, and that of the &lt;strong&gt;rough casting&lt;/strong&gt; (毛坯铸件), &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. Castings likely to produce rods that are too heavy can then be discarded
before undergoing the final (and costly) tooling process. The data are displayed below.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-1&#34;&gt;Table 1: &lt;/span&gt;rough weight vs. finished weight
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
id
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
rough_weight
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
finished_weight
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.745
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.080
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.700
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.045
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.690
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.050
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.680
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.005
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.675
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.035
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.670
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.035
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.665
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.020
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.660
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.005
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.655
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.010
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.655
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
11
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.650
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.650
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.005
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
13
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.645
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.015
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
14
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.635
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.990
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.630
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.990
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
16
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.625
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.995
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.625
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.985
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.620
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.970
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
19
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.615
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.985
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.615
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.990
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
21
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.615
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.995
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
22
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.610
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.990
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.590
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.975
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
24
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.590
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.995
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
25
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.565
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.955
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Consider the linear model
&lt;span class=&#34;math display&#34;&gt;\[y_i=\beta_0+\beta_1x_i+\epsilon_i,\ \epsilon_i\stackrel{iid}{\sim}N(0,\sigma^2).\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The observed data gives &lt;span class=&#34;math inline&#34;&gt;\(\bar x = 2.643\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\bar y=2.0048\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\ell_{xx}=0.0367\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\ell_{xy}=0.023565\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\hat\sigma = 0.0113\)&lt;/span&gt;.
The least square estimates are
&lt;span class=&#34;math display&#34;&gt;\[\hat\beta_1=\frac{\ell_{xy}}{\ell_{xx}}=\frac{0.023565}{0.0367}=0.642,\ \hat\beta_0=\bar y-\hat\beta_1\bar x=0.308.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The regession function &lt;span class=&#34;math inline&#34;&gt;\(\hat y = 0.308+0.642 x\)&lt;/span&gt;; see the blue line given below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attach(rod)
par(mar=c(4,4,1,0.5))
plot(rough_weight,finished_weight,type=&amp;quot;p&amp;quot;,pch=16,
     xlab = &amp;quot;Rough Weight&amp;quot;,ylab = &amp;quot;Finished Weight&amp;quot;)
lm.rod = lm(finished_weight~rough_weight)
abline(coef(lm.rod),col=&amp;quot;blue&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/chap04_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(lm.rod) #output the results&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = finished_weight ~ rough_weight)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.023558 -0.008242  0.001074  0.008179  0.024231 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)   0.30773    0.15608   1.972   0.0608 .  
## rough_weight  0.64210    0.05905  10.874 1.54e-10 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.01131 on 23 degrees of freedom
## Multiple R-squared:  0.8372, Adjusted R-squared:  0.8301 
## F-statistic: 118.3 on 1 and 23 DF,  p-value: 1.536e-10&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;assessing-the-fit&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Assessing the Fit&lt;/h3&gt;
&lt;p&gt;As an aid in assessing the quality of the fit, we will make extensive use of the residuals,
which are the differences between the observed and fitted values:
&lt;span class=&#34;math display&#34;&gt;\[\hat \epsilon_i = y_i-\hat\beta_0-\hat\beta_1x_i,\ i=1,\dots,n.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It is most useful to examine the residuals graphically. Plots of the residuals versus the
&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; values may reveal systematic misfit or ways in which the data do not conform to
the fitted model. Ideally, the residuals should show no relation to the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; values, and the plot should look like a horizontal blur. The residuals for case study 1 are plotted below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mar=c(4,4,2,1))
plot(lm.rod$fitted.values,lm.rod$residuals,&amp;quot;p&amp;quot;,
     xlab=&amp;quot;Fitted values&amp;quot;,ylab = &amp;quot;Residuals&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/chap04_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;
Standardized Residuals are graphed below. The key command is &lt;code&gt;rstandard&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mar=c(4,4,2,1))
plot(lm.rod$fitted.values,rstandard(lm.rod),&amp;quot;p&amp;quot;,
     xlab=&amp;quot;Fitted values&amp;quot;,ylab = &amp;quot;Standardized Residuals&amp;quot;)
abline(h=c(-2,2),lty=c(5,5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/chap04_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;drawing-inferences-about-ey&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Drawing Inferences about &lt;span class=&#34;math inline&#34;&gt;\(E[y]\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;For given &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, we want to estimate the expected value of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, i.e., &lt;span class=&#34;math inline&#34;&gt;\(E[y]=\beta_0+\beta_1x.\)&lt;/span&gt; A natural unbiased estimate is &lt;span class=&#34;math inline&#34;&gt;\(\hat y = \hat\beta_0+\hat\beta_1x\)&lt;/span&gt;. From the proof of Theorem 3, we have the variance
&lt;span class=&#34;math display&#34;&gt;\[Var[\hat y] = \left(\frac{1}{n}+\frac{(x-\bar x)^2}{\ell_{xx}}\right)\sigma^2.\]&lt;/span&gt;
Under Assumption B, by Theorem 4, we have
&lt;span class=&#34;math display&#34;&gt;\[\hat y\sim N(\beta_0+\beta_1x,(1/n+(x-\bar x)^2/\ell_{xx})\sigma^2),\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\frac{\hat y-E[\hat y]}{\hat{\sigma}\sqrt{1/n+(x-\bar x)/\ell_{xx}}}\sim t(n-2)\]&lt;/span&gt;
We thus have the following results.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Theorem 5&lt;/code&gt;: Suppose Assumption B is satisfied. Then we have
&lt;span class=&#34;math display&#34;&gt;\[\hat y = \hat\beta_0+\hat\beta_1x \sim N(\beta_0+\beta_1x,[1/n+(x-\bar x)^2/\ell_{xx}]\sigma^2).\]&lt;/span&gt;
A &lt;span class=&#34;math inline&#34;&gt;\(100(1−\alpha)\%\)&lt;/span&gt; confidence interval for &lt;span class=&#34;math inline&#34;&gt;\(E[y]=\beta_0+\beta_1x\)&lt;/span&gt; is
given by
&lt;span class=&#34;math display&#34;&gt;\[\hat y\pm t_{1-\alpha/2}(n-2)\hat{\sigma}\sqrt{\frac{1}{n}+\frac{(x-\bar x)^2}{\ell_{xx}}}.\]&lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Notice from the formula in Theorem 5 that the width of a confidence
interval for &lt;span class=&#34;math inline&#34;&gt;\(E[y]\)&lt;/span&gt; increases as the value of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; becomes more extreme. That
is, we are better able to predict the location of the regression line for an &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;-value
close to &lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; than we are for &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;-values that are either very small or very large.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For case study 1, we plot the lower and upper limits for the &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; confidence interval for &lt;span class=&#34;math inline&#34;&gt;\(E[y]\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x = seq(2.5,2.8,by=0.001)
newdata = data.frame(rough_weight= x)
pred_x = predict(lm.rod,newdata,interval = &amp;quot;confidence&amp;quot;)
par(mar=c(4,4,2,1))
matplot(x,pred_x,type=&amp;quot;l&amp;quot;,lty = c(1,5,5),
        col=c(&amp;quot;blue&amp;quot;,&amp;quot;red&amp;quot;,&amp;quot;red&amp;quot;),lwd=2,
        xlab=&amp;quot;Rough Weight&amp;quot;,ylab=&amp;quot;Finished Weight&amp;quot;)
abline(v=mean(rough_weight),lty=5)
points(rough_weight,finished_weight,pch=16)
legend(2.5,2.1,c(&amp;quot;Fitted&amp;quot;,&amp;quot;Lower limit&amp;quot;,&amp;quot;Upper limit&amp;quot;),
       lty = c(1,5,5),col=c(&amp;quot;blue&amp;quot;,&amp;quot;red&amp;quot;,&amp;quot;red&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/chap04_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;drawing-inferences-about-future-observations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Drawing Inferences about Future Observations&lt;/h3&gt;
&lt;p&gt;We now give a &lt;strong&gt;prediction interval&lt;/strong&gt; for the future observation &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; rather than its expected value &lt;span class=&#34;math inline&#34;&gt;\(E[y]\)&lt;/span&gt;. Note that here &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is no longer a fixed parameter, which is assumed to be independent of &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt;’s. A prediction interval is a range of numbers that
contains &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; with a specified probability. Consider &lt;span class=&#34;math inline&#34;&gt;\(y-\hat y\)&lt;/span&gt;. If Assumption A1 is satisfied, then
&lt;span class=&#34;math display&#34;&gt;\[E[y-\hat y] = E[y]-E[\hat y]= 0.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If Assumption A2 is satisfied, then
&lt;span class=&#34;math display&#34;&gt;\[Var[y-\hat y] = Var[y]+Var[\hat y]=\left(1+\frac{1}{n}+\frac{(x-\bar x)^2}{\ell_{xx}}\right)\sigma^2.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If Assumption B is satisfied, &lt;span class=&#34;math inline&#34;&gt;\(y-\hat y\)&lt;/span&gt; is then normally distributed.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Theorem 6&lt;/code&gt;: Suppose Assumption B is satisfied. Let &lt;span class=&#34;math inline&#34;&gt;\(y=\beta_0+\beta_1x+\epsilon\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\sim N(0,\sigma^2)\)&lt;/span&gt; is independent of &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_i\)&lt;/span&gt;’s. A &lt;span class=&#34;math inline&#34;&gt;\(100(1−\alpha)\%\)&lt;/span&gt; prediction interval for &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is
given by
&lt;span class=&#34;math display&#34;&gt;\[\hat y\pm t_{1-\alpha/2}(n-2)\hat{\sigma}\sqrt{1+\frac{1}{n}+\frac{(x-\bar x)^2}{\ell_{xx}}}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For case study 1, we plot the lower and upper limits for the &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; prediction interval for &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x = seq(2.5,2.8,by=0.001)
newdata = data.frame(rough_weight= x)
pred_x = predict(lm.rod,newdata,interval = &amp;quot;prediction&amp;quot;)
par(mar=c(4,4,2,1))
matplot(x,pred_x,type=&amp;quot;l&amp;quot;,lty = c(1,5,5),
        col=c(&amp;quot;blue&amp;quot;,&amp;quot;red&amp;quot;,&amp;quot;red&amp;quot;),lwd=2,
        xlab=&amp;quot;Rough Weight&amp;quot;,ylab=&amp;quot;Finished Weight&amp;quot;)
abline(v=mean(rough_weight),lty=5)
points(rough_weight,finished_weight,pch=16)
legend(2.5,2.1,c(&amp;quot;Fitted&amp;quot;,&amp;quot;Lower limit&amp;quot;,&amp;quot;Upper limit&amp;quot;),
       lty = c(1,5,5),col=c(&amp;quot;blue&amp;quot;,&amp;quot;red&amp;quot;,&amp;quot;red&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/chap04_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-control-y&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How to control y?&lt;/h3&gt;
&lt;p&gt;Consider case study 1 again. Castings likely to produce rods that are too heavy can then be discarded before undergoing the final (and costly) tooling process. The company’s quality-control department wants to produce the rod &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; with weights no large than 2.05 with probablity no less than 0.95. How to choose the rough casting?&lt;/p&gt;
&lt;p&gt;Now we want &lt;span class=&#34;math inline&#34;&gt;\(y\le y_0=2.05\)&lt;/span&gt; with probability &lt;span class=&#34;math inline&#34;&gt;\(1-\alpha\)&lt;/span&gt;. Similarly to Theorem 6, we can construct one-side confidence interval for &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, that is
&lt;span class=&#34;math display&#34;&gt;\[\bigg(-\infty,\hat y+t_{1-\alpha}(n-2)\hat{\sigma}\sqrt{1+\frac{1}{n}+\frac{(x-\bar x)^2}{\ell_{xx}}}\bigg].\]&lt;/span&gt;
This implies &lt;span class=&#34;math display&#34;&gt;\[\hat\beta_0+\hat\beta_1x+t_{1-\alpha}(n-2)\hat{\sigma}\sqrt{1+\frac{1}{n}+\frac{(x-\bar x)^2}{\ell_{xx}}}\le y_0.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/chap04_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-linear-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multiple linear regression&lt;/h2&gt;
&lt;p&gt;With problems more complex than fitting a straight line, it is very useful to approach the linear least squares analysis via linear algebra.
Consider a model of the form
&lt;span class=&#34;math display&#34;&gt;\[y_i=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\dots+\beta_{p-1}x_{i,p-1}+\epsilon_i,\ i=1,\dots,n.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(Y=(y_1,\dots,y_n)^\top\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\beta=(\beta_0,\dots,\beta_{p-1})^\top\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\epsilon=(\epsilon_1,\dots,\epsilon_n)^\top\)&lt;/span&gt;, and let &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; be the &lt;span class=&#34;math inline&#34;&gt;\(n\times p\)&lt;/span&gt; matrix
&lt;span class=&#34;math display&#34;&gt;\[
X=
\left[
\begin{matrix}
1 &amp;amp; x_{11} &amp;amp; x_{12} &amp;amp; \cdots &amp;amp; x_{1,p-1}\\
1 &amp;amp; x_{21} &amp;amp; x_{22} &amp;amp; \cdots &amp;amp; x_{2,p-1}\\
\vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \\
1 &amp;amp; x_{n1} &amp;amp; x_{n2} &amp;amp; \cdots &amp;amp; x_{n,p-1}\\
\end{matrix}
\right].
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The model can be rewritten as &lt;span class=&#34;math display&#34;&gt;\[Y=X\beta+\epsilon,\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the matrix &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is called the &lt;strong&gt;design matrix&lt;/strong&gt;,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;assume that &lt;span class=&#34;math inline&#34;&gt;\(p&amp;lt;n\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The
least squares problem can then be phrased as follows: Find &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; to minimize&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Q(\beta)=\sum_{i=1}^n(y_i-\beta_0-\beta_1x_{i1}-\dots-\beta_{p-1}x_{i,p-1})^2:=||Y-X\beta||^2,\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(||\cdot||\)&lt;/span&gt; is the Euclidean norm.&lt;/p&gt;
&lt;p&gt;Note that &lt;span class=&#34;math display&#34;&gt;\[Q=(Y-X\beta)^\top(Y-X\beta) = Y^\top Y-2Y^\top X\beta+\beta^\top X^\top X\beta.\]&lt;/span&gt;
If we differentiate &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; with respect to each &lt;span class=&#34;math inline&#34;&gt;\(\beta_i\)&lt;/span&gt; and set the derivatives equal to zero, we see that the minimizers &lt;span class=&#34;math inline&#34;&gt;\(\hat\beta_0,\dots,\hat\beta_{p-1}\)&lt;/span&gt; satisfy&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{\partial Q}{\partial \beta_i}=-2(Y^\top X)_i+2(X^{\top}X)_{i\cdot}\hat\beta=0.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We thus arrive at the so-called &lt;strong&gt;normal equations&lt;/strong&gt;: &lt;span class=&#34;math display&#34;&gt;\[X^\top X\hat\beta = X^\top Y.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If the design matrix &lt;span class=&#34;math inline&#34;&gt;\(X^\top X\)&lt;/span&gt; is &lt;strong&gt;nonsingular&lt;/strong&gt;, the formal solution is
&lt;span class=&#34;math display&#34;&gt;\[\hat\beta = (X^\top X)^{-1}X^\top Y.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The following lemma gives a criterion for the existence and uniqueness of solutions
of the normal equations.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Lemma 1&lt;/code&gt;: The design matrix &lt;span class=&#34;math inline&#34;&gt;\(X^\top X\)&lt;/span&gt; is nonsingular if and only if &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{rank}(X)=p\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Proof&lt;/code&gt;: First suppose that &lt;span class=&#34;math inline&#34;&gt;\(X^\top X\)&lt;/span&gt; is singular. There exists a nonzero vector &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; such that
&lt;span class=&#34;math inline&#34;&gt;\(X^\top Xu = 0\)&lt;/span&gt;. Multiplying the left-hand side of this equation by &lt;span class=&#34;math inline&#34;&gt;\(u^\top\)&lt;/span&gt;, we have &lt;span class=&#34;math inline&#34;&gt;\(0=u^\top X^\top Xu=(Xu)^\top (Xu)\)&lt;/span&gt;
So &lt;span class=&#34;math inline&#34;&gt;\(Xu=0\)&lt;/span&gt;, the columns of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; are linearly dependent, and the rank of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is less
than &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Next, suppose that the rank of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is less than &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; so that there exists a nonzero
vector &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(Xu = 0\)&lt;/span&gt;. Then &lt;span class=&#34;math inline&#34;&gt;\(X^\top Xu = 0\)&lt;/span&gt;, and hence &lt;span class=&#34;math inline&#34;&gt;\(X^\top X\)&lt;/span&gt; is singular.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In what follows, we assume that &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{rank}(X)=p&amp;lt;n\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div id=&#34;expected-values-and-variances&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Expected values and variances&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;Assumption A&lt;/code&gt;: Assume that &lt;span class=&#34;math inline&#34;&gt;\(E[\epsilon]=0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Var[\epsilon]=\sigma^2I_p\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Theorem 7&lt;/code&gt;: Suppose that Assumption A is satisfied and &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{rank}(X)=p&amp;lt;n\)&lt;/span&gt;, we have&lt;/p&gt;
&lt;p&gt;(1). &lt;span class=&#34;math inline&#34;&gt;\(E[\hat\beta]=\beta,\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;(2). &lt;span class=&#34;math inline&#34;&gt;\(Var[\hat\beta]=\sigma^2(X^\top X)^{-1}\)&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Proof&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
E[\hat\beta]&amp;amp;= E[(X^\top X)^{-1}X^{\top}Y] \\
&amp;amp;= (X^\top X)^{-1}X^{\top}E[Y]\\
&amp;amp;=(X^\top X)^{-1}X^{\top}(X\beta)=\beta.
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
Var[\hat\beta] &amp;amp;= Var[(X^\top X)^{-1}X^{\top}Y]\\
&amp;amp;=(X^\top X)^{-1}X^{\top}Var(Y)X(X^\top X)^{-1}\\
&amp;amp;=\sigma^2(X^\top X)^{-1}.
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We used the fact that &lt;span class=&#34;math inline&#34;&gt;\(Var(AY) = AVar(Y)A^\top\)&lt;/span&gt; for any fixed matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(X^\top X\)&lt;/span&gt; and therefore &lt;span class=&#34;math inline&#34;&gt;\((X^\top X)^{-1}\)&lt;/span&gt; are symmetric.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;estimation-of-sigma2-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Estimation of &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;Definition&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;The fitted values&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\hat Y = X\hat\beta\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;The vector of residuals&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\hat\epsilon = Y-\hat Y\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;The sum of squared errors (SSE)&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(S_e^2=Q(\hat\beta)=||Y-\hat Y||^2=||\hat\epsilon||^2\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat Y = X\hat\beta=X(X^\top X)^{-1}X^\top Y=:PY\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;The projection matrix&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(P = X(X^\top X)^{-1}X^\top\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The vector of residuals is then &lt;span class=&#34;math inline&#34;&gt;\(\hat\epsilon=(I_n-P)Y\)&lt;/span&gt;. Two useful properties of &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; are given in the following lemma.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Lemma 2&lt;/code&gt;: Let &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; be defined as before. Then
&lt;span class=&#34;math display&#34;&gt;\[P = P^\top=P^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[I_n-P = (I_n-P)^\top=(I_n-P)^2.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Note&lt;/code&gt;: We may think
geometrically of the fitted values, &lt;span class=&#34;math inline&#34;&gt;\(\hat Y=X\hat\beta=PY\)&lt;/span&gt;, as being the projection of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; onto the subspace
spanned by the columns of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The sum of squared residuals is then
&lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
S_e^2 := ||\hat \epsilon||^2 = Y^\top(I_n-P)^\top(I_n-P)Y=Y^\top(I_n-P)Y.
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Definition&lt;/code&gt;: Let &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; be an &lt;span class=&#34;math inline&#34;&gt;\(n\times n\)&lt;/span&gt; matrix. The trace of the matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is defined as &lt;span class=&#34;math inline&#34;&gt;\(tr(A) = \sum_{i=1}^n a_{ii}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(a_{ii}\)&lt;/span&gt; are the elements on the main diagonal.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Lemma 3&lt;/code&gt;: If &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is an &lt;span class=&#34;math inline&#34;&gt;\(m \times n\)&lt;/span&gt; matrix and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; is an &lt;span class=&#34;math inline&#34;&gt;\(n \times m\)&lt;/span&gt; matrix, then &lt;span class=&#34;math display&#34;&gt;\[tr(AB)=tr(BA).\]&lt;/span&gt; This is the cyclic property of the trace.&lt;/p&gt;
&lt;p&gt;Using Lemma 3, we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
E[S_e^2]&amp;amp;= E[Y^\top(I_n-P)Y]=E[tr(Y^\top(I_n-P)Y)] \\&amp;amp;= E[tr((I_n-P)YY^\top)]=tr((I_n-P)E[YY^\top])\\
&amp;amp;=tr((I_n-P)(Var[Y]+E[Y]E[Y^\top]))\\
&amp;amp;=tr((I_n-P)(\sigma^2 I_n))+tr((I_n-P)X\beta\beta^\top X^\top)\\
&amp;amp;=\sigma^2(n-tr(P))
\end{align}
\]&lt;/span&gt;
where we used &lt;span class=&#34;math inline&#34;&gt;\((I_n-P)X=X-X(X^\top X)^{-1}X^\top X=0\)&lt;/span&gt;. Using the cyclic property of the trace again gives&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
tr(P)&amp;amp;= tr(X(X^\top X)^{-1}X^\top)\\
&amp;amp;=tr(X^\top X(X^\top X)^{-1})=tr(I_p)=p.
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We therefore have &lt;span class=&#34;math inline&#34;&gt;\(E[S_e^2]=(n-p)\sigma^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Theorem 8&lt;/code&gt;: Suppose that Assumption A is satisfied and &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{rank}(X)=p&amp;lt;n\)&lt;/span&gt;,
&lt;span class=&#34;math display&#34;&gt;\[\hat\sigma^2 = \frac{S_e^2}{n-p}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;is an unbiased estimate of &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;normal-distribution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Normal distribution&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;Assumption B&lt;/code&gt;: Assume that &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\sim N(0,\sigma^2I_n)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Theorem 9&lt;/code&gt;: Suppose that Assumption B is satisfied and &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{rank}(X)=p&amp;lt;n\)&lt;/span&gt;, we have&lt;/p&gt;
&lt;p&gt;(1). &lt;span class=&#34;math inline&#34;&gt;\(\hat\beta \sim N(\beta, \sigma^2(X^\top X)^{-1})\)&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;(2). &lt;span class=&#34;math inline&#34;&gt;\(\frac{(n-p)\hat\sigma^2}{\sigma^2}=\frac{S_e^2}{\sigma^2}\sim \chi^2(n-p)\)&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;(3). &lt;span class=&#34;math inline&#34;&gt;\(\hat\epsilon\)&lt;/span&gt; is independent of &lt;span class=&#34;math inline&#34;&gt;\(\hat Y\)&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;(4). &lt;span class=&#34;math inline&#34;&gt;\(S_e^2\)&lt;/span&gt; (or equivalently &lt;span class=&#34;math inline&#34;&gt;\(\hat\sigma^2\)&lt;/span&gt;) is independent of &lt;span class=&#34;math inline&#34;&gt;\(\hat\beta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Proof&lt;/code&gt;: If Assumption B is satisfied, then &lt;span class=&#34;math inline&#34;&gt;\(Y = X\beta+\epsilon\sim N(X\beta,\sigma^2_n)\)&lt;/span&gt;. Recall that &lt;span class=&#34;math inline&#34;&gt;\(\hat\beta = (X^\top X)^{-1}X^\top Y\)&lt;/span&gt; is normally distributed. The mean and covariance are given in Theorem 7 since Assumption A is satisfied.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\xi_1,\dots,\xi_p\)&lt;/span&gt; be the orthogonal basis of the subspace &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{span}(X)\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^n\)&lt;/span&gt; generated by the &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; columns of the matrix &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\xi_{p+1},\dots,\xi_n\)&lt;/span&gt; be the orthogonal basis of the orthogonal complement &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{span}(X)^\perp\)&lt;/span&gt;. Since &lt;span class=&#34;math inline&#34;&gt;\(\hat Y = X\hat\beta\in \mathrm{span}(X)\)&lt;/span&gt;, there exists &lt;span class=&#34;math inline&#34;&gt;\(z_1,\dots,z_p\)&lt;/span&gt; such that
&lt;span class=&#34;math display&#34;&gt;\[\hat Y = \sum_{i=1}^p z_i\xi_i.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;On the other hand, &lt;span class=&#34;math inline&#34;&gt;\(\hat Y^\top \hat \epsilon = (PY)^\top (I_n-P)Y = Y^\top P^\top (I_n-P)Y\)&lt;/span&gt;. By Lemma 2, we have
&lt;span class=&#34;math display&#34;&gt;\[P^\top (I_n-P)=P-P^2 = 0.\]&lt;/span&gt;
As a result, &lt;span class=&#34;math inline&#34;&gt;\(\hat Y^\top \hat \epsilon = 0\)&lt;/span&gt;, implying &lt;span class=&#34;math inline&#34;&gt;\(\hat \epsilon\in \mathrm{span}(X)^\perp\)&lt;/span&gt;. So there exsits &lt;span class=&#34;math inline&#34;&gt;\(z_{p+1},\dots,z_n\)&lt;/span&gt; such that
&lt;span class=&#34;math display&#34;&gt;\[\hat \epsilon = \sum_{i=p+1}^n z_i\xi_i.\]&lt;/span&gt;
Let &lt;span class=&#34;math inline&#34;&gt;\(U = (\xi_1,\dots,\xi_n)\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; is an orthogonal matrix, and let &lt;span class=&#34;math inline&#34;&gt;\(z=(z_1,\dots,z_n)^\top\)&lt;/span&gt;. We thus have
&lt;span class=&#34;math display&#34;&gt;\[Y = \hat Y+\hat\epsilon =\sum_{i=1}^nz_i\xi_i=Uz.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore, &lt;span class=&#34;math display&#34;&gt;\[z=U^{-1}Y=U^\top Y\sim N(U^\top X\beta,U^\top(\sigma^2 I_n)U)=N(U^\top X\beta,\sigma^2 I_n).\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This implies that &lt;span class=&#34;math inline&#34;&gt;\(z_i\)&lt;/span&gt; are independently normally distributed. So &lt;span class=&#34;math inline&#34;&gt;\(\hat Y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat\epsilon\)&lt;/span&gt; are independent. We next prove that &lt;span class=&#34;math inline&#34;&gt;\(E[z_i]=0\)&lt;/span&gt; for all &lt;span class=&#34;math inline&#34;&gt;\(i&amp;gt;p\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(A=(\xi_{p+1},\dots,\xi_{n})\)&lt;/span&gt;, then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[A(z_{p+1},\dots,z_{n})^\top = \hat\epsilon=(I_n-P)Y.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This gives
&lt;span class=&#34;math display&#34;&gt;\[E[(z_{p+1},\dots,z_{n})^\top] = E[A^\top (I_n-P)Y]=A^\top (I_n-P)E[Y]=A^\top (I_n-P)X\beta=0.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Consequently, &lt;span class=&#34;math inline&#34;&gt;\(z_i\stackrel{iid}{\sim} N(0,\sigma^2),i=p+1,\dots,n\)&lt;/span&gt;, implying &lt;span class=&#34;math display&#34;&gt;\[S_e^2/\sigma^2 = \hat\epsilon^\top\hat\epsilon/\sigma^2 = \sum_{i=p+1}^n (z_i/\sigma)^2\sim \chi^2(n-p).\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note that &lt;span class=&#34;math inline&#34;&gt;\(S_e^2\)&lt;/span&gt; is a function of &lt;span class=&#34;math inline&#34;&gt;\(\hat\epsilon_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat \beta = (X^\top X)^{-1}X^\top Y =(X^\top X)^{-1} X^\top \hat Y\)&lt;/span&gt; are linear conbinations of &lt;span class=&#34;math inline&#34;&gt;\(\hat y_i\)&lt;/span&gt;. So &lt;span class=&#34;math inline&#34;&gt;\(S_e^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat\beta\)&lt;/span&gt; are independent.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;confidence-intervals-and-hypothesis-tests-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Confidence intervals and hypothesis tests&lt;/h3&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(C=(X^\top X)^{-1}\)&lt;/span&gt; with entries &lt;span class=&#34;math inline&#34;&gt;\(c_{ij}\)&lt;/span&gt;. Suppose that Assumption B is satisfied and &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{rank}(X)=p&amp;lt;n\)&lt;/span&gt;. If &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; is known, for each &lt;span class=&#34;math inline&#34;&gt;\(\beta_i\)&lt;/span&gt;, the &lt;span class=&#34;math inline&#34;&gt;\(100(1-\alpha)\%\)&lt;/span&gt; CI is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat\beta_i \pm u_{1-\alpha/2}\sigma\sqrt{c_{ii}}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; is unknown, for each &lt;span class=&#34;math inline&#34;&gt;\(\beta_i\)&lt;/span&gt;, the &lt;span class=&#34;math inline&#34;&gt;\(100(1-\alpha)\%\)&lt;/span&gt; CI is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat\beta_i \pm t_{1-\alpha/2}(n-p)\hat\sigma\sqrt{c_{ii}}.\]&lt;/span&gt;
For testing hypothesis &lt;span class=&#34;math inline&#34;&gt;\(H_0:\beta_i= 0\ vs.\ H_1:\beta_i\neq 0\)&lt;/span&gt;, the rejection region is
&lt;span class=&#34;math display&#34;&gt;\[W = \{|\beta_i|&amp;gt;t_{1-\alpha/2}(n-p)\hat\sigma\sqrt{c_{ii}}\}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The &lt;span class=&#34;math inline&#34;&gt;\(100(1-\alpha)\%\)&lt;/span&gt; CI for &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; is &lt;span class=&#34;math display&#34;&gt;\[\left[\frac{(n-p)\hat\sigma^2}{\chi_{1-\alpha/2}^2(n-p)},\frac{(n-p)\hat\sigma^2}{\chi_{\alpha/2}^2(n-2)}\right]=\left[\frac{S_e^2}{\chi_{1-\alpha/2}^2(n-p)},\frac{S_e^2}{\chi_{\alpha/2}^2(n-p)}\right].\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;significance-tests&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Significance tests&lt;/h3&gt;
&lt;p&gt;Consider the hypothesis test:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0:\beta_1=\dots=\beta_{p-1}=0\ vs.\ H_1: \beta_{i^*}\neq 0\text{ for some }i^*\ge 1.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Definition&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;The total sum of squares (SST)&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[S_T^2 = \sum_{i=1}^n(y_i-\bar Y)^2\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;The sum of squares due to regression (SSR)&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[S_R^2  = \sum_{i=1}^n(\hat y_i-\bar Y)^2\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;The sum of squared errors (SSE)&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[S_e^2 = \sum_{i=1}^n(y_i-\hat y_i)^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The relationship is
&lt;span class=&#34;math display&#34;&gt;\[S_T^2=S_R^2+S_e^2.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It is easy to see that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
S_T^2&amp;amp;=\sum_{i=1}^n (y_i-\bar Y)^2 = \sum_{i=1}^n (y_i-\hat y_i+\hat y_i-\bar Y)^2\\
&amp;amp;=\sum_{i=1}^n [(y_i-\hat y_i)^2+(\hat y_i-\bar Y)^2+2(y_i-\hat y_i)(\hat y_i-\bar Y)]\\
&amp;amp;=S_R^2+S_e^2 +2\sum_{i=1}^n (y_i-\hat y_i)(\hat y_i-\bar Y)
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Using Lemma 2, we have
&lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\sum_{i=1}^n (y_i-\hat y_i)(\hat y_i-\bar Y) &amp;amp;= \hat\epsilon^\top\hat Y-\bar Y\sum_{i=1}^n \hat\epsilon_i\\
&amp;amp;= [(I_n-P)Y]^\top PY-\bar Y(1,1,\dots,1) (I_n-P)Y\\
&amp;amp;=Y^\top (I_n-P)PY - \bar Y[(1,1,\dots,1)-(1,1,\dots,1)P]Y\\
&amp;amp;= 0.
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is because &lt;span class=&#34;math inline&#34;&gt;\(PX = X\)&lt;/span&gt; and the first column of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\((1,1,\dots,1)^\top\)&lt;/span&gt;. This implies that &lt;span class=&#34;math inline&#34;&gt;\(P(1,1,\dots,1)^\top = (1,1,\dots,1)^\top\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Theorem 10&lt;/code&gt;: Suppose that Assumption B is satisfied and &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{rank}(X)=p&amp;lt;n\)&lt;/span&gt;, we have&lt;/p&gt;
&lt;p&gt;(1). &lt;span class=&#34;math inline&#34;&gt;\(S_R^2,S_e^2,\bar Y\)&lt;/span&gt; are independent, and&lt;/p&gt;
&lt;p&gt;(2). if the null &lt;span class=&#34;math inline&#34;&gt;\(H_0:\beta_1=\dots=\beta_{p-1}=0\)&lt;/span&gt; is true, &lt;span class=&#34;math inline&#34;&gt;\(S_R^2/\sigma^2\sim\chi^2(p-1)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Proof&lt;/code&gt;: Using the same notations in Theorem 9. Since &lt;span class=&#34;math inline&#34;&gt;\((1,\dots,1)^\top\in \mathrm{span}(X)\)&lt;/span&gt;, we set &lt;span class=&#34;math inline&#34;&gt;\(\xi_1 = (1/\sqrt{n},1/\sqrt{n},\dots,1/\sqrt{n})^\top\)&lt;/span&gt;. Recall that
&lt;span class=&#34;math display&#34;&gt;\[Y = \sum_{i=1}^n z_i\xi_i = Uz.\]&lt;/span&gt;
The average &lt;span class=&#34;math inline&#34;&gt;\(\bar Y = (1/n,\dots,1/n)Y = (1/n,\dots,1/n)Uz=z_1/\sqrt{n},\)&lt;/span&gt; where we used the fact that &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; is orthogonal matrix and the first colum of &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(\xi_1 = (1/\sqrt{n},1/\sqrt{n},\dots,1/\sqrt{n})^\top\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Recall that &lt;span class=&#34;math inline&#34;&gt;\(\hat Y = \sum_{i=1}^p z_i\xi_i = (z_1/\sqrt{n},z_1/\sqrt{n},\dots,z_1/\sqrt{n})^\top+\sum_{i=2}^p z_i\xi_i\)&lt;/span&gt;.
This implies that
&lt;span class=&#34;math display&#34;&gt;\[(\hat y_1-\bar Y,\dots,\hat y_n-\bar Y)^\top = \sum_{i=2}^p z_i\xi_i.\]&lt;/span&gt;
As a result, &lt;span class=&#34;math inline&#34;&gt;\(S_R^2 = \sum_{i=2}^p z_i^2\)&lt;/span&gt;. Recall that &lt;span class=&#34;math inline&#34;&gt;\(S_e^2=\sum_{i=p+1}^n z_i^2\)&lt;/span&gt;. Since &lt;span class=&#34;math inline&#34;&gt;\(z_i\)&lt;/span&gt; are independent, &lt;span class=&#34;math inline&#34;&gt;\(S_R^2,S_e^2,\bar Y\)&lt;/span&gt; are independent.&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(\beta_1=\dots=\beta_{p-1}=0\)&lt;/span&gt;, we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[E[z] = U^\top X\beta = U^\top (\beta_0,\dots,\beta_0)^\top.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[E[z^\top]=\beta_0(1,1,\dots,1) U = \beta_0(\sqrt{n},0,\dots,0).\]&lt;/span&gt;
We therefore have &lt;span class=&#34;math inline&#34;&gt;\(z_i\sim N(0,\sigma^2)\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(i=2,\dots,n\)&lt;/span&gt;. So &lt;span class=&#34;math display&#34;&gt;\[S_R^2/\sigma^2 = \sum_{i=2}^p (z_i/\sigma)^2\sim \chi^2(p-1).\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We now use generalized likelihood (GLR) test. The likelihood function for &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is given by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[L(\beta,\sigma^2) = (2\pi \sigma^2)^{-n/2} e^{-\frac{||Y-X\beta||^2}{2\sigma^2}}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The maximizers for &lt;span class=&#34;math inline&#34;&gt;\(L(\beta,\sigma^2)\)&lt;/span&gt; over the parameter space &lt;span class=&#34;math inline&#34;&gt;\(\Theta=\{(\beta,\sigma^2)|\beta\in \mathbb{R}^p,\sigma^2&amp;gt;0\}\)&lt;/span&gt; are&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat\beta = (X^\top X)^{-1}X^\top Y,\ \hat\sigma^2 = \frac{||Y-X\hat \beta||}{n}=\frac{S_e^2}{n}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The maximizers for &lt;span class=&#34;math inline&#34;&gt;\(L(\beta,\sigma^2)\)&lt;/span&gt; over the sub-space &lt;span class=&#34;math inline&#34;&gt;\(\Theta_0=\{(\beta,\sigma^2)|\beta_i=0,i\ge 1,\sigma^2&amp;gt;0\}\)&lt;/span&gt; are&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat\beta^* = (\bar Y,0,\dots,0)^\top,\ \hat\sigma^{*2} = \frac{||Y-X\hat \beta^*||}{n}=\frac{S_T^2}{n}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The likelihood ratio is then given by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\lambda = \frac{\sup_{\theta\in\Theta}L(\beta,\sigma^2)}{\sup_{\theta\in\Theta_0}L(\beta,\sigma^2)} = \frac{L(\hat\beta,\hat\sigma^2)}{L(\hat\beta^*,\hat\sigma^{*2})}= \left(\frac{S_T^2}{S_e^2}\right)^{n/2}= \left(1+\frac{S_R^2}{S_e^2}\right)^{n/2}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;By Theorems 9 and 10, if the null is true we have
&lt;span class=&#34;math display&#34;&gt;\[F=\frac{S_R^2/(p-1)}{S_e^2/(n-p)}\sim F(p-1,n-p).\]&lt;/span&gt;
We take &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; as the test statistic. The rejection region is &lt;span class=&#34;math inline&#34;&gt;\(W=\{F&amp;gt;C\}\)&lt;/span&gt;, where the critical value &lt;span class=&#34;math inline&#34;&gt;\(C = F_{1-\alpha}(p-1,n-p)\)&lt;/span&gt; so that &lt;span class=&#34;math inline&#34;&gt;\(\sup_{\theta\in\Theta_0}P_\theta(F&amp;gt;C)=\alpha\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Definition&lt;/code&gt;: The &lt;strong&gt;coefficient of determination&lt;/strong&gt; is sometimes used as a crude measure of the strength of a relationship that has been
fit by least squares. This coefficient is defined as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[R^2 =\frac{S_R^2}{S_T^2}=\frac{\sum_{i=1}^n(\hat y_i-\bar y)^2}{\sum_{i=1}^n(y_i-\bar y)^2}.\]&lt;/span&gt;
It can be interpreted as the proportion of the variability of the dependent variable that
can be explained by the independent variables.&lt;/p&gt;
&lt;p&gt;It is easy to see that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[F = \frac{S_T^2 R^2/(p-1)}{S_T^2(1-R^2)/(n-p)}=\frac{ R^2/(p-1)}{(1-R^2)/(n-p)}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For the simple linear model &lt;span class=&#34;math inline&#34;&gt;\(p=2\)&lt;/span&gt;, we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[S_R^2 = \sum_{i=1}^n(\hat y_i-\bar y)^2 = \hat\beta_1^2\sum_{i=1}^n(x_i-\bar x)^2 = \frac{\ell_{xy}^2}{\ell_{xx}}.\]&lt;/span&gt;
This gives
&lt;span class=&#34;math display&#34;&gt;\[R^2 = \frac{\ell_{xy}^2}{\ell_{xx}\ell_{yy}} = \rho^2,\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\rho = \ell_{xy}/\sqrt{\ell_{xx}\ell_{yy}}\)&lt;/span&gt; is the &lt;strong&gt;correlation coefficient&lt;/strong&gt; between &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;case-study-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Case study 2&lt;/h2&gt;
&lt;p&gt;It is found that the systolic pressure is linked to the weight and the age. We now have the following data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;blood=data.frame(
weight=c(76.0,91.5,85.5,82.5,79.0,80.5,74.5,79.0,85.0,76.5,82.0,95.0,92.5),
age=c(50,20,20,30,30,50,60,50,40,55,40,40,20),
pressure=c(120,141,124,126,117,125,123,125,132,123,132,155,147))
knitr::kable(blood,format=&amp;quot;html&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
weight
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
age
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
pressure
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
76.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
120
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
91.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
141
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
85.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
124
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
82.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
30
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
126
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
79.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
30
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
117
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
80.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
125
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
74.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
123
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
79.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
125
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
85.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
132
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
76.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
55
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
123
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
82.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
132
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
95.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
155
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
92.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
147
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(blood)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/chap04_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm.blood=lm(pressure~weight+age,data=blood)
summary(lm.blood)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = pressure ~ weight + age, data = blood)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.0404 -1.0183  0.4640  0.6908  4.3274 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) -62.96336   16.99976  -3.704 0.004083 ** 
## weight        2.13656    0.17534  12.185 2.53e-07 ***
## age           0.40022    0.08321   4.810 0.000713 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 2.854 on 10 degrees of freedom
## Multiple R-squared:  0.9461, Adjusted R-squared:  0.9354 
## F-statistic: 87.84 on 2 and 10 DF,  p-value: 4.531e-07&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The regression function is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat y = -62.96336 + 2.13656 x_1+ 0.40022 x_2.\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n = length(blood$weight)
X = cbind(intercept=rep(1,n),weight=blood$weight,age=blood$age)
C = solve(t(X)%*%X)
SSE = sum(lm.blood$residuals^2) # sum of squared errors
# SST = var(blood$pressure)*(n-1)
# SSR = SST-SSE
# Fstat = SSR/(3-1)/(SSE/(n-3))
cov = SSE/(n-3)*C
knitr::kable(cov,format=&amp;quot;html&amp;quot;,caption = &amp;quot;Estimated Covariance Matrix&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-9&#34;&gt;Table 2: &lt;/span&gt;Estimated Covariance Matrix
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
intercept
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
weight
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
age
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
intercept
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
288.991861
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.9499280
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.1174334
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
weight
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.949928
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0307450
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0102176
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
age
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.117433
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0102176
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0069243
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Similarly to the simple linear model, we can construct the confidence intervals and prediction intervals for &lt;span class=&#34;math inline&#34;&gt;\(E[y]\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, respectively.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;newdata = data.frame(
        age = rep(31,100),
        weight = seq(70,100,length.out = 100)
)
CI = predict(lm.blood,newdata,interval = &amp;quot;confidence&amp;quot;)
Pred = predict(lm.blood,newdata,interval = &amp;quot;prediction&amp;quot;)
par(mar=c(4,4,2,1))
matplot(newdata$weight,cbind(CI,Pred[,-1]),type=&amp;quot;l&amp;quot;,lty = c(1,5,5,2,2),
        col=c(&amp;quot;blue&amp;quot;,&amp;quot;red&amp;quot;,&amp;quot;red&amp;quot;,&amp;quot;brown&amp;quot;,&amp;quot;brown&amp;quot;),lwd=2,
        xlab=&amp;quot;Weight&amp;quot;,ylab=&amp;quot;Pressure&amp;quot;,main = &amp;quot;Age = 31&amp;quot;)
legend(70,160,c(&amp;quot;Fitted&amp;quot;,&amp;quot;Confidence&amp;quot;,&amp;quot;Prediction&amp;quot;),
       lty = c(1,5,2),col=c(&amp;quot;blue&amp;quot;,&amp;quot;red&amp;quot;,&amp;quot;brown&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/chap04_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;newdata = data.frame(
        weight = rep(85,41),
        age = seq(20,60)
)
CI = predict(lm.blood,newdata,interval = &amp;quot;confidence&amp;quot;)
Pred = predict(lm.blood,newdata,interval = &amp;quot;prediction&amp;quot;)
par(mar=c(4,4,2,1))
matplot(newdata$age,cbind(CI,Pred[,-1]),type=&amp;quot;l&amp;quot;,lty = c(1,5,5,2,2),
        col=c(&amp;quot;blue&amp;quot;,&amp;quot;red&amp;quot;,&amp;quot;red&amp;quot;,&amp;quot;brown&amp;quot;,&amp;quot;brown&amp;quot;),lwd=2,
        xlab=&amp;quot;Age&amp;quot;,ylab=&amp;quot;Pressure&amp;quot;,main = &amp;quot;Weight = 85&amp;quot;)
legend(20,150,c(&amp;quot;Fitted&amp;quot;,&amp;quot;Confidence&amp;quot;,&amp;quot;Prediction&amp;quot;),
       lty = c(1,5,2),col=c(&amp;quot;blue&amp;quot;,&amp;quot;red&amp;quot;,&amp;quot;brown&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/chap04_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>第三章：假设检验</title>
      <link>/post/chap03/</link>
      <pubDate>Wed, 07 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/chap03/</guid>
      <description>&lt;div id=&#34;contents&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Contents&lt;/h2&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;1. 假设检验基本概念&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;1.1 检验问题的提出&lt;/li&gt;
&lt;li&gt;1.2 检验法则与两类错误&lt;/li&gt;
&lt;li&gt;1.3 检验水平与功效&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;2. 似然比检验&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;2.1 似然比检验法原理&lt;/li&gt;
&lt;li&gt;2.2 Neyman-Pearson引理&lt;/li&gt;
&lt;li&gt;2.3 单参数指数型的假设检验&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;3. 广义似然比检验&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;3.1 广义似然比检验法原理&lt;/li&gt;
&lt;li&gt;3.2 单个正态总体的假设检验&lt;/li&gt;
&lt;li&gt;3.3 两个独立正态总体的假设检验&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;4. 拟合优度检验&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;4.1 卡方检验法&lt;/li&gt;
&lt;li&gt;4.2 独立性检验&lt;/li&gt;
&lt;li&gt;4.3 柯尔莫哥洛夫检验法&lt;/li&gt;
&lt;li&gt;4.4 正态性检验&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;case-study&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Case study&lt;/h2&gt;
&lt;p&gt;奶茶是由牛奶与茶按一定比例混合而成，可以先倒茶后加奶，也可以先倒奶再倒茶。某女士声称她可以鉴别这两种混合方式，周围品茶的人对此产生了议论，都觉得不可思议。在场的费希尔也在思考这个问题，他提议做一项试验来检验如下命题是否可以接受：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;假设H: 该女士无此种鉴别能力&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;他准备了10杯调好的奶茶（两种顺序的都有）给该女士鉴别，结果那位女士竟然能够正确地分辨出10杯奶茶中的每一杯的调制顺序。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如何做出你的判断？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;假如该女士只猜对了9杯（或者8杯），又该如何判断？&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-examples&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Some examples&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;产品的次品率是否不超过&lt;span class=&#34;math inline&#34;&gt;\(3\%\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;男生群体平均身高是否大于女生群体平均身高？&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;身高是否服从正态分布？&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;抽烟与慢性支气管炎是否有关？&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;basis-concepts-for-ht&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Basis concepts for HT&lt;/h2&gt;
&lt;p&gt;设有来自某一参数分布族&lt;span class=&#34;math inline&#34;&gt;\(\{F(x,\theta)\}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\theta\in\Theta\)&lt;/span&gt;, 其中&lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt;为参数空间。&lt;/p&gt;
&lt;p&gt;原假设（零假设, Null Hypothesis）&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0:\theta \in \Theta_0\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;备选假设（对立假设，备择假设, Alternative Hypothesis）&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_1:\theta \in \Theta_1\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;span class=&#34;math inline&#34;&gt;\(\varnothing\neq \Theta_0,\Theta_1\subset \Theta,\Theta_0\cap \Theta_1=\varnothing\)&lt;/span&gt;. 最常见的情况&lt;span class=&#34;math inline&#34;&gt;\(\Theta_1=\Theta-\Theta_0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;简单原假设(simple null)：&lt;span class=&#34;math inline&#34;&gt;\(\Theta_0\)&lt;/span&gt;只包含一个点，如&lt;span class=&#34;math inline&#34;&gt;\(H_0:\theta=\theta_0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;复杂原假设(composite null)：&lt;span class=&#34;math inline&#34;&gt;\(\Theta_0\)&lt;/span&gt;只包含多个点，如&lt;span class=&#34;math inline&#34;&gt;\(H_0:\theta\le \theta_0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;备选假设&lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt;通常有三种可能：&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;双边(two-sided)：&lt;span class=&#34;math inline&#34;&gt;\(H_1:\theta\neq \theta_0\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;单边(one-sided)：&lt;span class=&#34;math inline&#34;&gt;\(H_1:\theta&amp;gt; \theta_0\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;单边(one-sided)：&lt;span class=&#34;math inline&#34;&gt;\(H_1:\theta&amp;lt; \theta_0\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;the-decision-rule-for-ht&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The decision rule for HT&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;检验法则&lt;/strong&gt;：检验本质上是把样本空间划分成两个互不相交的部分&lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(\bar{W}\)&lt;/span&gt;, 当样本属于&lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;时就拒绝&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;; 否则接受&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;. 称&lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;为该检验的&lt;em&gt;拒绝域(rejection region)&lt;/em&gt;，而&lt;span class=&#34;math inline&#34;&gt;\(\bar W\)&lt;/span&gt;为&lt;em&gt;接受域(acceptance region)&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;例&lt;/strong&gt;：假设总体为&lt;span class=&#34;math inline&#34;&gt;\(N(\theta,1)\)&lt;/span&gt;, 样本为&lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt;. 考虑检验问题&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0:\theta=\theta_0\ vs. \ H_1:\theta\neq\theta_0\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;拒绝域可以设为：&lt;span class=&#34;math inline&#34;&gt;\(W=\{(x_1,\dots,x_n):|\bar x-\theta_0|\ge c\}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;一个检验法则对应一种拒绝域；一个拒绝域决定一种检验法则。&lt;strong&gt;假设检验的目标是在给定准则下选取合适的拒绝域&lt;/strong&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;原假设&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;在客观上只有“真”和“假”两种可能&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;样本也只有两种可能性：&lt;span class=&#34;math inline&#34;&gt;\((X_1,\dots,X_n)\in W\)&lt;/span&gt; 或者 &lt;span class=&#34;math inline&#34;&gt;\((X_1,\dots,X_n)\notin W\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;由于样本的随机性，检验不可能&lt;span class=&#34;math inline&#34;&gt;\(100\%\)&lt;/span&gt;正确&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;two-types-of-errors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Two types of errors&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;接受原假设&lt;/th&gt;
&lt;th&gt;拒绝原假设&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;原假设为真&lt;/td&gt;
&lt;td&gt;正确&lt;/td&gt;
&lt;td&gt;第一类（拒真, Type I）错误&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;备择假设为真&lt;/td&gt;
&lt;td&gt;第二类（纳伪, Type II）错误&lt;/td&gt;
&lt;td&gt;正确&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;a-toy-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A toy example&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;例&lt;/strong&gt;：假设总体为&lt;span class=&#34;math inline&#34;&gt;\(N(\theta,1)\)&lt;/span&gt;, 样本为&lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt;. 考虑简单的检验问题(simple hypothesis)&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0:\theta=\theta_1\ vs. \ H_1:\theta=\theta_2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;span class=&#34;math inline&#34;&gt;\(\theta_1\neq\theta_2\)&lt;/span&gt;. 拒绝域选择为&lt;span class=&#34;math inline&#34;&gt;\(W=\{(x_1,\dots,x_n):|\bar x-\theta_1|\ge c\}\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;犯第一类错误的概率为&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\alpha = P_{\theta_1}(|\bar{X}-\theta_1|\ge c)=2-2\Phi(c\sqrt{n})\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;犯第二类错误的概率为&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\beta = P_{\theta_2}(|\bar{X}-\theta_1|&amp;lt; c)=\Phi((\theta_1-\theta_2+c)\sqrt{n})-\Phi((\theta_1-\theta_2-c)\sqrt{n})\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;变小，则&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;增大，于是&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;变大。&lt;/li&gt;
&lt;li&gt;如果&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;变小，则&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;减小，于是&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;变大。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;结论&lt;/strong&gt;：在样本量不变的前提下，两类错误不能同时减小。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;significance-level-and-power&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Significance level and power&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;拒绝域的选取准则&lt;/strong&gt;：在保证犯第一类错误的概率不超过一定水平的前提下，选择犯第二类错误的概率尽可能小的拒绝域&lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;功效函数(power function)&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\rho_W(\theta):=P_{\theta}((X_1,\dots,X_n)\in W)\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;当&lt;span class=&#34;math inline&#34;&gt;\(\theta\in \Theta_0\)&lt;/span&gt;时，&lt;span class=&#34;math inline&#34;&gt;\(\rho_W(\theta)\)&lt;/span&gt;表示犯第一类错误的概率&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;当&lt;span class=&#34;math inline&#34;&gt;\(\theta\in \Theta_1\)&lt;/span&gt;时，&lt;span class=&#34;math inline&#34;&gt;\(1-\rho_W(\theta)\)&lt;/span&gt;表示犯第二类错误的概率，&lt;span class=&#34;math inline&#34;&gt;\(\rho_W(\theta)\)&lt;/span&gt;表示检验的功效(power of a test)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;的&lt;strong&gt;检验水平&lt;/strong&gt;（显著性水平，level of significance）：
&lt;span class=&#34;math display&#34;&gt;\[\sup_{\theta\in\Theta_0} \rho_W(\theta)= \alpha\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一般取&lt;span class=&#34;math inline&#34;&gt;\(\alpha=0.1,0.05,0.01\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;小概率原理&lt;/strong&gt;：小概率事件在一次试验中是几乎不发生的。若&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;为真，样本落在拒绝域&lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;是小概率事件，不应发生。如发生，则拒绝原假设&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-choose-the-significance-level&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How to choose the significance level?&lt;/h2&gt;
&lt;p&gt;人们自然会产生这样的问题：概率小到什么程度才当作“小概率事件”呢？这要据实际情况而定，例如即使下雨的概率为10%，仍有人会因为它太小而不带雨具。但某航空公司的事故率为1%，人们就会因为它太大而不敢乘坐该公司的飞机，通常把概率不超过0.05 (或0.01)的事件当作“小概率事件(rare event)”。为此在假设检验时，必须先确定小概率即显著性的值&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; (即不超过&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;的概率认为是小概率)。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ump-tests&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;UMP tests&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;定义&lt;/strong&gt;：称&lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;为检验水平&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;的一致最大功效(uniformly most powerful, UMP)的拒绝域，若&lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;的水平为&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;且对一切水平不超过&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;的拒绝域&lt;span class=&#34;math inline&#34;&gt;\(W&amp;#39;\)&lt;/span&gt;均有
&lt;span class=&#34;math display&#34;&gt;\[\rho_W(\theta)\ge \rho_{W&amp;#39;}(\theta),\ \forall \theta\in \Theta_1\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;定义&lt;/strong&gt;：称&lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;为检验水平&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;的无偏拒绝域，若&lt;span class=&#34;math inline&#34;&gt;\(\forall \theta\in \Theta_1\)&lt;/span&gt;, 有&lt;span class=&#34;math inline&#34;&gt;\(\rho_W(\theta)\ge \alpha\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;定义&lt;/strong&gt;：称&lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;为检验水平&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;的一致最大功效无偏(uniformly most powerful unbiased, UMPU)的拒绝域，若&lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;是水平为&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;的无偏拒绝域且对一切水平不超过&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;的无偏拒绝域&lt;span class=&#34;math inline&#34;&gt;\(W&amp;#39;\)&lt;/span&gt;均有
&lt;span class=&#34;math display&#34;&gt;\[\rho_W(\theta)\ge \rho_{W&amp;#39;}(\theta),\ \forall \theta\in \Theta_1\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;UMP意味着在犯第一类错误的概率不超过&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;的前提下，犯第二类错误的概率最小&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;case-study-two-coins&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Case study: two coins&lt;/h2&gt;
&lt;p&gt;Suppose that I
have two coins, coin 0 has probability of heads equal to &lt;strong&gt;0.5&lt;/strong&gt; and coin 1 has probability
of heads equal to &lt;strong&gt;0.7&lt;/strong&gt;. I choose one of the coins, &lt;strong&gt;toss it 10 times&lt;/strong&gt; and tell you the
number of heads, but do not tell you whether it was coin 0 or coin 1. On the basis
of the number of heads, your task is to decide which coin it was. How should your
decision rule be?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;let &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; be the number of heads&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;let &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; be the coin that produce this result.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;your test might be: &lt;span class=&#34;math inline&#34;&gt;\(H_0: \theta=0\ vs.\ \theta=1\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-probability-mass-function-pmf&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The probability mass function (PMF)&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;2coins.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-decision-rule&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The decision rule&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Suppose that you observed two heads. Then &lt;span class=&#34;math inline&#34;&gt;\(P_0(2)/P_1(2)\approx 30\)&lt;/span&gt;, which we
will call the &lt;strong&gt;likelihood ratio (LR)&lt;/strong&gt;—coin 0 was about 30 times more likely to produce this
result than was coin 1. This result would favor coin 0.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;On the other hand, if there were
8 heads, the likelihood ratio would be &lt;span class=&#34;math inline&#34;&gt;\(P_0(8)/P_1(8)\approx 0.19\)&lt;/span&gt;, which would favor coin 1.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The likelihood ratio will play a central role in the procedures we develop.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;likelihood-ratio-tests&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Likelihood ratio tests&lt;/h2&gt;
&lt;p&gt;考虑最简单的假设检验&lt;span class=&#34;math inline&#34;&gt;\((\theta_1\neq \theta_2)\)&lt;/span&gt;：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0: \theta=\theta_1\ vs.\ \theta=\theta_2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;令&lt;span class=&#34;math inline&#34;&gt;\(\vec x=(x_1,\dots,x_n)\)&lt;/span&gt;, 似然比检验的拒绝域为：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[W=\{\vec x:\frac{L(\vec x;\theta_2)}{L(\vec x;\theta_1)}&amp;gt; \lambda\}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;span class=&#34;math inline&#34;&gt;\(\lambda\ge 0\)&lt;/span&gt;满足&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P_{\theta_1}((X_1,\dots,X_n)\in W)=\int_W L(\vec x;\theta_1)d\vec x=\alpha\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Neyman-Pearson Lemma&lt;/strong&gt;(课本P68)：上述定义的似然比检验&lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;是一致最大功效的&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;定理&lt;/strong&gt;：上述似然比拒绝域&lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;是无偏的(unbiased)，即&lt;span class=&#34;math inline&#34;&gt;\(\rho_{W}(\theta_2)\ge \alpha\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ump-tests-for-normal-mean&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;UMP tests for normal mean&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;例&lt;/strong&gt;：假设总体为&lt;span class=&#34;math inline&#34;&gt;\(N(\mu,\sigma^2)\)&lt;/span&gt;, 其中&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;已知， 样本为&lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt;. 考虑检验水平为&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;的检验问题&lt;span class=&#34;math inline&#34;&gt;\((\mu_2&amp;gt;\mu_1)\)&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0:\mu=\mu_1\ vs. \ H_1:\mu=\mu_2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;解&lt;/strong&gt;：似然比检验的拒绝域为：&lt;span class=&#34;math inline&#34;&gt;\(W=\{\vec x:\frac{L(\vec x;\mu_2)}{L(\vec x;\mu_1)}&amp;gt; \lambda\}\)&lt;/span&gt;. 令似然比
&lt;span class=&#34;math display&#34;&gt;\[\frac{L(X_1,\dots,X_n;\mu_2)}{L(X_1,\dots,X_n;\mu_1)}=\prod_{i=1}^n\frac{f(X_i;\mu_2,\sigma^2)}{f(X_i;\mu_1,\sigma^2)}=e^{\frac{n(\mu_2-\mu_1)(2\bar X-\mu_1-\mu_2)}{2\sigma^2}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;等价于找&lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;, 使得&lt;span class=&#34;math inline&#34;&gt;\(P_{\mu_1}(\bar X&amp;gt; C)=\alpha\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(C=\mu_1+u_{1-\alpha}\sigma/\sqrt{n}\)&lt;/span&gt;, 所以似然比检验的拒绝域为&lt;span class=&#34;math inline&#34;&gt;\(W=\{\vec x:\bar x&amp;gt; \mu_1+u_{1-\alpha}\sigma/\sqrt{n}\}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;与双侧拒绝域比较：&lt;span class=&#34;math inline&#34;&gt;\(W&amp;#39;=\{\vec x:|\bar x-\mu_1|&amp;gt; u_{1-\alpha/2}\sigma/\sqrt{n}\}\)&lt;/span&gt;, &lt;strong&gt;哪个好？&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;comparison&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comparison&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a = 1
b = 2
x = seq(-4,4,by=0.01)+2
y1 = dnorm(x,a,1)
y2 = dnorm(x,b,1)
par(mfrow=c(2,1),mar=c(2,2,2,1))
plot(x,y1,type=&amp;quot;l&amp;quot;,xlab=&amp;quot;&amp;quot;,ylab=&amp;quot;&amp;quot;,main=&amp;quot;H_0&amp;quot;)
abline(v=c(a+qnorm(0.9),a+qnorm(0.95),a-qnorm(0.95)),col = c(&amp;quot;red&amp;quot;,&amp;quot;green&amp;quot;,&amp;quot;green&amp;quot;),lty=2,lwd=3)
plot(x,y2,type=&amp;quot;l&amp;quot;,xlab=&amp;quot;&amp;quot;,ylab=&amp;quot;&amp;quot;,main=&amp;quot;H_1&amp;quot;)
abline(v=c(a+qnorm(0.9),a+qnorm(0.95),a-qnorm(0.95)),col = c(&amp;quot;red&amp;quot;,&amp;quot;green&amp;quot;,&amp;quot;green&amp;quot;),lty=2,lwd=3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/chap03_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ump-tests-for-normal-mean-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;UMP tests for normal mean&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;例&lt;/strong&gt;：假设总体为&lt;span class=&#34;math inline&#34;&gt;\(N(\mu,\sigma^2)\)&lt;/span&gt;, 其中&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;已知， 样本为&lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt;. 考虑检验水平为&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;的检验问题&lt;span class=&#34;math inline&#34;&gt;\((\mu_2&amp;lt;\mu_1)\)&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0:\mu=\mu_1\ vs. \ H_1:\mu=\mu_2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;解&lt;/strong&gt;：似然比检验的拒绝域为：&lt;span class=&#34;math inline&#34;&gt;\(W=\{\vec x:\frac{L(\vec x;\mu_2)}{L(\vec x;\mu_1)}&amp;gt; \lambda\}\)&lt;/span&gt;. 令似然比
&lt;span class=&#34;math display&#34;&gt;\[\frac{L(X_1,\dots,X_n;\mu_2)}{L(X_1,\dots,X_n;\mu_1)}=\prod_{i=1}^n\frac{f(X_i;\mu_2,\sigma^2)}{f(X_i;\mu_1,\sigma^2)}=e^{\frac{n(\mu_2-\mu_1)(2\bar X-\mu_1-\mu_2)}{2\sigma^2}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;等价于找&lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;, 使得&lt;span class=&#34;math inline&#34;&gt;\(P_{\mu_1}(\bar X&amp;lt; C)=\alpha\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(C=\mu_1+u_{\alpha}\sigma/\sqrt{n}\)&lt;/span&gt;, 所以似然比检验的拒绝域为&lt;span class=&#34;math inline&#34;&gt;\(W=\{\vec x:\bar x&amp;lt; \mu_1+u_{\alpha}\sigma/\sqrt{n}\}\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ump-tests-for-normal-mean-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;UMP tests for normal mean&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;例&lt;/strong&gt;：假设总体为&lt;span class=&#34;math inline&#34;&gt;\(N(\mu,\sigma^2)\)&lt;/span&gt;, 其中&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;已知， 样本为&lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt;. 分别求以下假设检验的一致最大功效拒绝域：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0:\mu=\mu_1\ vs. \ H_1:\mu&amp;gt;\mu_1\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0:\mu\le\mu_1\ vs. \ H_1:\mu&amp;gt;\mu_1\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0:\mu=\mu_1\ vs. \ H_1:\mu&amp;lt;\mu_1\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0:\mu\ge\mu_1\ vs. \ H_1:\mu&amp;lt;\mu_1\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;解&lt;/strong&gt;：前面两种情况的UMP拒绝域为&lt;span class=&#34;math display&#34;&gt;\[W=\{\vec x:\bar x&amp;gt; \mu_1+u_{1-\alpha}\sigma/\sqrt{n}\}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;后面两种情况的UMP拒绝域为&lt;span class=&#34;math display&#34;&gt;\[W=\{\vec x:\bar x&amp;lt; \mu_1+u_{\alpha}\sigma/\sqrt{n}\}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;single-parameter-exponential-families&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Single-parameter exponential families&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;定义&lt;/strong&gt;：设&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;的可能的集合为&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X}\)&lt;/span&gt;. 称&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;服从单参数指数型分布(single-parameter exponential family)，若&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;的密度函数（或者分布列）有下列表达式&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f(x;\theta) =  S(\theta)h(x)\exp(Q(\theta)V(x))\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;span class=&#34;math inline&#34;&gt;\(\theta\in\Theta=(a,b),-\infty\le a&amp;lt;b\le \infty,S(\theta)&amp;gt;0,x\in \mathcal{X},h(x)&amp;gt;0,Q(\theta)\)&lt;/span&gt;是&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的&lt;em&gt;严格增函数&lt;/em&gt;。常见的分布都是指数型分布，比如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;指数分布：&lt;span class=&#34;math inline&#34;&gt;\(f(x;\lambda)=\lambda e^{-\lambda x}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(Q(\lambda)=\lambda\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(V(x)=-x\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Poisson分布：&lt;span class=&#34;math inline&#34;&gt;\(f(x;\lambda)=\frac{e^{-\lambda}\lambda^x}{x!}=\frac{e^{-\lambda}e^{\log(\lambda) x}}{x!}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(Q(\lambda)=\log(\lambda)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(V(x)=x\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;正态分布(&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;已知)：&lt;span class=&#34;math inline&#34;&gt;\(f(x;\mu) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(Q(\mu)=\mu/\sigma^2\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(V(x)=x\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;正态分布(&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;已知)：&lt;span class=&#34;math inline&#34;&gt;\(Q(\sigma^2)=-\frac{1}{2\sigma^2}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(V(x)=(x-\mu)^2\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-tests&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The tests&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0:\theta\le \theta_1\ vs.\ H_1:\theta&amp;gt;\theta_1\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0:\theta\ge \theta_1\ vs.\ H_1:\theta&amp;lt;\theta_1\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0:\theta\notin (\theta_1,\theta_2)\ vs.\ H_1:\theta\in (\theta_1,\theta_2)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0:\theta\in [\theta_1,\theta_2]\ vs.\ H_1:\theta\notin [\theta_1,\theta_2]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0:\theta=\theta_0\ vs.\ H_1:\theta\neq\theta_0\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ump-tests-i&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;UMP tests I&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;定理&lt;/strong&gt;：考虑上述单参数指数型分布，给定检验问题&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0:\theta\le \theta_1\ vs.\ H_1:\theta&amp;gt;\theta_1\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;对&lt;span class=&#34;math inline&#34;&gt;\(\alpha\in(0,1)\)&lt;/span&gt;, 若存在&lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;满足&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P_{\theta_1}\left(\sum_{i=1}^n V(X_i)&amp;gt;C\right)=\alpha,\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;则检验水平为&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;的一致最大功效的拒绝域为：&lt;span class=&#34;math display&#34;&gt;\[W=\{\vec x:\sum_{i=1}^n V(x_i)&amp;gt;C\}.\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ump-tests-ii&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;UMP tests II&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;定理&lt;/strong&gt;：考虑上述单参数指数型分布，给定检验问题&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0:\theta\ge \theta_1\ vs.\ H_1:\theta&amp;lt;\theta_1\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;对&lt;span class=&#34;math inline&#34;&gt;\(\alpha\in(0,1)\)&lt;/span&gt;, 若存在&lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;满足&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P_{\theta_1}\left(\sum_{i=1}^n V(X_i)&amp;lt;C\right)=\alpha,\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;则检验水平为&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;的一致最大功效的拒绝域为：&lt;span class=&#34;math display&#34;&gt;\[W=\{\vec x:\sum_{i=1}^n V(x_i)&amp;lt;C\}.\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ump-tests-for-normal-mean-3&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;UMP tests for normal mean&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;例1&lt;/strong&gt;：假设总体为&lt;span class=&#34;math inline&#34;&gt;\(N(\mu,\sigma^2)\)&lt;/span&gt;, 其中&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;已知， 样本为&lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt;. 求下列检验的UMP&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0:\mu\le\mu_1\ vs. \ H_1:\mu&amp;gt;\mu_1\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;解&lt;/strong&gt;：因为在指数分布族形式中&lt;span class=&#34;math inline&#34;&gt;\(V(x)=x\)&lt;/span&gt;, UMP拒绝域为&lt;span class=&#34;math display&#34;&gt;\[W=\{\vec x:\sum_{i=1}^nx_i&amp;gt;C\}=\{\vec x:\bar x&amp;gt;C&amp;#39;\},\]&lt;/span&gt; 其中&lt;span class=&#34;math inline&#34;&gt;\(C&amp;#39;\)&lt;/span&gt;满足
&lt;span class=&#34;math inline&#34;&gt;\(P_{\mu_1}(\bar X&amp;gt;C&amp;#39;)=\alpha\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(C&amp;#39;=\mu_1+u_{1-\alpha}\sigma/\sqrt{n}\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ump-tests-for-normal-variance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;UMP tests for normal variance&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;例2&lt;/strong&gt;：假设总体为&lt;span class=&#34;math inline&#34;&gt;\(N(\mu,\sigma^2)\)&lt;/span&gt;, 其中&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;已知， 样本为&lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt;. 求下列检验的UMP&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0:\sigma^2\le\sigma^2_1\ vs. \ H_1:\sigma^2&amp;gt;\sigma^2_1\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;解&lt;/strong&gt;：因为在指数分布族形式中&lt;span class=&#34;math inline&#34;&gt;\(V(x)=(x-\mu)^2\)&lt;/span&gt;, UMP拒绝域为
&lt;span class=&#34;math display&#34;&gt;\[W=\{\vec x:\sum_{i=1}^n(x_i-\mu)^2&amp;gt;C\}=\{\vec x:\sum_{i=1}^n\frac{(x_i-\mu)^2}{\sigma^2_1}&amp;gt;C&amp;#39;\},\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;span class=&#34;math inline&#34;&gt;\(C&amp;#39;\)&lt;/span&gt;满足&lt;span class=&#34;math inline&#34;&gt;\(P_{\sigma^2_1}(\sum_{i=1}^n\frac{(x_i-\mu)^2}{\sigma^2_1}&amp;gt;C&amp;#39;)=\alpha\)&lt;/span&gt;, 所以&lt;span class=&#34;math inline&#34;&gt;\(C&amp;#39;=\chi^2_{1-\alpha}(n)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ump-tests-iii&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;UMP tests III&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;定理&lt;/strong&gt;：考虑上述单参数指数型分布，给定检验问题&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0:\theta\notin (\theta_1,\theta_2)\ vs.\ H_1:\theta\in (\theta_1,\theta_2)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;令
&lt;span class=&#34;math display&#34;&gt;\[W=\{\vec x:C_1&amp;lt;\sum_{i=1}^n V(x_i)&amp;lt;C_2\}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;若存在&lt;span class=&#34;math inline&#34;&gt;\(C_1,C_2\)&lt;/span&gt;满足
&lt;span class=&#34;math display&#34;&gt;\[P_{\theta_1}(\vec X\in W)=P_{\theta_2}(\vec X\in W)=\alpha\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;则检验水平为&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;的一致最大功效的拒绝域为&lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;umpu-tests-i&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;UMPU tests I&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;定理&lt;/strong&gt;：考虑上述单参数指数型分布，给定检验问题&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0:\theta\in [\theta_1,\theta_2]\ vs.\ H_1:\theta\notin [\theta_1,\theta_2]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;令&lt;span class=&#34;math display&#34;&gt;\[W=\{\vec x:\sum_{i=1}^n V(x_i)\notin[C_1,C_2]\}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;若存在&lt;span class=&#34;math inline&#34;&gt;\(C_1,C_2\)&lt;/span&gt;满足
&lt;span class=&#34;math display&#34;&gt;\[P_{\theta_1}(\vec X\in W)=P_{\theta_2}(\vec X\in W)=\alpha\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;则检验水平为&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;的一致最大功效无偏(UMPU)的拒绝域为&lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;umpu-tests-ii&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;UMPU tests II&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;定理&lt;/strong&gt;：考虑上述单参数指数型分布，给定检验问题&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0:\theta=\theta_0\ vs.\ H_1:\theta\neq\theta_0\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;令&lt;span class=&#34;math inline&#34;&gt;\(W=\{\vec x:\sum_{i=1}^n V(x_i)\notin[C_1,C_2]\}.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;若存在&lt;span class=&#34;math inline&#34;&gt;\(C_1,C_2\)&lt;/span&gt;满足&lt;br /&gt;
&lt;span class=&#34;math display&#34;&gt;\[P_{\theta_0}(\vec X\in W)=\alpha\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[E_{\theta_0}\left[1\{\vec X\in W\}\sum_{i=1}^n V(X_i)\right]=\alpha E_{\theta_0}\left[\sum_{i=1}^n V(X_i)\right]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;则检验水平为&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;的一致最大功效无偏(UMPU)的拒绝域为&lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;umpu-tests-ii-continued&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;UMPU tests II (continued)&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;推论&lt;/strong&gt;：考虑上述单参数指数型分布，给定检验问题&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0:\theta=\theta_0\ vs.\ H_1:\theta\neq\theta_0\]&lt;/span&gt;
如果在&lt;span class=&#34;math inline&#34;&gt;\(\theta=\theta_0\)&lt;/span&gt;下，&lt;span class=&#34;math inline&#34;&gt;\(T(\vec X) = \sum_{i=1}^n V(X_i)\)&lt;/span&gt;的分布关于某数&lt;span class=&#34;math inline&#34;&gt;\(r_0\)&lt;/span&gt;对称，取&lt;span class=&#34;math display&#34;&gt;\[W=\{\vec x:|T(\vec x)-r_0|&amp;gt;C\}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;若存在&lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;满足&lt;br /&gt;
&lt;span class=&#34;math display&#34;&gt;\[P_{\theta_0}(\vec X\in W)=\alpha,\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;则检验水平为&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;的一致最大功效无偏(UMPU)的拒绝域为&lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;two-sided-tests-for-normal-mean&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Two-sided tests for normal mean&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;例&lt;/strong&gt;：假设总体为&lt;span class=&#34;math inline&#34;&gt;\(N(\mu,\sigma^2)\)&lt;/span&gt;, 其中&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;已知， 样本为&lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt;. 求下列检验的UMPU拒绝域&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0:\mu=\mu_0\ vs. \ H_1:\mu\neq\mu_0\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;解&lt;/strong&gt;：因为在指数分布族形式中&lt;span class=&#34;math inline&#34;&gt;\(V(x)=x\)&lt;/span&gt;, 此时&lt;span class=&#34;math inline&#34;&gt;\(T(\vec X)=\sum_{i=1}^nX_i\)&lt;/span&gt;.
在&lt;span class=&#34;math inline&#34;&gt;\(\mu=\mu_0\)&lt;/span&gt;下，&lt;span class=&#34;math inline&#34;&gt;\(T(\vec X)\sim N(n\mu_0,n\sigma^2)\)&lt;/span&gt;, 故其分布关于&lt;span class=&#34;math inline&#34;&gt;\(r_0=n\mu_0\)&lt;/span&gt;对称，&lt;/p&gt;
&lt;p&gt;UMPU拒绝域为
&lt;span class=&#34;math display&#34;&gt;\[W=\{\vec x:|T(\vec x)-n\mu_0|&amp;gt;C\}=\{\vec x:|\bar x-\mu_0|&amp;gt;C&amp;#39;\},\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;span class=&#34;math inline&#34;&gt;\(C&amp;#39;\)&lt;/span&gt;满足&lt;span class=&#34;math inline&#34;&gt;\(P_{\mu_0}(|\bar X-\mu_0|&amp;gt;C&amp;#39;)=\alpha\)&lt;/span&gt;, 所以&lt;span class=&#34;math inline&#34;&gt;\(C&amp;#39;=u_{1-\alpha/2}\sigma/\sqrt{n}\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;two-sided-tests-for-normal-variance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Two-sided tests for normal variance&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;例&lt;/strong&gt;：假设总体为&lt;span class=&#34;math inline&#34;&gt;\(N(\mu,\sigma^2)\)&lt;/span&gt;, 其中&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;已知， 样本为&lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt;. 求下列检验的UMPU拒绝域&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0:\sigma^2=\sigma_0^2\ vs. \ H_1:\sigma^2\neq\sigma_0^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;解&lt;/strong&gt;：为方便起见，令&lt;span class=&#34;math inline&#34;&gt;\(T(\vec X)=\sum_{i=1}^n(X_i-\mu)^2/\sigma_0^2\)&lt;/span&gt;. 这样，
在&lt;span class=&#34;math inline&#34;&gt;\(\mu=\mu_0\)&lt;/span&gt;下，&lt;span class=&#34;math inline&#34;&gt;\(T(\vec X)\sim \chi^2(n)\)&lt;/span&gt;. UMPU拒绝域表示为
&lt;span class=&#34;math inline&#34;&gt;\(W=\{\vec x:T(\vec x)\notin [C_1,C_2]\}\)&lt;/span&gt;，其中&lt;span class=&#34;math inline&#34;&gt;\(C_1,C_2\)&lt;/span&gt;满足
&lt;span class=&#34;math display&#34;&gt;\[P_{\sigma^2_0}(T(\vec X)\notin W)=\int_{C_1}^{C_2} f(x;n) dx=1-\alpha\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[E_{\sigma^2_0}[1\{\vec X\notin W\}T(\vec X)]=\int_{C_1}^{C_2} x f(x;n)dx=(1-\alpha)E_{\sigma_0^2}[T(\vec X)]=n(1-\alpha)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;two-sided-tests-for-normal-variance-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Two-sided tests for normal variance&lt;/h2&gt;
&lt;p&gt;其中，&lt;span class=&#34;math inline&#34;&gt;\(f(x;n)\)&lt;/span&gt;为&lt;span class=&#34;math inline&#34;&gt;\(\chi^2(n)\)&lt;/span&gt;的密度函数，即&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f(x;n)=\frac{1}{2^{n/2}\Gamma(n/2)}x^{n/2-1}e^{-x/2}1\{x&amp;gt;0\}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\int_{C_1}^{C_2} \frac{x}{n} f(x;n)dx=\int_{C_1}^{C_2} \frac{x}{n} \frac{1}{2^{n/2}\Gamma(n/2)}x^{n/2-1}e^{-x/2}dx=\int_{C_1}^{C_2} f(x;n+2)dx=\int_{C_1}^{C_2} f(x;n) dx=1-\alpha\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;实际上，求解&lt;span class=&#34;math inline&#34;&gt;\(C_1,C_2\)&lt;/span&gt;比较困难，为方便起见，不妨用平均法取&lt;span class=&#34;math inline&#34;&gt;\(C_1=\chi_{\alpha/2}^2(n), C_2=\chi_{1-\alpha/2}^2(n)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;我们已经在单参数指数型分布总体下给出常见的假设检验的UMP/UMPU, 具体步骤可以归纳如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;根据指数型分布写出&lt;strong&gt;检验统计量(test statistic)&lt;/strong&gt;&lt;span class=&#34;math inline&#34;&gt;\(T(\vec X)=\sum_{i=1}^nV(X_i)\)&lt;/span&gt;，或者它的常数倍&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;根据假设检验的类型写出拒绝域&lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;的形式，一般有&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[T(\vec x)&amp;gt;C,T(\vec x)&amp;lt;C,T(\vec x)\in (C_1,C_2),T(\vec x)\notin [C_1,C_2]\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果是只有一个待定参数&lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;时，可以检验水平&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;来得到&lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;的值，即&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P_{\theta_0}(T(\vec X)\in W)=\alpha,\  \theta_0\text{为}\Theta_0\text{的边界点}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果有两个待定参数&lt;span class=&#34;math inline&#34;&gt;\(C_1,C_2\)&lt;/span&gt;时, 可能还需另外一个等式来求解（比如双边假设检验）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;正态总体的期望的检验统计量为&lt;span class=&#34;math inline&#34;&gt;\(n\bar X\)&lt;/span&gt;或者&lt;span class=&#34;math inline&#34;&gt;\(\bar X\)&lt;/span&gt;, 称为&lt;strong&gt;U检验(U test)&lt;/strong&gt;；方差的检验统计量为&lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^n(X_i-\mu)^2\)&lt;/span&gt;, 称为&lt;strong&gt;卡方检验(chi-square test)&lt;/strong&gt;。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;generalized-lr-tests&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Generalized LR tests&lt;/h2&gt;
&lt;p&gt;设总体的密度函数(分布函数)为&lt;span class=&#34;math inline&#34;&gt;\(f(x;\theta),\theta\in\Theta\)&lt;/span&gt;, 研究检验问题&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0:\theta\in \Theta_0\ vs.\ H_1:\theta \notin \Theta_0\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;设似然函数(likelihood function)为&lt;span class=&#34;math inline&#34;&gt;\(L(\vec x;\theta)=\prod_{i=1}^nf(x_i;\theta)\)&lt;/span&gt;, 令&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[L(A):= \sup_{\theta\in A}L(\vec x;\theta),\ A\subseteq \Theta\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;定义&lt;strong&gt;广义似然比&lt;/strong&gt;为：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\lambda(\vec x):=\frac{L(\Theta)}{L(\Theta_0)}\ge 1\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;广义似然比拒绝域&lt;/strong&gt;为：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[W=\{\vec x:\lambda(\vec x)&amp;gt;\lambda_0\}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;span class=&#34;math inline&#34;&gt;\(\lambda_0\)&lt;/span&gt;满足&lt;span class=&#34;math inline&#34;&gt;\(\sup_{\theta\in\Theta_0}P_{\theta}(\vec X\in W)=\alpha\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;为检验水平。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;generalized-lr-and-sufficient-statistic&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Generalized LR and sufficient statistic&lt;/h2&gt;
&lt;p&gt;设充分统计量(sufficient statistic)为&lt;span class=&#34;math inline&#34;&gt;\(\psi(\vec X)\)&lt;/span&gt;, 由因子分解定理(factorization theorem)知，&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[L(\vec x;\theta)=g(\psi(\vec x),\theta)h(\vec x)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\lambda(\vec x):=\frac{\sup_{\theta\in\Theta} g(\psi(\vec x),\theta)h(\vec x)}{\sup_{\theta\in\Theta_0} g(\psi(\vec x),\theta)h(\vec x)}=\frac{\sup_{\theta\in\Theta} g(\psi(\vec x),\theta)}{\sup_{\theta\in\Theta_0} g(\psi(\vec x),\theta)}=\ell(\psi(\vec x))\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论&lt;/strong&gt;：广义似然比是充分统计量的函数，所以拒绝域可以写成&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[W=\{\vec x:\lambda(\vec x)&amp;gt;\lambda_0\}=\{\vec x:\psi(\vec x)\in B\}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;问题转化成求解集合&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;使得检验水平为&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;. 如果充分统计量在给定&lt;span class=&#34;math inline&#34;&gt;\(\theta=\theta_0\)&lt;/span&gt;下容易得到，这个问题则比较容易处理。
下面只针对&lt;strong&gt;正态总体&lt;/strong&gt;来分析。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ht-for-normal-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;HT for normal distribution&lt;/h2&gt;
&lt;p&gt;前面我们已经分析了&lt;span class=&#34;math inline&#34;&gt;\(N(\mu,\sigma^2)\)&lt;/span&gt;只有一个未知参数的假设检验问题，现在利用广义似然比来分析两个未知参数的情形。我们只考虑下面三种情况：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\theta=\theta_0\ vs.\ \theta\neq \theta_0\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\theta\le \theta_0\ vs.\ \theta&amp;gt;\theta_0\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\theta\ge \theta_0\ vs.\ \theta&amp;lt;\theta_0\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;span class=&#34;math inline&#34;&gt;\(\theta=\mu\)&lt;/span&gt; 或者 &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;, 另外一个参数未知。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;two-sided-tests-for-normal-mean-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Two-sided tests for normal mean&lt;/h2&gt;
&lt;p&gt;总体&lt;span class=&#34;math inline&#34;&gt;\(X\sim N(\mu,\sigma^2)\)&lt;/span&gt;, 考虑检验问题&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mu=\mu_0\ vs.\ \mu\neq \mu_0\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;则，&lt;span class=&#34;math inline&#34;&gt;\(\theta=(\mu,\sigma^2),\Theta_0=\{(\mu_0,\sigma^2):\sigma^2&amp;gt;0\},\Theta=\{(\theta,\sigma^2):\theta\in\mathbb{R},\sigma^2&amp;gt;0\}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;似然函数：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[L(\vec x;\theta)=(2\pi \sigma^2)^{-n/2}\exp\left(-\frac 1{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(L(\Theta)=L(\vec x;\bar x,s_n^2)=(2\pi s_n^2e)^{-n/2}\)&lt;/span&gt;, 令&lt;span class=&#34;math inline&#34;&gt;\(q_n^2=\frac 1n\sum_{i=1}^n (x_i-\mu_0)^2\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(L(\Theta_0)=L(\vec x;\mu_0, q_n^2)=(2\pi q_n^2e)^{-n/2}\)&lt;/span&gt;. 似然比为
&lt;span class=&#34;math display&#34;&gt;\[\lambda(\vec x)=(q_n^2/s_n^2)^{n/2}=\left(1+\frac{T(\vec x)^2}{n-1}\right)^{n/2}
,\ T(\vec x)=\frac{\bar x-\mu_0}{s_n/\sqrt{n-1}}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;two-sided-tests-for-normal-mean-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Two-sided tests for normal mean&lt;/h2&gt;
&lt;p&gt;所以，拒绝域为&lt;span class=&#34;math inline&#34;&gt;\(W=\{\vec x:|T(\vec x)|&amp;gt;C\}\)&lt;/span&gt;, 其中&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P_{\mu_0}(\vec X\in W)=P_{\mu_0}(|T(\vec X)|&amp;gt;C)=\alpha\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;因为在&lt;span class=&#34;math inline&#34;&gt;\(\mu=\mu_0\)&lt;/span&gt;时，&lt;span class=&#34;math inline&#34;&gt;\(T\sim t(n-1)\)&lt;/span&gt;, 所以，&lt;span class=&#34;math inline&#34;&gt;\(C=t_{1-\alpha/2}(n-1)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;可以证明&lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;是UMPU的，证明较复杂（略）&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;这种检验方式称为&lt;strong&gt;t检验&lt;/strong&gt;，&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;为检验统计量&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;one-sided-tests-for-normal-mean-i&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;One-sided tests for normal mean I&lt;/h2&gt;
&lt;p&gt;总体&lt;span class=&#34;math inline&#34;&gt;\(X\sim N(\mu,\sigma^2)\)&lt;/span&gt;, 考虑检验问题&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mu\le\mu_0\ vs.\ \mu&amp;gt; \mu_0\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;此时，&lt;span class=&#34;math inline&#34;&gt;\(\Theta_0=\{(\mu,\sigma^2):\mu\le \mu_0,\sigma^2&amp;gt;0\}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[L(\Theta_0)=
\begin{cases}
L(\vec x;\bar x,s_n^2),\ &amp;amp;\bar x\le \mu_0\\
L(\vec x;\mu_0,q_n^2),\ &amp;amp;\bar x&amp;gt; \mu_0
\end{cases}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;似然比为
&lt;span class=&#34;math display&#34;&gt;\[\lambda(\vec x)=\begin{cases}
\left(1+\frac{T(\vec x)^2}{n-1}\right)^{n/2}, \ &amp;amp;\bar x&amp;gt; \mu_0\\
1,\ &amp;amp;\bar x\le \mu_0
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以，拒绝域为&lt;span class=&#34;math inline&#34;&gt;\(W=\{\vec x:T(\vec x)&amp;gt;C\}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(C=t_{1-\alpha}(n-1)\)&lt;/span&gt;, 是UMPU&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;one-sided-tests-for-normal-mean-ii&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;One-sided tests for normal mean II&lt;/h2&gt;
&lt;p&gt;总体&lt;span class=&#34;math inline&#34;&gt;\(X\sim N(\mu,\sigma^2)\)&lt;/span&gt;, 考虑检验问题&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mu\ge\mu_0\ vs.\ \mu&amp;lt; \mu_0\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;类似地，拒绝域为&lt;span class=&#34;math inline&#34;&gt;\(W=\{\vec x:T(\vec x)&amp;lt;C\}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(C=t_{\alpha}(n-1)\)&lt;/span&gt;, 是UMPU&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;总结&lt;/strong&gt;：在方差未知的情况下考虑期望的检验问题，由&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;检验可以得到UMPU拒绝域，检验统计量为
&lt;span class=&#34;math display&#34;&gt;\[T=\frac{\bar X-\mu_0}{S_n/\sqrt{n-1}}=\frac{\bar X-\mu_0}{S_n^*/\sqrt{n}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;拒绝域&lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;形式为&lt;span class=&#34;math inline&#34;&gt;\(|T|&amp;gt;C,\ T&amp;gt;C,\ T&amp;lt;C\)&lt;/span&gt;, 其中&lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;满足&lt;span class=&#34;math inline&#34;&gt;\(P_{\mu_0}(T\in W)=\alpha\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;illustration&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Illustration&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;test.jpg&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;two-sided-tests-for-normal-variance-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Two-sided tests for normal variance&lt;/h2&gt;
&lt;p&gt;总体&lt;span class=&#34;math inline&#34;&gt;\(X\sim N(\mu,\sigma^2)\)&lt;/span&gt;, 考虑检验问题&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma^2=\sigma^2_0\ vs.\ \sigma^2\neq \sigma^2_0\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;利用广义似然比检验法可以得到拒绝域为&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[W=\{\vec x: \chi^2&amp;gt;C_2, \chi^2&amp;lt;C_1\}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;span class=&#34;math inline&#34;&gt;\(\chi^2 = \frac{1}{\sigma^2_0}\sum_{i=1}^n(x_i-\bar x)^2=ns_n^2/\sigma_0^2\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(C_1,C_2\)&lt;/span&gt;满足&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\int_{C_1}^{C_2} g_{n-1}(x) dx = 1-\alpha\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;span class=&#34;math inline&#34;&gt;\(g_n(x)\)&lt;/span&gt;表示&lt;span class=&#34;math inline&#34;&gt;\(\chi^2(n)\)&lt;/span&gt;的密度函数。可以证明当&lt;span class=&#34;math inline&#34;&gt;\(C_1,C_2\)&lt;/span&gt;再满足&lt;span class=&#34;math inline&#34;&gt;\(\int_{C_1}^{C_2} g_{n+1}(x) dx = 1-\alpha\)&lt;/span&gt;时，拒绝域&lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;为UMPU.&lt;/p&gt;
&lt;p&gt;为方便起见，取&lt;span class=&#34;math inline&#34;&gt;\(C_1=\chi^2_{\alpha/2}(n-1),\ C_2=\chi^2_{1-\alpha/2}(n-1)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;one-sided-tests-for-normal-variance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;One-sided tests for normal variance&lt;/h2&gt;
&lt;p&gt;总体&lt;span class=&#34;math inline&#34;&gt;\(X\sim N(\mu,\sigma^2)\)&lt;/span&gt;, 考虑检验问题&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma^2\le\sigma^2_0\ vs.\ \sigma^2&amp;gt; \sigma^2_0\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;利用广义似然比检验法可以得到拒绝域为&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[W=\{\vec x: \chi^2&amp;gt;C\}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;span class=&#34;math inline&#34;&gt;\(C=\chi^2_{1-\alpha}(n-1)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;考虑检验问题&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma^2\ge\sigma^2_0\ vs.\ \sigma^2&amp;lt; \sigma^2_0\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;利用广义似然比检验法可以得到拒绝域为&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[W=\{\vec x: \chi^2&amp;lt;C\}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;span class=&#34;math inline&#34;&gt;\(C=\chi^2_{\alpha}(n-1)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;这两者都是UMPU.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ht-for-two-independent-normals&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;HT for two independent normals&lt;/h2&gt;
&lt;p&gt;设总体&lt;span class=&#34;math inline&#34;&gt;\(X\sim N(\mu_1,\sigma_1^2)\)&lt;/span&gt;, 另有与&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;独立的总体&lt;span class=&#34;math inline&#34;&gt;\(Y\sim N(\mu_2,\sigma_2^2)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0: \mu_1-\mu_2=\delta,\ H_1: \mu_1-\mu_2\neq \delta\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果&lt;span class=&#34;math inline&#34;&gt;\(\sigma_1^2,\sigma_2^2\)&lt;/span&gt;已知, 选择&lt;strong&gt;U检验统计量&lt;/strong&gt;：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[U=\frac{(\bar X-\bar Y)-(\mu_1-\mu_2)}{\sqrt{\sigma_1^2/m+\sigma_2^2/n}}=\frac{\bar X-\bar Y-\delta}{\sqrt{\sigma_1^2/m+\sigma_2^2/n}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;若&lt;span class=&#34;math inline&#34;&gt;\(|U|&amp;gt; u_{1-\alpha/2}\)&lt;/span&gt;拒绝&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;，否则接受&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果&lt;span class=&#34;math inline&#34;&gt;\(\sigma_1^2,\sigma_2^2\)&lt;/span&gt;未知，已知&lt;span class=&#34;math inline&#34;&gt;\(\sigma_1^2=\sigma_2^2\)&lt;/span&gt;，选择&lt;strong&gt;t检验统计量&lt;/strong&gt;：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[T=\frac{\bar X-\bar Y-\delta}{S_w\sqrt{1/m+1/n}}\sim t(m+n-2)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;若&lt;span class=&#34;math inline&#34;&gt;\(|T|&amp;gt; t_{1-\alpha/2}(n+m-2)\)&lt;/span&gt;拒绝&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;，否则接受&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ht-for-two-independent-normals-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;HT for two independent normals&lt;/h2&gt;
&lt;p&gt;如果&lt;span class=&#34;math inline&#34;&gt;\(\sigma_1^2,\sigma_2^2\)&lt;/span&gt;未知，但&lt;span class=&#34;math inline&#34;&gt;\(\sigma_1^2\neq\sigma_2^2\)&lt;/span&gt;, 选择&lt;strong&gt;检验统计量&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[T=\frac{(\bar X-\bar Y)-\delta}{\sqrt{S_{1m}^{*2}/m+S_{2n}^{*2}/n}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;在&lt;span class=&#34;math inline&#34;&gt;\(\mu_1-\mu_2=\delta\)&lt;/span&gt;下，&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;近似服从自由度为&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;的&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;分布，其中&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;为接近&lt;span class=&#34;math inline&#34;&gt;\(k^*\)&lt;/span&gt;的整数，&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[k^*=\frac{(S_{1m}^{*2}/m+S_{2n}^{*2}/n)^2}{(S_{1m}^{*2}/m)^2/(m-1)+(S_{2n}^{*2}/n)^2/(n-1)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;这就是著名的&lt;strong&gt;Behrens-Fisher问题&lt;/strong&gt;。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ht-for-two-independent-normals-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;HT for two independent normals&lt;/h2&gt;
&lt;p&gt;设总体&lt;span class=&#34;math inline&#34;&gt;\(X\sim N(\mu_1,\sigma_1^2)\)&lt;/span&gt;, 另有与&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;独立的总体&lt;span class=&#34;math inline&#34;&gt;\(Y\sim N(\mu_2,\sigma_2^2)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0: \sigma_1^2=\sigma_2^2,\ H_1: \sigma_1^2\neq \sigma_2^2\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果&lt;span class=&#34;math inline&#34;&gt;\(\mu_1,\mu_2\)&lt;/span&gt;已知，选择&lt;strong&gt;F检验统计量&lt;/strong&gt;：
&lt;span class=&#34;math display&#34;&gt;\[F=\frac{\frac 1 m\sum_{i=1}^m(X_i-\mu_1)^2}{\frac 1 n\sum_{i=1}^n(Y_i-\mu_2)^2}\frac{\sigma_2^2}{\sigma_1^2}=\frac{\frac 1 m\sum_{i=1}^m(X_i-\mu_1)^2}{\frac 1 n\sum_{i=1}^n(Y_i-\mu_2)^2}\sim F(m,n)\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;若&lt;span class=&#34;math inline&#34;&gt;\(F&amp;gt; F_{1-\alpha/2}(m,n)\)&lt;/span&gt;或者&lt;span class=&#34;math inline&#34;&gt;\(F&amp;lt;F_{\alpha/2}(m,n)\)&lt;/span&gt;拒绝&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;，否则接受&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果&lt;span class=&#34;math inline&#34;&gt;\(\mu_1,\mu_2\)&lt;/span&gt;未知，选择&lt;strong&gt;F检验统计量&lt;/strong&gt;：
&lt;span class=&#34;math display&#34;&gt;\[F=\frac{\frac 1 {m-1}\sum_{i=1}^m(X_i-\bar X)^2}{\frac 1 {n-1}\sum_{i=1}^n(Y_i-\bar Y)^2}=\frac{S_{1m}^{*2}}{S_{2n}^{*2}}\sim F(m-1,n-1)\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;若&lt;span class=&#34;math inline&#34;&gt;\(F&amp;gt; F_{1-\alpha/2}(m-1,n-1)\)&lt;/span&gt;或者&lt;span class=&#34;math inline&#34;&gt;\(F&amp;lt;F_{\alpha/2}(m-1,n-1)\)&lt;/span&gt;拒绝&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;，否则接受&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;case-study-heights-dataset&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Case study: Heights dataset&lt;/h2&gt;
&lt;p&gt;假设男生身高&lt;span class=&#34;math inline&#34;&gt;\(X\sim N(\mu_1,\sigma_1^2)\)&lt;/span&gt;, 女生身高&lt;span class=&#34;math inline&#34;&gt;\(X\sim N(\mu_2,\sigma_2^2)\)&lt;/span&gt;, 比较他们的均值与方差的差异。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;均值的假设检验R命令&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;t.test(x, y = NULL,
alternative = c(“two.sided”, “less”, “greater”),
mu = 0, paired = FALSE, var.equal = FALSE,
conf.level = 0.95, …)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方差的假设检验R命令&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;var.test(x, y, ratio = 1,
alternative = c(“two.sided”, “less”, “greater”),
conf.level = 0.95, …)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mean-difference-test-i&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mean difference test (I)&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
attach(heights)
female_height = height[sex==&amp;quot;Female&amp;quot;]#提取女生数据
male_height = height[sex==&amp;quot;Male&amp;quot;]#提取男生数据
## 方差相等时双边假设检验
t.test(male_height,female_height,var.equal = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Two Sample t-test
## 
## data:  male_height and female_height
## t = 16.283, df = 1048, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  3.848073 4.902589
## sample estimates:
## mean of x mean of y 
##  69.31475  64.93942&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;mean-difference-test-ii&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mean difference test (II)&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## 方差不相等时双边假设检验
t.test(male_height,female_height)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Welch Two Sample t-test
## 
## data:  male_height and female_height
## t = 15.925, df = 374.41, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  3.835108 4.915553
## sample estimates:
## mean of x mean of y 
##  69.31475  64.93942&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;mean-difference-test-iii&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mean difference test (III)&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## 方差不相等时单边假设检验
t.test(male_height,female_height,alternative = &amp;quot;less&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Welch Two Sample t-test
## 
## data:  male_height and female_height
## t = 15.925, df = 374.41, p-value = 1
## alternative hypothesis: true difference in means is less than 0
## 95 percent confidence interval:
##      -Inf 4.828355
## sample estimates:
## mean of x mean of y 
##  69.31475  64.93942&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;variance-ratio-test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Variance ratio test&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;var.test(male_height,female_height)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  F test to compare two variances
## 
## data:  male_height and female_height
## F = 0.92201, num df = 811, denom df = 237, p-value = 0.4226
## alternative hypothesis: true ratio of variances is not equal to 1
## 95 percent confidence interval:
##  0.7466979 1.1252727
## sample estimates:
## ratio of variances 
##          0.9220053&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;confidence-intervals-vs-ht&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Confidence intervals vs HT&lt;/h2&gt;
&lt;p&gt;假设&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的100&lt;span class=&#34;math inline&#34;&gt;\((1-\alpha)\%\)&lt;/span&gt; 置信区间(confidence interval, CI)为 &lt;span class=&#34;math inline&#34;&gt;\([L(X_1,\dots,X_n),U(X_1,\dots,X_n)]\)&lt;/span&gt;. 这表明&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P_{\theta}(\theta\in [L,U])=1-\alpha,\ \forall\theta\in\Theta.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;考虑假设检验: &lt;span class=&#34;math display&#34;&gt;\[H_0:\theta=\theta_0\ vs.\ H_1:\theta\neq\theta_0\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;检验法则&lt;/strong&gt;: 如果&lt;span class=&#34;math inline&#34;&gt;\(\theta_0\notin [L,U]\)&lt;/span&gt;, 拒绝原假设；否则接受原假设。于是得到一个拒绝域：&lt;span class=&#34;math inline&#34;&gt;\(W=\{\vec x:\theta_0\notin [L(\vec x),U(\vec x)]\}\)&lt;/span&gt;, 显著性水平为&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P_{\theta_0}(\theta_0\notin [L,U])=\alpha\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;但这样得到的拒绝域不一定是UMP或者UMPU!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;confidence-intervals-vs-ht-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Confidence intervals vs HT&lt;/h2&gt;
&lt;p&gt;反过来，假如我们有以下检验的一个拒绝域&lt;span class=&#34;math inline&#34;&gt;\(W(\theta_0)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0:\theta=\theta_0\ vs.\ H_1:\theta\neq\theta_0,\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;span class=&#34;math inline&#34;&gt;\(P_{\theta_0}((X_1,\dots,X_n)\in W(\theta_0))=\alpha, \forall \theta_0\in\Theta\)&lt;/span&gt;. 可以得到一个置信集&lt;strong&gt;(confidence set)&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[S(X_1,\dots,X_n)=\{\theta:(X_1,\dots,X_n)\notin W(\theta)\}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P_\theta(\theta\in S) = P_{\theta}((X_1,\dots,X_n)\notin W(\theta)) = 1-\alpha,\ \forall\theta\in\Theta\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;该置信集是由所有“接受”的&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的值组成的&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;一般情况下，该置信集为区间形式&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example&lt;/h2&gt;
&lt;p&gt;考虑总体&lt;span class=&#34;math inline&#34;&gt;\(X\sim N(\mu,\sigma^2)\)&lt;/span&gt;，其中&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;未知，方差&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;已知。则有
&lt;span class=&#34;math display&#34;&gt;\[P(\bar X-u_{1-\alpha/2}\frac{\sigma}{\sqrt{n}}\le \mu\le \bar X+u_{1-\alpha/2}\frac{\sigma}{\sqrt{n}})=1-\alpha\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;于是得到一个置信区间：&lt;span class=&#34;math inline&#34;&gt;\([\bar X-u_{1-\alpha/2}\frac{\sigma}{\sqrt{n}},\bar X+u_{1-\alpha/2}\frac{\sigma}{\sqrt{n}}]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;由此可以构造拒绝域：&lt;span class=&#34;math display&#34;&gt;\[W=\{\mu_0\notin [\bar x-u_{1-\alpha/2}\frac{\sigma}{\sqrt{n}},\bar x+u_{1-\alpha/2}\frac{\sigma}{\sqrt{n}}]\}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[W=\{\vec x:|\vec x-\mu_0|&amp;gt;u_{1-\alpha/2}\frac{\sigma}{\sqrt{n}}\}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;此拒绝域和我们之前得到的一样的。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;p-values&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;p-values&lt;/h2&gt;
&lt;p&gt;假设拒绝域具备如下形式&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[W=\{\vec x:T(\vec x)&amp;gt;\lambda\}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;为检验统计量&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;满足&lt;span class=&#34;math inline&#34;&gt;\(\sup_{\theta\in\Theta_0}P_{\theta}(T(\vec X)&amp;gt;\lambda)=\alpha\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由此可得：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;对于固定的样本，显著性水平&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;越大，&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;越小，这样越容易拒绝原假设&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;对于固定的样本，是否存在一个临界值&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, 使得当&lt;span class=&#34;math inline&#34;&gt;\(p&amp;lt;\alpha\)&lt;/span&gt;时拒绝原假设，当&lt;span class=&#34;math inline&#34;&gt;\(p\ge \alpha\)&lt;/span&gt;时接受原假设？这个临界值称为&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;值(p-value)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p=p(\vec x)=\sup_{\theta\in\Theta_0}P_\theta(T(X_1,\dots,X_n)\ge T(x_1,\dots,x_n))\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;如果给定&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;存在且唯一，则可以证明：&lt;span class=&#34;math inline&#34;&gt;\(T(\vec x)&amp;gt;\lambda\)&lt;/span&gt;当且仅当&lt;span class=&#34;math inline&#34;&gt;\(p(\vec x)&amp;lt;\alpha\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;p-values-for-simple-null&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;p-values for simple null&lt;/h2&gt;
&lt;p&gt;如果原假设是简单的, 即&lt;span class=&#34;math inline&#34;&gt;\(H_0:\theta=\theta_0\)&lt;/span&gt;，则&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p = P_{\theta_0}(T(X_1,\dots,X_n)\ge T(x_1,\dots,x_n))\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;例&lt;/strong&gt;：总体&lt;span class=&#34;math inline&#34;&gt;\(X\sim N(\mu,\sigma^2)\)&lt;/span&gt;，其中&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;未知，方差&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;已知，考虑检验&lt;span class=&#34;math display&#34;&gt;\[H_0:\mu=\mu_0\ vs.\ H_1:\mu\neq\mu_0.\]&lt;/span&gt; 拒绝域为&lt;span class=&#34;math inline&#34;&gt;\(W=\{\vec x:\sqrt{n}|\bar x-\mu_0|/\sigma&amp;gt;u_{1-\alpha/2}\}\)&lt;/span&gt;, 其中检验统计量为&lt;span class=&#34;math inline&#34;&gt;\(T = \sqrt{n}|\bar X-\mu_0|/\sigma\)&lt;/span&gt;. 故p值为&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p = P_{\mu_0}(T(\vec X)\ge T(\vec x))=2-2\Phi(T(\vec x))=2-2\Phi\left(\frac{|\bar x-\mu_0|}{\sigma/\sqrt{n}}\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;comments&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comments&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;p值可以看作样本与原假设&lt;strong&gt;相容程度的度量&lt;/strong&gt;。p值越大相容度越高；反之，p值越小相容度越低。当p值小于&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;时认为两者不相容，拒绝原假设&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;做检验时不需要事先确定显著性水平&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;（它的具有一定的主观性），如果p值非常小，则毫不犹豫地拒绝原假设；同样地，如果p值比较大，则接受原假设，这样就不用争论&lt;span class=&#34;math inline&#34;&gt;\(\alpha=0.1,0.05\)&lt;/span&gt;或者其他。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;p值提供更多的信息，可以用于&lt;strong&gt;保护隐私数据&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;统计软件提供的是p值&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-tests&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multiple tests&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;如果独立检验同一个假设&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;次，我们可以得到&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;个p值: &lt;span class=&#34;math inline&#34;&gt;\(p_1,\dots,p_k\)&lt;/span&gt;, 可否由这&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;个p值汇总成一个p值来检验该假设？&lt;strong&gt;元分析(meta-analysis)&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;假如我们有&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;个不同的原假设&lt;span class=&#34;math inline&#34;&gt;\(H_{0j},j=1,\dots,k\)&lt;/span&gt;，这种问题称为&lt;strong&gt;多重假设(multiple tests)&lt;/strong&gt;问题。可否利用&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;个不同假设的p值: &lt;span class=&#34;math inline&#34;&gt;\(p_1,\dots,p_k\)&lt;/span&gt;来进一步控制错误的发生概率?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;例子：吃果冻与长青春痘的联系：&lt;a href=&#34;https://xkcd.com/882/&#34; class=&#34;uri&#34;&gt;https://xkcd.com/882/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://imgs.xkcd.com/comics/significant.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;binomial-tests&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Binomial tests&lt;/h2&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;服从两点分布&lt;span class=&#34;math inline&#34;&gt;\(B(1,p)\)&lt;/span&gt;, 下面考虑以下三种常见的假设检验&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0:p\le p_0\ vs.\ H_1:p&amp;gt;p_0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0:p\ge p_0\ vs.\ H_1:p&amp;lt;p_0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0:p= p_0\ vs.\ H_1:p\neq p_0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于该总体，我们选&lt;span class=&#34;math inline&#34;&gt;\(S=\sum_{i=1}^nX_i\sim B(n,p)\)&lt;/span&gt;为检验统计量。相应的拒绝域形式为&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(W=\{s\ge c\}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(W=\{s\le c\}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(W=\{s\ge c_2\}\cup\{s\le c_1\}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;注意到&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;为离散型随机变量，所以满足&lt;span class=&#34;math inline&#34;&gt;\(\sup_{p\in\Theta_0}P_p(\vec X\in W)=\alpha\)&lt;/span&gt;的分界点不一定存在。因此，我们考虑&lt;span class=&#34;math inline&#34;&gt;\(\sup_{p\in\Theta_0}P_p(\vec X\in W)\le \alpha\)&lt;/span&gt;下分界点的选取。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ht-for-ratio-one-sided-i&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;HT for ratio: one-sided I&lt;/h2&gt;
&lt;p&gt;考虑单边假设&lt;span class=&#34;math inline&#34;&gt;\(H_0:p\le p_0\ vs.\ H_1:p&amp;gt;p_0\)&lt;/span&gt;, 临界值&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;为满足下式&lt;strong&gt;最小的整数&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sup_{p\le p_0}P_p(S\ge c)\le \alpha\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;由于&lt;span class=&#34;math inline&#34;&gt;\(P_p(S\ge c)=\sum_{i=c}^nC_n^ip^i(1-p)^{n-i}\)&lt;/span&gt;关于&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;&lt;strong&gt;单调递增&lt;/strong&gt;(=&lt;span class=&#34;math inline&#34;&gt;\(Beta(c,n-c+1)\)&lt;/span&gt;分布的CDF在&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;点的取值，见课本P44)，所以只需考虑&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P_{p_0}(S\ge c)=\sum_{i=c}^nC_n^ip_0^i(1-p_0)^{n-i}\le \alpha\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;计算&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;比较复杂，为了避免此，我们将拒绝域&lt;span class=&#34;math inline&#34;&gt;\(\{s\ge c\}\)&lt;/span&gt;等价转化为&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[W=\{\sum_{i=s}^nC_n^ip_0^i(1-p_0)^{n-i}\le \alpha\}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ht-for-ratio-one-sided-i-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;HT for ratio: one-sided I&lt;/h2&gt;
&lt;p&gt;更进一步，假设&lt;span class=&#34;math inline&#34;&gt;\(p_\alpha(s)\)&lt;/span&gt;为方程&lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=s}^nC_n^ip^i(1-p)^{n-i}=\alpha\)&lt;/span&gt;的根，则拒绝域等价转化为&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[W=\{p_0\le p_\alpha(s)\}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;span class=&#34;math inline&#34;&gt;\(p_\alpha(s)=Beta_{\alpha}(s,n-s+1)\)&lt;/span&gt;, 或者可以表示成&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p_\alpha(s)=(1+\frac{n-s+1}{s}F_{1-\alpha}(2(n-s+1),2s))^{-1}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;详细的转化见课本P105引理4.2.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example&lt;/h2&gt;
&lt;p&gt;考虑女士品茶问题，设该女士鉴别的成功率为&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. 设&lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt;表示第&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;次鉴别结果，即&lt;span class=&#34;math inline&#34;&gt;\(X_i=1\)&lt;/span&gt;表示成功，&lt;span class=&#34;math inline&#34;&gt;\(X_i=0\)&lt;/span&gt;表示失败。如果&lt;span class=&#34;math inline&#34;&gt;\(p&amp;gt;p_0\)&lt;/span&gt;我们认为该女士具备这种辨别能力，其中&lt;span class=&#34;math inline&#34;&gt;\(p_0\ge 1/2\)&lt;/span&gt;为给定的数。故考虑检验&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0: p\le p_0\ vs.\ H_1:p&amp;gt;p_0.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;二项分布检验的R代码&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;binom.test(x, n, p = 0.5,
alternative = c(“two.sided”, “less”, “greater”),
conf.level = 0.95)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-results-for-n10&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The results for n=10&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;alpha = 0.1
n = 10
s = 1:n
pr = qbeta(alpha,s,n-s+1)

par(mfrow = c(1,2),mar=c(4,4,2,0.5))
plot(s,pr,type=&amp;quot;b&amp;quot;,ylab=expression(p[alpha](s)), main=expression(alpha==0.1))
abline(h=0.5,col=&amp;quot;red&amp;quot;)
lb = expression(p[0]==0.5)
text(3,0.55,lb)

alpha = 0.05
pr = qbeta(alpha,s,n-s+1)
plot(s,pr,type=&amp;quot;b&amp;quot;,ylab=expression(p[alpha](s)), main=expression(alpha==0.05))
abline(h=0.5,col=&amp;quot;red&amp;quot;)
text(3,0.55,lb)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/chap03_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;binom.test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;binom.test&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;binom.test(8,10,0.5,alternative = &amp;quot;greater&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Exact binomial test
## 
## data:  8 and 10
## number of successes = 8, number of trials = 10, p-value = 0.05469
## alternative hypothesis: true probability of success is greater than 0.5
## 95 percent confidence interval:
##  0.4930987 1.0000000
## sample estimates:
## probability of success 
##                    0.8&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;ht-for-ratio-one-sided-ii&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;HT for ratio: one-sided II&lt;/h2&gt;
&lt;p&gt;考虑单边假设&lt;span class=&#34;math inline&#34;&gt;\(H_0:p\ge p_0\ vs.\ H_1:p&amp;lt;p_0\)&lt;/span&gt;, 临界值&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;为满足下式&lt;strong&gt;最大的整数&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sup_{p\ge p_0}P_p(S\le c)\le \alpha\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;由于&lt;span class=&#34;math inline&#34;&gt;\(P_p(S\le c)=\sum_{i=0}^cC_n^ip^i(1-p)^{n-i}\)&lt;/span&gt;关于&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;&lt;strong&gt;单调递减&lt;/strong&gt;，所以只需考虑&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P_{p_0}(S\le c)=\sum_{i=0}^cC_n^ip_0^i(1-p_0)^{n-i}\le \alpha\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;计算&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;比较复杂，为了避免此，我们将拒绝域&lt;span class=&#34;math inline&#34;&gt;\(\{s\ge c\}\)&lt;/span&gt;等价转化为&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[W=\{\sum_{i=s+1}^nC_n^ip_0^i(1-p_0)^{n-i}\ge 1-\alpha\}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ht-for-ratio-one-sided-ii-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;HT for ratio: one-sided II&lt;/h2&gt;
&lt;p&gt;更进一步，假设&lt;span class=&#34;math inline&#34;&gt;\(\tilde p_\alpha(s)\)&lt;/span&gt;为方程&lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=s+1}^nC_n^ip^i(1-p)^{n-i}=1-\alpha\)&lt;/span&gt;的根，则拒绝域等价转化为&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[W=\{p_0\ge \tilde p_\alpha(s)\}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;span class=&#34;math inline&#34;&gt;\(\tilde p_\alpha(s)=Beta_{1-\alpha}(s+1,n-s)\)&lt;/span&gt;, 或者可以表示成&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\tilde p_\alpha(s)=(1+\frac{n-s}{(s+1)F_{1-\alpha}(2(s+1),2(n-s))})^{-1}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;详细的转化见课本P105引理4.2.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ht-for-ratio-two-sided&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;HT for ratio: two-sided&lt;/h2&gt;
&lt;p&gt;考虑双边假设&lt;span class=&#34;math display&#34;&gt;\[H_0:p= p_0\ vs.\ H_1:p\neq p_0\]&lt;/span&gt;
拒绝域为&lt;span class=&#34;math inline&#34;&gt;\(\{s\le c_1\}\cup\{s\ge c_2\}\)&lt;/span&gt;，其中临界值&lt;span class=&#34;math inline&#34;&gt;\(c_1\)&lt;/span&gt;为满足&lt;span class=&#34;math inline&#34;&gt;\(P_{p_0}(S\le c_1)=\alpha/2\)&lt;/span&gt;&lt;strong&gt;最大的整数&lt;/strong&gt;，临界值&lt;span class=&#34;math inline&#34;&gt;\(c_1\)&lt;/span&gt;为满足&lt;span class=&#34;math inline&#34;&gt;\(P_{p_0}(S\ge c_2)=\alpha/2\)&lt;/span&gt;&lt;strong&gt;最小的整数&lt;/strong&gt;。由前面分析，该拒绝域等价于&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\{p_0\le p_{\alpha/2}(s)\}\cup\{p_0\ge \tilde{p}_{\alpha/2}(s)\}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;goodness-of-fit-tests&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. Goodness-of-fit tests&lt;/h2&gt;
&lt;p&gt;考虑离散型分布的假设检验，&lt;span class=&#34;math inline&#34;&gt;\(X\in \{t_1,\dots,t_m\}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0: P(X=t_i)=p_i,\ i=1,\dots,m,\ vs.\ H_1: P(X=t_i)\neq p_i\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^m p_i=1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;卡方检验法检验统计量：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[V=\sum_{i=1}^{m} \frac{(v_i-np_i)^2}{np_i}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;span class=&#34;math inline&#34;&gt;\(v_i\)&lt;/span&gt;表示&lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt;中包含&lt;span class=&#34;math inline&#34;&gt;\(t_i\)&lt;/span&gt;的个数，即&lt;span class=&#34;math inline&#34;&gt;\(v_i=\sum_{j=1}^m 1\{X_j=t_i\}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;拒绝域&lt;span class=&#34;math inline&#34;&gt;\(W=\{V&amp;gt;\lambda\}\)&lt;/span&gt;. 可以证明在&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;下，&lt;span class=&#34;math inline&#34;&gt;\(V\stackrel{\cdot}{\sim} \chi^2(m-1)\)&lt;/span&gt;, 故&lt;span class=&#34;math inline&#34;&gt;\(\lambda=\chi^2_{1-\alpha}(m-1)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-mendels-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example: Mendel’s Data&lt;/h2&gt;
&lt;p&gt;In one of his famous experiments, Mendel crossed 556 smooth, yellow male peas
with wrinkled, green female peas. According to now established genetic theory, the
relative frequencies of the progeny should be as given below.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(\text{smooth yellow}) = 9/16, P(\text{smooth green}) = 3/16\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(\text{wrinkled yellow}) = 3/16, P(\text{wrinkled green}) = 1/16\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The counts that Mendel recorded are 315, 108, 102, 31, respectively.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; test statistic is &lt;span class=&#34;math inline&#34;&gt;\(V=0.604\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\chi^2_{1-0.1}(3)=6.25\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;p-value is &lt;span class=&#34;math inline&#34;&gt;\(0.90\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;R code:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;chisq.test&lt;/strong&gt;(x, y = NULL, correct = TRUE,
p = rep(1/length(x), length(x)), rescale.p = FALSE,
simulate.p.value = FALSE, B = 2000)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;chisq.test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;chisq.test&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x = c(315, 108, 102, 31)
p = c(9/16,3/16,3/16,1/16)
chisq.test(x,p=p)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Chi-squared test for given probabilities
## 
## data:  x
## X-squared = 0.60432, df = 3, p-value = 0.8954&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;goodness-of-fit-tests-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Goodness-of-fit tests&lt;/h2&gt;
&lt;p&gt;考虑连续型分布：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0:F(x)=F_0(x)\ vs. \ F(x)\neq F_0(x)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;把整个实轴分成&lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt;份，&lt;span class=&#34;math inline&#34;&gt;\((-\infty,t_1],\ (t_1,t_2],\dots,(t_{m-2},t_{m-1}],\ (t_{m-1},\infty)\)&lt;/span&gt;, 分别计算这&lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt;个区间的概率&lt;span class=&#34;math inline&#34;&gt;\(p_i,i=1,\dots,m\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(v_i\)&lt;/span&gt;表示&lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt;落到第&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;个区间的个数， 类似离散的分布的检验。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(t_{i},m\)&lt;/span&gt;的选择？借鉴直方图法的选取&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;其他检验：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;独立性检验&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;正态性检验&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;柯尔莫哥洛夫检验法&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>第八次作业</title>
      <link>/post/homework8/</link>
      <pubDate>Wed, 07 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/homework8/</guid>
      <description>&lt;p&gt;True or false, and state why:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The generalized likelihood ratio statistic &lt;span class=&#34;math inline&#34;&gt;\(\lambda(\vec x)\)&lt;/span&gt; (see P.87 of our textbook) is always greater than or equal to 1.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If the p-value is 0.03, the corresponding test will reject at the significance
level 0.02.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If a test rejects at significance level 0.06, then the p-value is less than or equal
to 0.06.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The p-value of a test is the probability that the null hypothesis is correct.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In testing a simple versus simple hypothesis via the likelihood ratio test, the
p-value equals the inverse of the likelihood ratio.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;code&gt;Case study 1&lt;/code&gt;: Mutual funds are investment vehicles consisting of a portfolio of various types
of investments. If such an investment is to meet annual spending needs, the
owner of shares in the fund is interested in the average of the annual returns of
the fund. Investors are also concerned with the volatility of the annual returns,
measured by the variance or standard deviation. One common method of evaluating
a mutual fund is to compare it to a benchmark, the Lipper Average being
one of these. This index number is the average of returns from a universe of
mutual funds.
The Global Rock Fund is a typical mutual fund, with heavy investments in
international funds. It claimed to best the Lipper Average in terms of volatility
over the period from 1989 through 2007. Its returns are given in the table below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Year&lt;/th&gt;
&lt;th&gt;Investment Return %&lt;/th&gt;
&lt;th&gt;Year&lt;/th&gt;
&lt;th&gt;Investment Return %&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;1989&lt;/td&gt;
&lt;td&gt;15.32&lt;/td&gt;
&lt;td&gt;1999&lt;/td&gt;
&lt;td&gt;27.43&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;1990&lt;/td&gt;
&lt;td&gt;1.62&lt;/td&gt;
&lt;td&gt;2000&lt;/td&gt;
&lt;td&gt;8.57&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;1991&lt;/td&gt;
&lt;td&gt;28.43&lt;/td&gt;
&lt;td&gt;2001&lt;/td&gt;
&lt;td&gt;1.88&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;1992&lt;/td&gt;
&lt;td&gt;11.91&lt;/td&gt;
&lt;td&gt;2002&lt;/td&gt;
&lt;td&gt;−7.96&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;1993&lt;/td&gt;
&lt;td&gt;20.71&lt;/td&gt;
&lt;td&gt;2003&lt;/td&gt;
&lt;td&gt;35.98&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;1994&lt;/td&gt;
&lt;td&gt;−2.15&lt;/td&gt;
&lt;td&gt;2004&lt;/td&gt;
&lt;td&gt;14.27&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;1995&lt;/td&gt;
&lt;td&gt;23.29&lt;/td&gt;
&lt;td&gt;2005&lt;/td&gt;
&lt;td&gt;10.33&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;1996&lt;/td&gt;
&lt;td&gt;15.96&lt;/td&gt;
&lt;td&gt;2006&lt;/td&gt;
&lt;td&gt;15.94&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;1997&lt;/td&gt;
&lt;td&gt;11.12&lt;/td&gt;
&lt;td&gt;2007&lt;/td&gt;
&lt;td&gt;16.71&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;1998&lt;/td&gt;
&lt;td&gt;0.37&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The standard deviation for the Lipper Average is &lt;span class=&#34;math inline&#34;&gt;\(11.67\%\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; denote the variance of the population represented by the return
percentages shown in the table above. Consider the test
&lt;span class=&#34;math display&#34;&gt;\[H_0: \sigma^2=(11.67)^2\ vs.\ H_1:\sigma^2&amp;lt;(11.67)^2.\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If the significance level &lt;span class=&#34;math inline&#34;&gt;\(\alpha=0.05\)&lt;/span&gt;, what’s your decision?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Show up the p-value of your test.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;code&gt;Case study 2&lt;/code&gt;: Forensic scientists sometimes have difficulty identifying the sex of a murder
victim whose body is discovered badly decomposed. Often, dental structure can
provide useful clues because female teeth and male teeth have different physical and chemical characteristics. The extent to which X-rays can penetrate tooth
enamel, for instance, is not the same for the two sexes.&lt;/p&gt;
&lt;p&gt;Table below lists the enamel spectropenetration gradients for eight male
teeth and eight female teeth. These measurements have all the characteristics
of the two-sample format: the data are quantitative, the units are similar,
two factor levels (male and female) are involved, and the observations are
independent.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Male&lt;/th&gt;
&lt;th&gt;Female&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;4.9&lt;/td&gt;
&lt;td&gt;4.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;5.4&lt;/td&gt;
&lt;td&gt;5.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;5.0&lt;/td&gt;
&lt;td&gt;3.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;5.5&lt;/td&gt;
&lt;td&gt;4.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;5.4&lt;/td&gt;
&lt;td&gt;5.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;6.6&lt;/td&gt;
&lt;td&gt;4.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;6.3&lt;/td&gt;
&lt;td&gt;3.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;4.3&lt;/td&gt;
&lt;td&gt;5.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Assume that the enamel spectropenetration gradients for male teeth and female teeth are normally distributed. Based on the data above, conduct a test (the significance level &lt;span class=&#34;math inline&#34;&gt;\(\alpha=0.05\)&lt;/span&gt;) to judge whether female teeth and male teeth have different physical and chemical characteristics.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Assume that their variances are the same, what’s your decision?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you were not able to have the prior information that their variances are the same, what would you do? This is the case of &lt;strong&gt;Behrens-Fisher Problem&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The data are paired. Is it possible to do a paired test, without judging whether their variances are the same?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;code&gt;Case study 3&lt;/code&gt;: The National Center for Health Statistics (1970) gives the following data on
distribution of suicides in the United States by month in 1970. Is there any
evidence that the suicide rate varies seasonally, or are the data consistent with
the hypothesis that the rate is constant (the significance level &lt;span class=&#34;math inline&#34;&gt;\(\alpha=0.05\)&lt;/span&gt;)? (Hint: Under the latter hypothesis, model
the number of suicides in each month as a multinomial random variable with the
appropriate probabilities and conduct a goodness-of-fit test.)&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Month&lt;/th&gt;
&lt;th&gt;Number of Suicides&lt;/th&gt;
&lt;th&gt;Days/Month&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Jan.&lt;/td&gt;
&lt;td&gt;1867&lt;/td&gt;
&lt;td&gt;31&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Feb.&lt;/td&gt;
&lt;td&gt;1789&lt;/td&gt;
&lt;td&gt;28&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Mar.&lt;/td&gt;
&lt;td&gt;1944&lt;/td&gt;
&lt;td&gt;31&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Apr.&lt;/td&gt;
&lt;td&gt;2094&lt;/td&gt;
&lt;td&gt;30&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;May&lt;/td&gt;
&lt;td&gt;2097&lt;/td&gt;
&lt;td&gt;31&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;June&lt;/td&gt;
&lt;td&gt;1981&lt;/td&gt;
&lt;td&gt;30&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;July&lt;/td&gt;
&lt;td&gt;1887&lt;/td&gt;
&lt;td&gt;31&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Aug.&lt;/td&gt;
&lt;td&gt;2024&lt;/td&gt;
&lt;td&gt;31&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Sept.&lt;/td&gt;
&lt;td&gt;1928&lt;/td&gt;
&lt;td&gt;30&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Oct.&lt;/td&gt;
&lt;td&gt;2032&lt;/td&gt;
&lt;td&gt;31&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Nov.&lt;/td&gt;
&lt;td&gt;1978&lt;/td&gt;
&lt;td&gt;30&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Dec.&lt;/td&gt;
&lt;td&gt;1859&lt;/td&gt;
&lt;td&gt;31&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;code&gt;Case study 4&lt;/code&gt;: Under (the assumption of) simple Mendelian inheritance, a cross
between plants of two particular genotypes produces progeny 1/4 of
which are “dwarf” and &lt;span class=&#34;math inline&#34;&gt;\(3/4\)&lt;/span&gt; of which are “giant”, respectively.
In an experiment to determine if this assumption is reasonable, a
cross results in progeny having 243 dwarf and 682 giant plants.
If “giant” is taken as success, the null hypothesis is that &lt;span class=&#34;math inline&#34;&gt;\(p =3/4\)&lt;/span&gt; and the alternative that &lt;span class=&#34;math inline&#34;&gt;\(p \neq 3/4\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(X_i,i=1,\dots,n\)&lt;/span&gt; be the sample of the population &lt;span class=&#34;math inline&#34;&gt;\(B(1,p)\)&lt;/span&gt;. By central limit theorem (CLT), the distribution of &lt;span class=&#34;math inline&#34;&gt;\(\bar X\)&lt;/span&gt; can be approximated by a normal distribution &lt;span class=&#34;math inline&#34;&gt;\(N(p,p(1-p)/n)\)&lt;/span&gt;. Please use this approximation to do the binominal test above.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Actually, we can do the exact binominal test according to the formula given in P.114 of our textbook. Compare the results in the exact test and the approximate test for significance levels &lt;span class=&#34;math inline&#34;&gt;\(\alpha=0.05,0.01,0.001\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>第七次作业</title>
      <link>/post/homework7/</link>
      <pubDate>Mon, 05 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/homework7/</guid>
      <description>&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt; be a sample from an exponential distribution &lt;span class=&#34;math inline&#34;&gt;\(Exp(\lambda)\)&lt;/span&gt;. Given a significance level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;, derive a likelihood ratio test of&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0:\lambda=\lambda_1\ vs.\ H_1:\lambda=\lambda_2,\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1\neq\lambda_2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Solution&lt;/code&gt;: The likelihood function is
&lt;span class=&#34;math display&#34;&gt;\[L(\lambda)=\prod_{i=1}^n (\lambda e^{-\lambda x_i}) = \lambda^ne^{-\lambda n\bar x}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The likelihood ratio is given by
&lt;span class=&#34;math display&#34;&gt;\[\lambda(\vec x)= \frac{L(\lambda_2)}{L(\lambda_1)}=\frac{\lambda_2^ne^{-\lambda_2 n\bar x}}{\lambda_1^ne^{-\lambda_1 n\bar x}}=(\lambda_2/\lambda_1)^ne^{(\lambda_1-\lambda_2)n\bar x}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Choose the &lt;code&gt;test statistic&lt;/code&gt; &lt;span class=&#34;math inline&#34;&gt;\(T(\vec x) = 2\lambda_1n\bar x\)&lt;/span&gt;. When &lt;span class=&#34;math inline&#34;&gt;\(\lambda=\lambda_1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(T(\vec X)\sim \chi^2(2n)\)&lt;/span&gt;. Also,
&lt;span class=&#34;math display&#34;&gt;\[\lambda(\vec x) = (\lambda_2/\lambda_1)^ne^{(\lambda_1-\lambda_2)T(\vec x)/(2\lambda_1)}.\]&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Note that &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1,\lambda_2&amp;gt;0\)&lt;/span&gt;. If &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1&amp;gt;\lambda_2\)&lt;/span&gt;, the rejection region is of the form &lt;span class=&#34;math inline&#34;&gt;\(W=\{T(\vec x)&amp;gt;C\}\)&lt;/span&gt;. We thus have &lt;span class=&#34;math inline&#34;&gt;\(C=\chi_{1-\alpha}^2(2n)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1&amp;lt;\lambda_2\)&lt;/span&gt;, the rejection region is of the form &lt;span class=&#34;math inline&#34;&gt;\(W=\{T(\vec x)&amp;lt;C\}\)&lt;/span&gt;. We thus have &lt;span class=&#34;math inline&#34;&gt;\(C=\chi_{\alpha}^2(2n)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt; be a sample from an exponential distribution &lt;span class=&#34;math inline&#34;&gt;\(Exp(\lambda)\)&lt;/span&gt;. Given a significance level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;, derive a UMPU test of&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0:\lambda=\lambda_0\ vs.\ H_1:\lambda\neq\lambda_0.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Solution&lt;/code&gt;: Exponential distribution belongs to exponential family of the form &lt;span class=&#34;math inline&#34;&gt;\(S(\lambda)h(x)e^{Q(\lambda)V(x)}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(V(x) = -x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Q(\lambda)=\lambda\)&lt;/span&gt;. Choose the &lt;code&gt;test statistic&lt;/code&gt; &lt;span class=&#34;math inline&#34;&gt;\(T(\vec x)=2\lambda_0 n\bar x\)&lt;/span&gt;. As a result, the UMPU rejection region has the form
&lt;span class=&#34;math display&#34;&gt;\[W = \{T(\vec x)&amp;lt;C_1 \text{ or } &amp;gt;C_2\},\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(C_1,C_2\)&lt;/span&gt; satisfy
&lt;span class=&#34;math display&#34;&gt;\[P_{\lambda_0}(\bar X\in W)=\alpha\]&lt;/span&gt;
and
&lt;span class=&#34;math display&#34;&gt;\[E_{\lambda_0}[1\{\vec X\in W\}T(\vec X)]=\alpha E_{\lambda_0}[T(\vec X)].\]&lt;/span&gt;
Let &lt;span class=&#34;math inline&#34;&gt;\(f(x;n)\)&lt;/span&gt; be the density of &lt;span class=&#34;math inline&#34;&gt;\(\chi^2(n)\)&lt;/span&gt;, that is
&lt;span class=&#34;math display&#34;&gt;\[f(x;n)=\frac{1}{2^{n/2}\Gamma(n/2)}x^{n/2-1}e^{-x/2}1\{x&amp;gt;0\}.\]&lt;/span&gt;
If &lt;span class=&#34;math inline&#34;&gt;\(\lambda=\lambda_0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(T(\vec X)\sim \chi^2(2n)\)&lt;/span&gt;. We thus have
&lt;span class=&#34;math display&#34;&gt;\[\int_{C_1}^{C_2}f(x;2n) d x=1-\alpha,\quad(1)\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\int_{C_1}^{C_2} x f(x;2n)dx = 2n(1-\alpha).\]&lt;/span&gt;
The later equality can be expressed as
&lt;span class=&#34;math display&#34;&gt;\[\int_{C_1}^{C_2} \frac{x}{2n} f(x;2n)dx=\int_{C_1}^{C_2} \frac{x}{2n} \frac{1}{2^{n}\Gamma(n)}x^{n-1}e^{-x/2}dx=\int_{C_1}^{C_2} f(x;2n+2)dx=1-\alpha.\quad(2)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It is hard to solve the equations (1) and (2). In practice, we may take
&lt;span class=&#34;math inline&#34;&gt;\(C_1=\chi_{\alpha/2}^2(2n)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(C_2=\chi_{1-\alpha/2}^2(2n)\)&lt;/span&gt; so that the significance level of the test is &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;. BUT, it is not the exact UMPU test since (2) is not satisfied. If &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is large enough, &lt;span class=&#34;math inline&#34;&gt;\(f(x;2n+2)\approx f(x;2n)\)&lt;/span&gt; (see figure below). This implies the resulting rejection region is almost UMPU when &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is large.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/homework7_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Of course, you can solve the equations (1) and (2) by numerical algorithms, such as bisection and Newton’s methods. Let &lt;span class=&#34;math inline&#34;&gt;\(F(x;n)\)&lt;/span&gt; be the CDF of &lt;span class=&#34;math inline&#34;&gt;\(\chi^2(n)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(F^{-1}\)&lt;/span&gt; denote its inverse. By (1), we have &lt;span class=&#34;math inline&#34;&gt;\(C_2=F^{-1}(F(C_1;2n)+1-\alpha;2n)\)&lt;/span&gt;. Substituting it into (2), we arrive at an equation:
&lt;span class=&#34;math display&#34;&gt;\[F(F^{-1}(F(C_1;2n)+1-\alpha;2n);2n+2)-F(C_1;2n+2)=1-\alpha.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can slove the equation using R function &lt;code&gt;uniroot&lt;/code&gt;. The code is given below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;myfun &amp;lt;- function(c,n,alpha)
  pchisq(qchisq(pchisq(c,2*n)+1-alpha,2*n),2*n+2)-pchisq(c,2*n+2)-1+alpha

mysolver &amp;lt;- function(n,alpha){
  a = qchisq(alpha/2,2*n)
  b = qchisq(alpha/2,2*n+2)
  ## solve the equation by using the root finding algorithm
  r = uniroot(myfun,n=n,alpha=alpha,interval = c(a,b))
  c1 = r$root
  c2 = qchisq(pchisq(c1,2*n)+1-alpha,2*n)
  err1 = pchisq(c2,2*n+2)-pchisq(c1,2*n+2)-1+alpha #check the error for eq. (2)
  ## the approximate method
  c11 = qchisq(alpha/2,2*n)
  c22 = qchisq(1-alpha/2,2*n)
  err2 = pchisq(c22,2*n+2)-pchisq(c11,2*n+2)-1+alpha #check the error for eq. (2)
  output = data.frame(exact=c(c1,c2,abs(err1)),rough=c(c11,c22,abs(err2)),
                      row.names = c(&amp;quot;C1&amp;quot;,&amp;quot;C2&amp;quot;,&amp;quot;error&amp;quot;))
  return(output)
}
alpha = 0.5
n = 10
output = mysolver(n,alpha)
knitr::kable(output,&amp;quot;html&amp;quot;,caption = &amp;quot;n=10, alpha=0.05&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-2&#34;&gt;Table 1: &lt;/span&gt;n=10, alpha=0.05
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
exact
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
rough
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
C1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
16.00121
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15.451773
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
C2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
24.61524
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23.827692
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
error
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.00000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.014194
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;For large &lt;span class=&#34;math inline&#34;&gt;\(n=100\)&lt;/span&gt;, we have the following results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n = 100
output = mysolver(n,alpha)
knitr::kable(output,&amp;quot;html&amp;quot;,caption = &amp;quot;n=100, alpha=0.05&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-3&#34;&gt;Table 2: &lt;/span&gt;n=100, alpha=0.05
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
exact
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
rough
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
C1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
186.8010
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
186.171668
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
C2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
213.8065
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
213.102185
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
error
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.001428
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As expected, the error for the rough estimates &lt;span class=&#34;math inline&#34;&gt;\(C_1=\chi_{\alpha/2}^2(2n)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(C_2=\chi_{1-\alpha/2}^2(2n)\)&lt;/span&gt; decreases as &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; goes up.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt; be a sample from &lt;span class=&#34;math inline&#34;&gt;\(U[0,\theta]\)&lt;/span&gt;. Given a significance level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;, derive a UMP test of&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0:\theta=\theta_0\ vs.\ H_1:\theta&amp;gt;\theta_0.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Solution&lt;/code&gt;: Firstly, consider the simple alternative:
&lt;span class=&#34;math display&#34;&gt;\[H_0:\theta=\theta_0\ vs.\ H_1:\theta=\theta_1&amp;gt;\theta_0.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The likelihood function is &lt;span class=&#34;math inline&#34;&gt;\(L(\theta)=\theta^{-n}1\{x_{(n)}\le \theta\}\)&lt;/span&gt;.
The likelihood ratio for the simple test is
&lt;span class=&#34;math display&#34;&gt;\[\lambda(\vec x) = \frac{\theta_1^{-n}1\{x_{(n)}\le \theta_1\}}{\theta_0^{-n}1\{x_{(n)}\le \theta_0\}}
=\begin{cases}
(\theta_0/\theta_1)^n,\ &amp;amp;x_{(n)}\le \theta_0\\
\infty,\ &amp;amp;x_{(n)}&amp;gt; \theta_0
\end{cases}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We therefore cannot find a &lt;span class=&#34;math inline&#34;&gt;\(\lambda_0\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(P_{\theta_0}(\lambda(\vec X)&amp;gt;\lambda_0)=\alpha\)&lt;/span&gt;. This implies that the N-P lemma cannot be applied. As we can see, the likelihood ratio is a function of &lt;span class=&#34;math inline&#34;&gt;\(x_{(n)}\)&lt;/span&gt;. We thus can use &lt;span class=&#34;math inline&#34;&gt;\(X_{(n)}\)&lt;/span&gt; as the test statistic. A resonable rejection region would be
&lt;span class=&#34;math inline&#34;&gt;\(W = \{x_{(n)}&amp;gt;C\}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; satisfies
&lt;span class=&#34;math display&#34;&gt;\[P_{\theta_0}(X_{(n)}&amp;gt;C)=\alpha.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;When &lt;span class=&#34;math inline&#34;&gt;\(\theta=\theta_0\)&lt;/span&gt;, the order statistic &lt;span class=&#34;math inline&#34;&gt;\(X_{(n)}/\theta_0\)&lt;/span&gt; has a CDF &lt;span class=&#34;math inline&#34;&gt;\(F(x)=x^n\)&lt;/span&gt; (see &lt;a href=&#34;https://hezhijian.netlify.com/post/homework5/&#34;&gt;Exercise 1&lt;/a&gt;). So we have &lt;span class=&#34;math inline&#34;&gt;\(C=(1-\alpha)^{1/n}\theta_0\)&lt;/span&gt;. The rejection region is
&lt;span class=&#34;math display&#34;&gt;\[W = \{\vec x:x_{(n)}&amp;gt;(1-\alpha)^{1/n}\theta_0\}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We next prove that &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; is UMP. Suppose that there exists a rejection region &lt;span class=&#34;math inline&#34;&gt;\(W&amp;#39;\)&lt;/span&gt; satisfying &lt;span class=&#34;math inline&#34;&gt;\(P_{\theta_0}(\vec X\in W&amp;#39;)\le \alpha\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(A=[0,\theta_0]^n\)&lt;/span&gt; be the sample space when &lt;span class=&#34;math inline&#34;&gt;\(\theta=\theta_0\)&lt;/span&gt;, and let &lt;span class=&#34;math inline&#34;&gt;\(B=[0,\theta_1]^n\)&lt;/span&gt; be the sample space when &lt;span class=&#34;math inline&#34;&gt;\(\theta=\theta_1\)&lt;/span&gt;. It is clear that &lt;span class=&#34;math inline&#34;&gt;\(A\subseteq B\)&lt;/span&gt; since &lt;span class=&#34;math inline&#34;&gt;\(\theta_1&amp;gt;\theta_0\)&lt;/span&gt;. This implies
&lt;span class=&#34;math display&#34;&gt;\[P_{\theta_0}(\vec X\in W&amp;#39;) = P_{\theta_0}(\vec X\in W&amp;#39;\cap A)=\frac{1}{\theta_0^n}\int_{W&amp;#39;\cap A} 1 d x_1\dots d x_n\le \alpha.\]&lt;/span&gt;
Similarly,
&lt;span class=&#34;math display&#34;&gt;\[P_{\theta_0}(\vec X\in W) = P_{\theta_0}(\vec X\in W\cap A)=\frac{1}{\theta_0^n}\int_{W\cap A} 1 d x_1\dots d x_n= \alpha.\]&lt;/span&gt;
Define &lt;span class=&#34;math inline&#34;&gt;\(\mu(E) = \int_E 1 d x_1\dots d x_n\)&lt;/span&gt;. So we have &lt;span class=&#34;math inline&#34;&gt;\(\mu(W\cap A)\ge \mu(W&amp;#39;\cap A)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;On the other hand, noticing that &lt;span class=&#34;math inline&#34;&gt;\(W\cap B=W\cap A+\bar A\cap B\)&lt;/span&gt;, we thus have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
P_{\theta_1}(\vec X\in W) &amp;amp;=\frac{1}{\theta_1^n}\int_{W\cap B} 1 d x_1\dots d x_n\\&amp;amp;=\frac{1}{\theta_1^n}\int_{W\cap A}1 d x_1\dots d x_n+\frac{1}{\theta_1^n}\int_{\bar A\cap B}1 d x_1\dots d x_n\\
&amp;amp;=\theta_1^{-n}[\mu(W\cap A)+\mu(\bar A\cap B)]\\
&amp;amp;\ge \theta_1^{-n}[\mu(W&amp;#39;\cap A)+\mu(W&amp;#39;\cap\bar A\cap B)\\
&amp;amp;=\theta_1^{-n}\mu(W&amp;#39;\cap B)=P_{\theta_1}(\bar X\in W&amp;#39;).
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore, &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; is UMP rejection region. Since &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; does not depend on &lt;span class=&#34;math inline&#34;&gt;\(\theta_1\)&lt;/span&gt;, it is also the UMP rejection region for the alternative &lt;span class=&#34;math inline&#34;&gt;\(H_1:\theta&amp;gt;\theta_0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For this example, the UMP rejection region is not unique. Following the same procedure above, one can easily prove that for any
set &lt;span class=&#34;math inline&#34;&gt;\(W_0\subset A\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\mu(W_0)=\alpha\theta_0^n\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(W=W_0\cup \bar A\)&lt;/span&gt; is UMP.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(X_1,X_2,X_3,X_4\)&lt;/span&gt; be a sample from &lt;span class=&#34;math inline&#34;&gt;\(N(\theta,1)\)&lt;/span&gt;. Given a significance level &lt;span class=&#34;math inline&#34;&gt;\(\alpha=0.1\)&lt;/span&gt;, derive a UMP test of&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_0:\theta\ge 10\ vs.\ H_1:\theta&amp;lt;10.\]&lt;/span&gt;
Calculate the power of the test when &lt;span class=&#34;math inline&#34;&gt;\(\theta=9\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Solution&lt;/code&gt;: The test statistics is &lt;span class=&#34;math inline&#34;&gt;\(T(\vec x)=\frac{\bar x-10}{1/\sqrt{n}}=\sqrt{n}(\bar x-10)=2(\bar x-10)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(n=4\)&lt;/span&gt;. The UMP rejection region has the form &lt;span class=&#34;math inline&#34;&gt;\(W=\{T(\vec x)&amp;lt;C\}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; satisfies
&lt;span class=&#34;math display&#34;&gt;\[P(T(\vec X)&amp;lt;C|\theta=10)=\alpha=0.1\]&lt;/span&gt;
This gives &lt;span class=&#34;math inline&#34;&gt;\(C= u_{0.1}=-u_{0.9}=-1.28\)&lt;/span&gt;. So &lt;span class=&#34;math display&#34;&gt;\[W=\{\vec x|2(\bar x-10)&amp;lt;-1.28\}=\{\vec x|\bar x&amp;lt;9.36\}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The power of the test is
&lt;span class=&#34;math display&#34;&gt;\[P(\bar X&amp;lt;9.36|\theta=9)=P(2(\bar X-9)&amp;lt;0.72|\theta=9)=\Phi(0.72)=0.76.\]&lt;/span&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>第六次作业</title>
      <link>/post/homework6/</link>
      <pubDate>Fri, 26 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/homework6/</guid>
      <description>&lt;p&gt;True or false, and state why:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The significance level of a statistical test is equal to the probability that the
null hypothesis is true.&lt;/li&gt;
&lt;li&gt;If the significance level of a test is decreased, the power of the test would be expected to
increase.&lt;/li&gt;
&lt;li&gt;The probability that the null hypothesis is falsely rejected is equal to the power
of the test.&lt;/li&gt;
&lt;li&gt;A type I error occurs when the test statistic falls in the rejection region of the
test.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;Solution&lt;/code&gt;:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;FALSE&lt;/code&gt;. The significance level is the (tight) upper bound of the probability of type I error. It has nothing to do with the conclusion “the null hypothesis is true”.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;FALSE&lt;/code&gt;. If the significance level of a test is decreased, the probability of type II error would be expected to increase. This implies that the power of the test would be decreased.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;FALSE&lt;/code&gt;. The probability that the null hypothesis is falsely rejected is equal to the probability of type I error.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;FALSE&lt;/code&gt;. A type I error occurs when the test statistic falls in the rejection region of the test and &lt;strong&gt;the null is true&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;p&gt;A coin is thrown independently 10 times to test the hypothesis that the probability of heads is &lt;span class=&#34;math inline&#34;&gt;\(1/2\)&lt;/span&gt; versus the alternative that the probability is not &lt;span class=&#34;math inline&#34;&gt;\(1/2\)&lt;/span&gt;. The test rejects
if either 0 or 10 heads are observed.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;What is the significance level of the test?&lt;/li&gt;
&lt;li&gt;If in fact the probability of heads is &lt;span class=&#34;math inline&#34;&gt;\(0.1\)&lt;/span&gt;, what is the power of the test?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;Solution&lt;/code&gt;: Let &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; be the probability of heads. We are testing
&lt;span class=&#34;math display&#34;&gt;\[H_0:\theta=1/2\ vs.\ H_1:\theta\neq 1/2.\]&lt;/span&gt;
Denote &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; as the number of heads. The rejection region is
&lt;span class=&#34;math inline&#34;&gt;\(W=\{S=0 \text{ or }10\}\)&lt;/span&gt;. Under &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;, we have &lt;span class=&#34;math inline&#34;&gt;\(S\sim B(10,1/2)\)&lt;/span&gt;.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The significance level of the test is
&lt;span class=&#34;math display&#34;&gt;\[\alpha = P(W|\theta=0.5) = P(S=0|\theta=0.5)+P(S=10|\theta=0.5)=2^{-10}+2^{-10}=2^{-9}.\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The power of the test is
&lt;span class=&#34;math display&#34;&gt;\[\rho_W(0.1) = P(W|\theta=0.1) = 0.1^{10}+0.9^{10}=0.349.\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;p&gt;Suppose that &lt;span class=&#34;math inline&#34;&gt;\(X_1,X_2,X_3\)&lt;/span&gt; are samples of Bernoulli &lt;span class=&#34;math inline&#34;&gt;\(B(1,p)\)&lt;/span&gt; population. For testing the hypothesis &lt;span class=&#34;math inline&#34;&gt;\(H_0:p=1/2\ vs.\ H_1:p=3/4\)&lt;/span&gt;, we use a rejection region:
&lt;span class=&#34;math display&#34;&gt;\[W=\{(x_1,x_2,x_3):x_1+x_2+x_3\ge 2\}.\]&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;What are the probabilities of the two types of errors for &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;?&lt;/li&gt;
&lt;li&gt;What is the power of the test? Graph the power as a function of &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;Solution&lt;/code&gt;: Let &lt;span class=&#34;math inline&#34;&gt;\(S=X_1+X_2+X_3\)&lt;/span&gt;.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Under &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(S\sim B(3,1/2)\)&lt;/span&gt;. The probability of type I error is
&lt;span class=&#34;math display&#34;&gt;\[\alpha = P(S\ge 2|p=1/2)=C_3^22^{-3}+C_3^32^{-3}=0.5.\]&lt;/span&gt;
Under &lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(S\sim B(3,3/4)\)&lt;/span&gt;. The probability of type II error is
&lt;span class=&#34;math display&#34;&gt;\[\beta = P(S&amp;lt; 2|p=3/4)=C_3^0(1/4)^{3}+C_3^1(3/4)(1/4)^2=\frac 5{32}.\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The power of the test is &lt;span class=&#34;math inline&#34;&gt;\(1-\beta=27/32\)&lt;/span&gt;. The power function is
&lt;span class=&#34;math display&#34;&gt;\[\rho_W(p) = P(S\ge 2) = C_3^2p^2(1-p)+C_3^3p^3=3p^2-2p^3.\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p = seq(0,1,by=0.001)
power = 3*p^2-2*p^3
par(mar=c(4,4,1,1))
plot(p,power,type=&amp;quot;l&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/homework6_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;hr /&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 10: Bayesian computation</title>
      <link>/post/bayes_chap10/</link>
      <pubDate>Mon, 22 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/bayes_chap10/</guid>
      <description>&lt;div id=&#34;introduction-to-bayesian-computation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction to Bayesian computation&lt;/h2&gt;
&lt;p&gt;The goals are to estimate&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the posterior distribution &lt;span class=&#34;math inline&#34;&gt;\(p(\theta|y)\propto p(\theta)p(y|\theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the posterior predictive distribution
&lt;span class=&#34;math display&#34;&gt;\[p(\tilde y|y) = \int p(\tilde y|\theta)p(\theta|y)d \theta =E[p(\tilde y|\theta)|y]\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We are therefore insterested in estimating the posterior expectation
&lt;span class=&#34;math display&#34;&gt;\[\mu=E[h(\theta)|y]=\int h(\theta)p(\theta|y)d \theta\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;moments: &lt;span class=&#34;math inline&#34;&gt;\(h(\theta)=\theta^k\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;probability: &lt;span class=&#34;math inline&#34;&gt;\(h(\theta)=1_A(\theta)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(A\subseteq \Theta\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;predictive density: &lt;span class=&#34;math inline&#34;&gt;\(h(\theta)=p(\tilde y|\theta)\)&lt;/span&gt; for fixed &lt;span class=&#34;math inline&#34;&gt;\(\tilde y\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;monte-carlo-methods&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Monte Carlo methods&lt;/h2&gt;
&lt;p&gt;Suppose we can simulate &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(i)},\dots,\theta^{(N)}\sim p(\theta|y)\)&lt;/span&gt; independently. Monte Carlo (MC) esimate is then the sample average:
&lt;span class=&#34;math display&#34;&gt;\[\hat{\mu}_N = \frac1N\sum_{i=1}^N h(\theta^{(i)})\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LLN: &lt;span class=&#34;math inline&#34;&gt;\(\hat\mu_N\to \mu\)&lt;/span&gt; w.p.1 as &lt;span class=&#34;math inline&#34;&gt;\(N\to \infty\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;CLT: &lt;span class=&#34;math inline&#34;&gt;\(\hat\mu_N-\mu=O_p(N^{-1/2})\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Traditional quadrature rules’ have error rate &lt;span class=&#34;math inline&#34;&gt;\(O(N^{-r/d})\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(r\ge 1\)&lt;/span&gt; depends on the smoothness of the functions, and &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is the dimension of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. This suffers &lt;strong&gt;the curse of dimensionality&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;MC has an error rate &lt;span class=&#34;math inline&#34;&gt;\(O(N^{-1/2})\)&lt;/span&gt; independently of the smoothness and the dimension of the functions. The task is to simulate iid samples &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(i)}\sim p(\theta|y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;random-number-generators&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Random number generators&lt;/h2&gt;
&lt;p&gt;We start with a pseudo-random number generator:
&lt;span class=&#34;math display&#34;&gt;\[u_1,\dots,u_n,\dots\stackrel{iid}\sim U(0,1)\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Mersenne Twister&lt;/strong&gt; by Matsumoto &amp;amp; Nishimura (1998), whose period is &lt;span class=&#34;math inline&#34;&gt;\(2^{19937}-1&amp;gt;10^{6000}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RngStreams&lt;/strong&gt; by L’Ecuyer, Simard, Chen, Kelton (2002)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;They aren’t really uniform random, but good ones are close enough.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;non-uniform-random-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Non-uniform random variables&lt;/h2&gt;
&lt;p&gt;Some common distributions (such as Normal, exponential, binomial, Poisson etc.) are already in some scientific softwares (R, Python, Matlab, Julia, Mathematica, etc.)&lt;/p&gt;
&lt;p&gt;We are now concerned with a general distribution. Principled approaches are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;inversion&lt;/li&gt;
&lt;li&gt;acceptance-rejection&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;inversion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Inversion&lt;/h2&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(F(x)\)&lt;/span&gt; be the CDF of the distribution of interest. We can simulate the distribution via iid uniforms &lt;span class=&#34;math inline&#34;&gt;\(U_1,\dots,U_N\stackrel{iid}\sim U(0,1)\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X_i=F^{-1}(U_i),i=1,\dots,N\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(F^{-1}\)&lt;/span&gt; is the inverse of the CDF &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;, definited by
&lt;span class=&#34;math display&#34;&gt;\[F^{-1}(u)=\inf\{x\in\mathbb{R}|F(x)\ge u\}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;it is easy to see that &lt;span class=&#34;math inline&#34;&gt;\(X_i\stackrel{iid}\sim F\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;inversion-examples&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Inversion: examples&lt;/h2&gt;
&lt;div id=&#34;gaussian&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Gaussian&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Z=\Phi^{-1}(U)\sim N(0,1)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X=\mu+\sigma\Phi^{-1}(U) \sim N(\mu,\sigma^2)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exponential&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exponential&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X= -\frac 1\lambda \log (1-U)\sim Exp(\lambda)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bernoulli&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bernoulli&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X=1\{U\le p\}\sim Bin(1,p)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;multivariate-inverse-transformation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multivariate inverse transformation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;let &lt;span class=&#34;math inline&#34;&gt;\(F(x_1,\dots,x_d)\)&lt;/span&gt; be the PDF of &lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_d\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;let &lt;span class=&#34;math inline&#34;&gt;\(F_i(x_i)\)&lt;/span&gt; be the marginal distribution of &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;for &lt;span class=&#34;math inline&#34;&gt;\(i=2,\dots,d\)&lt;/span&gt;, let &lt;span class=&#34;math inline&#34;&gt;\(F_i(x_i|x_1,\dots,x_{i-1})\)&lt;/span&gt; be the conditional CDF&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The multivariate inverse transformation is proposed by Rosenblatt (1952), which simulates the components &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt; recursively, i.e.,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X_1=F_1^{-1}(U_1)\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[X_i=F_i^{-1}(U_i|X_1,\dots,X_{i-1}),\ i=2,\dots,d\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the output has the destribution &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;the order of simulating the components can be arbitrary&lt;/li&gt;
&lt;li&gt;the critical issue is to know the conditional CDFs in advance&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;acceptance-rejection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Acceptance-rejection&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;suppose the target distrubtion is &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; with the support &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;we can sample &lt;span class=&#34;math inline&#34;&gt;\(Y\sim g\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; is another density satisfying: there exists &lt;span class=&#34;math inline&#34;&gt;\(M&amp;gt;0\)&lt;/span&gt; such that
&lt;span class=&#34;math display&#34;&gt;\[\frac{f(x)}{g(x)}\le M\ \forall x\in \mathcal{X}\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;we can compute &lt;span class=&#34;math inline&#34;&gt;\(f(x)/g(x)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The algorithm goes below&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Step 1: simulate &lt;span class=&#34;math inline&#34;&gt;\(Y\sim g\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 2: accept &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; as a draw from &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; with probability &lt;span class=&#34;math inline&#34;&gt;\(f(Y)/(Mg(Y))\)&lt;/span&gt;. If the draw is rejected, return to Step 1.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 2’: simulate &lt;span class=&#34;math inline&#34;&gt;\(U\sim U(0,1)\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[
\begin{cases}
\text{accept } Y &amp;amp; U\le f(Y)/(Mg(Y))\\
\text{go to Step 1 }&amp;amp; else
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;acceptance-rejection-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Acceptance-rejection&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;AR.png&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the acceptance probability: &lt;span class=&#34;math display&#34;&gt;\[E[f(Y)/(Mg(Y))]=\frac 1 M\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;we may choose the smallest &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(f(x)\le Mg(x)\)&lt;/span&gt; for all &lt;span class=&#34;math inline&#34;&gt;\(x\in\mathcal{X}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;acceptance-rejection-for-bayesian-computation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Acceptance-rejection for Bayesian computation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the target density
&lt;span class=&#34;math display&#34;&gt;\[p(\theta|y)=\frac{p(\theta)p(y|\theta)}{p(y)}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the constant &lt;span class=&#34;math inline&#34;&gt;\(p(y)\)&lt;/span&gt; is unknown&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the AR algorithm works well if taking &lt;span class=&#34;math inline&#34;&gt;\(f(\theta)=p(\theta)p(y|\theta)\)&lt;/span&gt;
and using proposal density &lt;span class=&#34;math inline&#34;&gt;\(\propto g(\theta)\)&lt;/span&gt; with
&lt;span class=&#34;math display&#34;&gt;\[\frac{p(\theta)p(y|\theta)}{g(\theta)}\le M\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;example-gamma-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example: Gamma distribution&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Gamma(\alpha,\lambda)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\alpha&amp;gt;0\)&lt;/span&gt; is the shape, &lt;span class=&#34;math inline&#34;&gt;\(\lambda&amp;gt;0\)&lt;/span&gt; is the rate&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;density&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f(x) = \frac{\lambda^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\lambda x}1\{x&amp;gt; 0\}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Gamma(\alpha,\lambda)\stackrel{d}{=}\frac 1 \lambda Gamma(\alpha,1)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Gamma(\alpha,1)\stackrel{d}{=}U(0,1)^{1/\alpha}Gamma(\alpha+1,1)\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;so our target is &lt;span class=&#34;math inline&#34;&gt;\(Gamma(\alpha,1)\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\alpha&amp;gt;1\)&lt;/span&gt;. For this case, the density is bounded.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the proposal
&lt;span class=&#34;math display&#34;&gt;\[g(x)=?\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;gamma-density&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gamma density&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayes_chap10_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Ahrens and Dieter (1974) took proposals from a density that combines a
Gaussian density in the center and an exponential density in the right tail.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Marsaglia and Tsang (2000) present an AR algorithm from a truncated
&lt;span class=&#34;math inline&#34;&gt;\(N(0,1)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;example-beta-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example: Beta distribution&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Beta(\alpha,\beta)\)&lt;/span&gt; density
&lt;span class=&#34;math display&#34;&gt;\[f(x)=\frac{\Gamma(\alpha,\beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1}1\{0&amp;lt;x&amp;lt;1\}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;generate a Beta from two independent Gammas
&lt;span class=&#34;math display&#34;&gt;\[Beta(\alpha,\beta)\stackrel{d}{=}\frac{Gamma(\alpha,\lambda)}{Gamma(\alpha,\lambda)+Gamma(\beta,\lambda)}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;for &lt;span class=&#34;math inline&#34;&gt;\(\alpha&amp;gt;1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta&amp;gt;1\)&lt;/span&gt;, the beta density is unimodal and achieves its maximum at &lt;span class=&#34;math inline&#34;&gt;\(x^*=(\alpha-1)/(\alpha+\beta-2)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;beta-density&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Beta density&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayes_chap10_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Proposal distribution: &lt;span class=&#34;math inline&#34;&gt;\(U(0,1)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(M=f(x^*)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;accept &lt;span class=&#34;math inline&#34;&gt;\(U\sim U(0,1)\)&lt;/span&gt; with probability &lt;span class=&#34;math inline&#34;&gt;\(f(U)/M\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;beta-generator-r-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Beta generator: R code&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;myBeta &amp;lt;- function(n,alpha,beta){
  if(alpha&amp;lt;=1 | beta&amp;lt;=1)
    stop(&amp;quot;alpha, beta cannot be &amp;lt;= 1&amp;quot;)
  M = dbeta((alpha-1)/(alpha+beta-2),alpha,beta)
  x = rep(0,n)
  for(i in 1:n){
    while (TRUE){
      U = runif(1)
      if(dbeta(U,alpha,beta)&amp;gt;= M*runif(1)){
        x[i] = U
        break
      }
    }
  }
  return(x)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;simulation-results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulation results&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayes_chap10_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-5&#34;&gt;Table 1: &lt;/span&gt;&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mean&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;sd&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;myBeta&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4001780&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1968478&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;dbeta&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3996059&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2000705&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;true values&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2000000&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;importance-sampling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Importance Sampling&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;the target is to estimate &lt;span class=&#34;math inline&#34;&gt;\(\mu=E_f[h(X)]\)&lt;/span&gt; w.r.t. the density &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the proposal density &lt;span class=&#34;math inline&#34;&gt;\(q(x)\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(q(x)&amp;gt;0\)&lt;/span&gt; whenever &lt;span class=&#34;math inline&#34;&gt;\(h(x)f(x)&amp;gt;0\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\mu=\int h(x)f(x)dx=\int h(x)\frac{f(x)}{g(x)}g(x)dx=E_g[h(X)f(X)/g(X)]\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(f(x)/g(x)\)&lt;/span&gt; called the &lt;strong&gt;likelihood ratio (LR)&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The IS algorithm goes below&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Step 1: simulate &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; samples &lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\(g(x)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Step 2: compute the sample average:
&lt;span class=&#34;math display&#34;&gt;\[\hat{\mu}_{IS}=\frac 1N\sum_{i=1}^N \frac{h(X_i)f(X_i)}{g(X_i)}\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;choosing-the-proposal&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Choosing the proposal&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Var[\hat{\mu}_{IS}] = \frac{\sigma^2_g}{N}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma^2_g = \int \left(\frac{h(x)f(x)}{g(x)}-\mu\right)^2g(x)d x=\int\frac{(h(x)f(x)-\mu g(x))^2}{g(x)}dx\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;if &lt;span class=&#34;math inline&#34;&gt;\(g(x)=h(x)f(x)/\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(h\ge 0\)&lt;/span&gt;, then we have &lt;strong&gt;the optimal case&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_g=0\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;but unattainable: &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is unknown constant&lt;/li&gt;
&lt;li&gt;we may find &lt;span class=&#34;math inline&#34;&gt;\(g(x)\approx h(x)f(x)/\mu\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-weight-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The weight function&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;let &lt;span class=&#34;math inline&#34;&gt;\(w(x)=f(x)/g(x)\)&lt;/span&gt; be the LR
&lt;span class=&#34;math display&#34;&gt;\[\sigma^2_g =\int \frac{(hf)^2}{g}dx -\mu^2\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\int \frac{(hf)^2}{g}dx=E_f[w(X)h(X)^2]=E_g[w(X)^2h(X)^2]\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;if &lt;span class=&#34;math inline&#34;&gt;\(w(x)\)&lt;/span&gt; is bounded, then &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_g\)&lt;/span&gt; is bounded&lt;/li&gt;
&lt;li&gt;if &lt;span class=&#34;math inline&#34;&gt;\(w(x)\)&lt;/span&gt; is unbounded, then &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_g\)&lt;/span&gt; may be unbounded (&lt;strong&gt;the worst case!&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;self-normalized-is-snis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Self-normalized IS (SNIS)&lt;/h2&gt;
&lt;p&gt;What if we cannot compute &lt;span class=&#34;math inline&#34;&gt;\(f/g\)&lt;/span&gt;? Suppose that
&lt;span class=&#34;math display&#34;&gt;\[f(x)=c_f\tilde{f}(x),\ g(x)=c_g\tilde{g}(x)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and we can compute &lt;span class=&#34;math inline&#34;&gt;\(\tilde f,\tilde g\)&lt;/span&gt; but not the constants &lt;span class=&#34;math inline&#34;&gt;\(c_f,c_g\)&lt;/span&gt;. Then we use&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{\mu}_{SNIS}= \frac{\frac 1 N\sum_{i=1}^nh(X_i)\tilde{f}(X_i)/\tilde{g}(X_i)}{\frac 1 N\sum_{i=1}^n\tilde{f}(X_i)/\tilde{g}(X_i)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;or, equivalently,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{\mu}_{SNIS}= \frac{\frac 1 N\sum_{i=1}^Nh(X_i){f}(X_i)/{g}(X_i)}{\frac 1 N\sum_{i=1}^N{f}(X_i)/{g}(X_i)}=\frac{\frac 1 N\sum_{i=1}^Nh(X_i)w(X_i)}{\frac 1 N\sum_{i=1}^Nw(X_i)}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;variance-of-snis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Variance of SNIS&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Taylor expansions
&lt;span class=&#34;math display&#34;&gt;\[f(\bar X,\bar Y)\approx f(\mu_1,\mu_2)+f_x(\mu_1,\mu_2)(\bar X-\mu_1)+f_y(\mu_1,\mu_2)(\bar Y-\mu_2)\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[E[f(\bar X,\bar Y)]\approx f(\mu_1,\mu_2)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Var[f(\bar X,\bar Y)]\approx f_x^2Var[\bar X]+f_y^2Var[\bar Y]+2f_xf_yCov(\bar X,\bar Y)\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;for &lt;span class=&#34;math inline&#34;&gt;\(f(x,y)=x/y\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(f_x=1/y,f_y=-x/y^2\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[Var[f(\bar X,\bar Y)]\approx \frac{\sigma_X^2}{N\mu_2^2}+\frac{\mu_1^2\sigma_Y^2}{N\mu_2^4}-\frac{2\mu_1}{N\mu_2^3}Cov(X,Y)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Var[\hat{\mu}_{SNIS}]\approx \frac{1}{N}E_g[w(X)^2(h(X)-\mu)^2]\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Var[\hat{\mu}_{IS}]= \frac{1}{N}E_g[(h(X)w(X)-\mu)^2]\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;optimal-snis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Optimal SNIS&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;SNIS: &lt;span class=&#34;math inline&#34;&gt;\(g_{opt}(x)\propto f(x)|h(x)-\mu|\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;IS: &lt;span class=&#34;math inline&#34;&gt;\(g_{opt}(x)\propto f(x)|h(x)|\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;effective-sample-size&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Effective sample size&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Unequal weighting raises variance, see, Kong (1992), Evans and Swartz (1995)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;for iid &lt;span class=&#34;math inline&#34;&gt;\(Y_i\)&lt;/span&gt; with variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; and fixed &lt;span class=&#34;math inline&#34;&gt;\(w_i\ge 0\)&lt;/span&gt;,
&lt;span class=&#34;math display&#34;&gt;\[Var\left(\frac{\sum_{i}w_iY_i}{\sum_iw_i}\right)=\frac{\sum_iw_i^2\sigma^2}{(\sum_iw_i)^2}=\frac{\sigma^2}{N_{eff}}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where the effective sample size &lt;span class=&#34;math inline&#34;&gt;\(N_{eff}\)&lt;/span&gt; is defined as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[N_{eff} = \frac{(\sum_{i=1}^Nw_i)^2}{\sum_{i=1}^Nw_i^2}\in [1,N]\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(N_{eff}\)&lt;/span&gt; is small if there are few extremely high weights which would unduly influence the distribution&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;for equal weights, we have &lt;span class=&#34;math inline&#34;&gt;\(N_{eff}=N\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;example-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 1&lt;/h2&gt;
&lt;p&gt;Suppose the posterior distribution is &lt;span class=&#34;math inline&#34;&gt;\(N(\mu,\sigma^2)\)&lt;/span&gt;, the proposal distribution is &lt;span class=&#34;math inline&#34;&gt;\(t_3(\mu,\sigma^2)\)&lt;/span&gt;. Consider &lt;span class=&#34;math inline&#34;&gt;\(\mu=\sigma=2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayes_chap10_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-1-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 1&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Effective sample size is  9178 / 10000&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayes_chap10_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-7&#34;&gt;Table 2: &lt;/span&gt;&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n=100&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n=1000&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n=10000&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;exact_value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Mean&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.214328&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.011347&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.945335&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Variance&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.069694&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.688183&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.052519&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;example-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 2&lt;/h2&gt;
&lt;p&gt;Suppose the posterior distribution is &lt;span class=&#34;math inline&#34;&gt;\(t_3(\mu,\sigma^2)\)&lt;/span&gt;, the proposal distribution is &lt;span class=&#34;math inline&#34;&gt;\(N(\mu,\sigma^2)\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Effective sample size is  6180 / 10000&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayes_chap10_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-8&#34;&gt;Table 3: &lt;/span&gt;&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n=100&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n=1000&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n=10000&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;exact_value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Mean&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.802681&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.784954&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.019707&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Variance&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.630305&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.931088&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.875331&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;is-vs-acceptance-rejection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;IS vs acceptance rejection&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Acceptance-rejection requires bounded LR &lt;span class=&#34;math inline&#34;&gt;\(f/g\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We also have to know a bound&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;IS and SNIS require us to keep track of weights&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Plain IS requires normalized &lt;span class=&#34;math inline&#34;&gt;\(f/g\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Acceptance-rejection samples cost more (due to rejections)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;is-for-rare-events&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;IS for rare events&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;rare events:
&lt;span class=&#34;math display&#34;&gt;\[h(x)=1_A(x), \mu = E_f[h(x)]=\int_A f(x) dx=\epsilon\approx 0\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;coefficient of variation of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[cv:=\frac{\sigma/\sqrt{N}}{\mu}=\frac{\sqrt{\epsilon(1-\epsilon)}}{\sqrt{n}\epsilon}\approx \frac{1}{\sqrt{n\epsilon}}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;to get &lt;span class=&#34;math inline&#34;&gt;\(cv=0.1\)&lt;/span&gt; takes &lt;span class=&#34;math inline&#34;&gt;\(N\ge 100/\epsilon\)&lt;/span&gt;, e.g., &lt;span class=&#34;math inline&#34;&gt;\(\epsilon = 10^{-5}\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(N\ge 10^7\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Taking &lt;span class=&#34;math inline&#34;&gt;\(X\sim f\)&lt;/span&gt; does not get enough data from the important region &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Get more data from &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; (from a proper proposal &lt;span class=&#34;math inline&#34;&gt;\(g(x)\)&lt;/span&gt;), and then correct the bias (the LR function)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;changing-a-parameter&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Changing a parameter&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;norminal distribution &lt;span class=&#34;math inline&#34;&gt;\(p(x;\theta_0)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\theta_0\in\Theta\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;proposal distribution &lt;span class=&#34;math inline&#34;&gt;\(p(x;\theta)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\theta\in\Theta\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;estimator
&lt;span class=&#34;math display&#34;&gt;\[\hat\mu_\theta=\frac{1}{N}\sum_{i=1}^N h(X_i) \frac{p(X_i;\theta_0)}{p(X_i;\theta)}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The importance ratio often simplifies, e.g., in exponential families.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exponential-tilting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exponential tilting&lt;/h2&gt;
&lt;p&gt;Many important distributions can be written in the form
&lt;span class=&#34;math display&#34;&gt;\[p(x;\theta) = a(\theta)\exp[\eta(\theta)^\top T(x)]b(x), \theta\in \Theta\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat\mu_\theta=\frac{a(\theta_0)}{a(\theta)}\frac{1}{N}\sum_{i=1}^N h(X_i) \exp[(\eta(\theta_0)-\eta(\theta))^\top T(X_i)]\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\eta(\theta)\)&lt;/span&gt; is the natrual parameter&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This is called the ‘exponential twisting’.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The goal is to choose &lt;span class=&#34;math inline&#34;&gt;\(\theta\in\Theta\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(Var[\hat\mu_\theta]\)&lt;/span&gt; is minimized.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;a-simple-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A simple example&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;norminal distribution &lt;span class=&#34;math inline&#34;&gt;\(p(x;\theta_0)=N(x;0,1)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;proposal distribution &lt;span class=&#34;math inline&#34;&gt;\(p(x;\theta)=N(x;\theta,1)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\theta\in\mathbb{R}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;target function &lt;span class=&#34;math inline&#34;&gt;\(h(x) = 1\{x&amp;gt;c\}\)&lt;/span&gt;, for large &lt;span class=&#34;math inline&#34;&gt;\(c&amp;gt;0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mu=E[h(X)]=1-\Phi(c)\approx 0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;IS estimator
&lt;span class=&#34;math display&#34;&gt;\[\hat\mu_\theta=\frac{1}{N}\sum_{i=1}^N h(X_i) \frac{N(X_i;0,1)}{N(X_i;\theta,1)}=\frac{1}{N}\sum_{i=1}^N h(X_i) e^{-\frac{2\theta X_i-\theta^2}{2}}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;IS variance &lt;span class=&#34;math inline&#34;&gt;\(Var[\hat\mu_\theta]=\sigma^2_\theta/N\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\sigma^2_\theta=\frac{e^{\theta^2}}{\sqrt{2\pi}}\int_c^\infty e^{-\frac{(x+\theta)^2}{2}}dx-\mu^2=\frac{e^{\theta^2}[1-\Phi(c+\theta)]}{\sqrt{2\pi}}-\mu^2\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the optimal parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta^*=\arg \min_{\theta\in \mathbb{R}} e^{\theta^2}[1-\Phi(c+\theta)]\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-effect-of-different-parameters&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The effect of different parameters&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayes_chap10_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;the threshold c = 3&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;the true value is 0.0013498980316301&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;the optimal theta is 3.155&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;variance reduction factor is 404&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;applications-in-computational-finance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Applications in Computational Finance&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;P. Glasserman, P. Heidelberger, and P. Shahabuddin. Asymptotically optimal importance
sampling and stratification for pricing path-dependent options. &lt;em&gt;Mathematical Finance&lt;/em&gt;, 9
(2):117–152, 1999.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;P. Glasserman, P. Heidelberger, and P. Shahabuddin. Variance reduction techniques for
estimating value-at-risk. &lt;em&gt;Management Science&lt;/em&gt;, 46(10):1349–1364, 2000.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;P. Glasserman, J. Li. Importance Sampling for Portfolio Credit Risk. &lt;em&gt;Management Science&lt;/em&gt;, 51(11):1643–1656, 2005.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Xie, Fei, &lt;strong&gt;Zhijian He&lt;/strong&gt;, and Xiaoqun Wang. An Importance Sampling-Based Smoothing Approach for Quasi-Monte Carlo Simulation of Discrete Barrier Options. &lt;em&gt;European Journal of Operational Research&lt;/em&gt;, October 17, 2018.
&lt;a href=&#34;https://doi.org/10.1016/j.ejor.2018.10.030&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/j.ejor.2018.10.030&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;importance-sampling-for-portfolio-credit-risk&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Importance Sampling for Portfolio Credit Risk&lt;/h2&gt;
&lt;p&gt;Our interest centers on the distribution of losses
from default over a fixed horizon.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt;: number of obligors&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y_k\)&lt;/span&gt;: default indicator for &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;th obligor, &lt;span class=&#34;math inline&#34;&gt;\(Y_k=1\)&lt;/span&gt; denotes the default; &lt;span class=&#34;math inline&#34;&gt;\(Y_k=0\)&lt;/span&gt; otherwise&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(p_k\)&lt;/span&gt;: marginal probability that &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;th obligor defaults&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(c_k\)&lt;/span&gt;: loss resulting from default of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;th obligor&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(L=c_1Y_1+\dots+c_mY_m\)&lt;/span&gt;: total loss from defaults&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Our goal is to estimate tail probabilities &lt;span class=&#34;math inline&#34;&gt;\(P(L&amp;gt;x)\)&lt;/span&gt;, especially at large values of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;normal-copula-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Normal copula model&lt;/h2&gt;
&lt;p&gt;In the normal copula model, dependence
is introduced through a multivariate normal vector &lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_m\)&lt;/span&gt; of latent variables. Each default indicator is represented as
&lt;span class=&#34;math display&#34;&gt;\[Y_k = 1\{X_k&amp;gt; x_k\},\ k=1,\dots,m.\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[X_k = a_{k1}Z_1+\dots+a_{kd}Z_d+b_k\epsilon_k\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_k\)&lt;/span&gt; are chosen to match &lt;span class=&#34;math inline&#34;&gt;\(P(X_k&amp;gt;x_k)=p_k\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Z_1,\dots,Z_d\stackrel{iid}{\sim} N(0,1)\)&lt;/span&gt; are systematic risk factors&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\epsilon_k\stackrel{iid}{\sim} N(0,1)\)&lt;/span&gt; is an idiosyncratic risk&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(a_{k1},\dots,a_{kd}\)&lt;/span&gt; are the loading factors satisfying &lt;span class=&#34;math inline&#34;&gt;\(\sum_{j=1}^d a_{kj}^2\le 1\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(b_k=\sqrt{1-\sum_{j=1}^d a_{kj}^2}\)&lt;/span&gt; so that &lt;span class=&#34;math inline&#34;&gt;\(X_k\sim N(0,1)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;is-for-independent-obligors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;IS for independent obligors&lt;/h2&gt;
&lt;p&gt;Consider the simple case of independent obligors: &lt;span class=&#34;math inline&#34;&gt;\(a_{ij}=0,\ b_k=1\)&lt;/span&gt;, i.e., &lt;span class=&#34;math inline&#34;&gt;\(Y_k\sim Bin(1,p_k)\)&lt;/span&gt; independently. The idea is to replace each default probability &lt;span class=&#34;math inline&#34;&gt;\(p_k\)&lt;/span&gt; by some other default probability &lt;span class=&#34;math inline&#34;&gt;\(q_k\)&lt;/span&gt;, the basic IS identity is
&lt;span class=&#34;math display&#34;&gt;\[P(L&amp;gt;x)= \tilde{E}\left[1\{L&amp;gt;x\}\prod_{k=1}^m\frac{p_k^{Y_k}(1-p_k)^{1-Y_k}}{q_k^{Y_k}(1-q_k)^{1-Y_k}}\right]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Exponential Twisting&lt;/strong&gt;: Glasserman and Li (2005) chooses
&lt;span class=&#34;math display&#34;&gt;\[q_{k,\theta} = \frac{p_ke^{\theta c_k}}{1+p_k(e^{\theta c_k}-1)}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The original probabilities correspond to &lt;span class=&#34;math inline&#34;&gt;\(\theta=0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;if &lt;span class=&#34;math inline&#34;&gt;\(\theta&amp;gt;0\)&lt;/span&gt;, this does indeed increase the default
probabilities; a larger exposure &lt;span class=&#34;math inline&#34;&gt;\(c_k\)&lt;/span&gt; results in a greater
increase in the default probability.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;choosing-the-optimal-parameter&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Choosing the optimal parameter&lt;/h2&gt;
&lt;p&gt;The LR is reduced to
&lt;span class=&#34;math display&#34;&gt;\[\prod_{k=1}^m\frac{p_k^{Y_k}(1-p_k)^{1-Y_k}}{q_k^{Y_k}(1-q_k)^{1-Y_k}}=\exp(-\theta L+\psi(\theta))\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where
&lt;span class=&#34;math display&#34;&gt;\[\psi(\theta)=\log E[e^{\theta L}]=\sum_{k=1}^m \log(1+p_k(e^{\theta c_k}-1))\]&lt;/span&gt;
is the cumulant generating function (CGF) of L.&lt;/p&gt;
&lt;p&gt;The optimal parameter is
&lt;span class=&#34;math display&#34;&gt;\[\theta^* = \arg \min_{\theta\ge 0} \{M_2(\theta)=E_\theta[1\{L&amp;gt;x\}e^{-2\theta L+2\psi(\theta)}]\}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;choosing-the-sub-optimal-parameter&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Choosing the sub-optimal parameter&lt;/h2&gt;
&lt;p&gt;Observe that for &lt;span class=&#34;math inline&#34;&gt;\(\theta\ge 0\)&lt;/span&gt;,
&lt;span class=&#34;math display&#34;&gt;\[M_2(\theta)\le e^{-2\theta x+2\psi(\theta)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Minimizing &lt;span class=&#34;math inline&#34;&gt;\(M_2(\theta)\)&lt;/span&gt; is difficult, but minimizing
the upper bound is easy:
&lt;span class=&#34;math display&#34;&gt;\[\theta_x = \arg \min_{\theta\ge 0}e^{-2\theta x+2\psi(\theta)}=\arg \max_{\theta\ge 0} \{\theta x-\psi(\theta)\}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The function &lt;span class=&#34;math inline&#34;&gt;\(\psi(\theta)\)&lt;/span&gt; is strictly convex and passes through the origin, so the maximum
is attained at
&lt;span class=&#34;math display&#34;&gt;\[\theta_x = 
\begin{cases}
\text{unique solution to }\psi&amp;#39;(\theta)=x,\ &amp;amp;x&amp;gt;\psi&amp;#39;(0)\\
0,\ &amp;amp;x\le \psi&amp;#39;(0).
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;for the first case, &lt;span class=&#34;math inline&#34;&gt;\(E_{\theta_x}[L]=\psi&amp;#39;(\theta_x)=x\)&lt;/span&gt;, thus, we have shifted the distribution of L so that x is now its mean.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;for the second case, the event &lt;span class=&#34;math inline&#34;&gt;\(\{L&amp;gt;x\}\)&lt;/span&gt; is not rare, so we do not change the probabilities.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;dependent-obligors-conditional-importance-sampling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Dependent Obligors: Conditional Importance Sampling&lt;/h2&gt;
&lt;p&gt;For general factor models, &lt;span class=&#34;math inline&#34;&gt;\(Y_k\)&lt;/span&gt; are dependent; but they are independent conditinal on the systematic risk factors &lt;span class=&#34;math inline&#34;&gt;\(Z_1,\dots,Z_d\)&lt;/span&gt;. So we can apply the so-called conditional IS.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Step 1: simulate &lt;span class=&#34;math inline&#34;&gt;\(Z_1,\dots,Z_d\sim N(0,1)\)&lt;/span&gt; and compute the default probability
&lt;span class=&#34;math inline&#34;&gt;\(p_k=p_k(Z_1,\dots,Z_d)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 2: for simulated &lt;span class=&#34;math inline&#34;&gt;\(p_k\)&lt;/span&gt;, obtain the twisting parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta_x=\theta_x(Z_1,\dots,Z_d)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 3: compute the LR for the &lt;span class=&#34;math inline&#34;&gt;\(\theta_x\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 4: repeat Steps 1–4 &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; times and then obtain the final IS estimate&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;numerical-results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Numerical results&lt;/h2&gt;
&lt;p&gt;The numerical results were reported in Glasserman and Li (2005).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(21\)&lt;/span&gt;-factor model with &lt;span class=&#34;math inline&#34;&gt;\(m=1000\)&lt;/span&gt; obligors&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(p_k = 0.01(1+\sin(16\pi k/m))\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(c_k=(\lceil5k/m\rceil)^2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;VRF = “Variance Reduction Factor”&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(L&amp;gt;x)\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;VRF&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;10,000&lt;/td&gt;
&lt;td&gt;0.0114&lt;/td&gt;
&lt;td&gt;33&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;14,000&lt;/td&gt;
&lt;td&gt;0.0065&lt;/td&gt;
&lt;td&gt;53&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;18,000&lt;/td&gt;
&lt;td&gt;0.0037&lt;/td&gt;
&lt;td&gt;83&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;22,000&lt;/td&gt;
&lt;td&gt;0.0021&lt;/td&gt;
&lt;td&gt;125&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;30,000&lt;/td&gt;
&lt;td&gt;0.0006&lt;/td&gt;
&lt;td&gt;278&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;40,000&lt;/td&gt;
&lt;td&gt;0.0001&lt;/td&gt;
&lt;td&gt;977&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;extensions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Extensions&lt;/h2&gt;
&lt;p&gt;The defual indicators&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y_k=1\{X_k&amp;gt;x_k\}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(X_k\)&lt;/span&gt; follow t copula model&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Joshua C.C. Chan, Dirk P. Kroese. Efficient estimation of large portfolio loss probabilities in t-copula models. &lt;em&gt;European Journal of Operational Research&lt;/em&gt;, 205:361–367, 2010.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(X_k\)&lt;/span&gt; follow another advanced models, e.g., self-exciting model, Giesecke et al. (2010)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Random default exposures: &lt;span class=&#34;math inline&#34;&gt;\(c_k=e_k\ell_k\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\ell_k\in[0,1]\)&lt;/span&gt; denotes a random
percentage loss, and &lt;span class=&#34;math inline&#34;&gt;\(e_k&amp;gt;0\)&lt;/span&gt; are constants.
&lt;span class=&#34;math display&#34;&gt;\[L = \sum_{k=1}^m e_k\ell_k1\{X_k&amp;gt;x_k\}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\ell_k\)&lt;/span&gt; are iid truncated normals or betas&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;cross-entropy&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Cross-entropy&lt;/h2&gt;
&lt;p&gt;The optimal proposal
density is obtained by locating the member &lt;span class=&#34;math inline&#34;&gt;\(p(x;\theta),\theta\in\Theta\)&lt;/span&gt; that minimizes
its cross-entropy distance to the zero-variance proposal
density &lt;span class=&#34;math inline&#34;&gt;\(q^*(x)\propto h(x)p(x;\theta_0)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The minimization of the cross-entropy is equivalent to solving
the following maximization problem
&lt;span class=&#34;math display&#34;&gt;\[\max_{\theta\in\Theta} \int h(x)p(x;\theta_0)\log p(x;\theta)d x=\max_{\theta\in\Theta}  E_{\theta_0}[h(X)\log p(X;\theta)]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since most often an analytical solution to the above maximization
problem is not available, we consider instead its stochastic
counterpart
&lt;span class=&#34;math display&#34;&gt;\[\theta^*=\arg \max_{\theta\in\Theta}\frac 1{N_0}\sum_{i=1}^{N_0}h(X_i)\log p(X_i;\theta),\ X_i\stackrel{iid}{\sim} p(x;\theta_0)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;More detials see Rubinstein (1997), Rubinstein &amp;amp; Kroese (2004).&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>第五次作业</title>
      <link>/post/homework5/</link>
      <pubDate>Mon, 22 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/homework5/</guid>
      <description>&lt;p&gt;课本P61第23题：设&lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt;是&lt;span class=&#34;math inline&#34;&gt;\(U(0,\theta)\)&lt;/span&gt;的样本，求&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的置信水平为&lt;span class=&#34;math inline&#34;&gt;\(1-\alpha\)&lt;/span&gt;的置信区间。设得到了&lt;span class=&#34;math inline&#34;&gt;\(5\)&lt;/span&gt;个样本值&lt;span class=&#34;math inline&#34;&gt;\(0.08,0.28,0.53,0.91,0.89\)&lt;/span&gt;, 求&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的置信水平为&lt;span class=&#34;math inline&#34;&gt;\(0.95\)&lt;/span&gt;的置信区间。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;解&lt;/code&gt;：因为&lt;span class=&#34;math inline&#34;&gt;\(X_i\stackrel{iid}{\sim} U(0,\theta)\)&lt;/span&gt;, 所以&lt;span class=&#34;math inline&#34;&gt;\(Y_i:=X_i/\theta\stackrel{iid}{\sim} U(0,1)\)&lt;/span&gt;. 由此可以构造很多种枢轴量。由于&lt;span class=&#34;math inline&#34;&gt;\(X_{(n)}\)&lt;/span&gt;为&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;的最大似然估计，所以很自然想到通过&lt;span class=&#34;math inline&#34;&gt;\(X_{(n)}\)&lt;/span&gt;来构造枢轴量。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[G_1= \frac{X_{(n)}}{\theta}=Y_{(n)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;因为&lt;span class=&#34;math inline&#34;&gt;\(Y_i\stackrel{iid}{\sim} U(0,1)\)&lt;/span&gt;, 不难计算&lt;span class=&#34;math inline&#34;&gt;\(Y_{(n)}\)&lt;/span&gt;(也就是&lt;span class=&#34;math inline&#34;&gt;\(G_1\)&lt;/span&gt;)的分布函数为：
&lt;span class=&#34;math display&#34;&gt;\[F_1(x) = P(Y_{(n)}\le x ) = \prod_{i=1}^n P(Y_i\le x) = x^n,x\in (0,1) \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;假设存在&lt;span class=&#34;math inline&#34;&gt;\(0\le a&amp;lt;b\le 1\)&lt;/span&gt;满足&lt;span class=&#34;math inline&#34;&gt;\(P(a\le G_1\le b)=b^n-a^n=1-\alpha\)&lt;/span&gt;, 则有可以得到一个
置信水平为&lt;span class=&#34;math inline&#34;&gt;\(1-\alpha\)&lt;/span&gt;的置信区间
&lt;span class=&#34;math display&#34;&gt;\[\left[\frac{X_{(n)}}{b},\frac{X_{(n)}}{a}\right].\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;显然满足方程&lt;span class=&#34;math inline&#34;&gt;\(b^n-a^n=1-\alpha\)&lt;/span&gt;的解有无穷多种，最好的选择方案是使得&lt;span class=&#34;math inline&#34;&gt;\(1/a-1/b\)&lt;/span&gt;最短，即
&lt;span class=&#34;math display&#34;&gt;\[a_{opt} = \arg \min_{a\in[0,\alpha^{1/n}]} \left[\frac{1}{a}-\frac{1}{(1-\alpha+a^n)^{1/n}}\right]= \arg \min_{a\in[0,\alpha^{1/n}]}g(a)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;span class=&#34;math inline&#34;&gt;\(g(a) = \frac{1}{a}-\frac{1}{(1-\alpha+a^n)^{1/n}},a\in[0,\alpha^{1/n}]\)&lt;/span&gt;。因为
&lt;span class=&#34;math display&#34;&gt;\[g&amp;#39;(a) = -\frac 1{a^2}+\frac{a^{n-1}}{(1-\alpha+a^n)^{1+1/n}}&amp;lt; -\frac{1}{a^2}+\frac{a^{n-1}}{(a^n)^{1+1/n}}=0\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以&lt;span class=&#34;math inline&#34;&gt;\(g(a)\)&lt;/span&gt;为单调递减函数，于是&lt;span class=&#34;math inline&#34;&gt;\(a_{opt}=\alpha^{1/n},b_{opt}=1\)&lt;/span&gt;. 所以最优的置信区间为&lt;span class=&#34;math inline&#34;&gt;\(\left[X_{(n)},\frac{X_{(n)}}{\alpha^{1/n}}\right]\)&lt;/span&gt;，把具体数据代进去可得置信区间为&lt;span class=&#34;math inline&#34;&gt;\([0.91,1.66]\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;当然你可以用平分法得到&lt;span class=&#34;math inline&#34;&gt;\(a=(\alpha/2)^{1/n},b=(1-\alpha/2)^{1/n}\)&lt;/span&gt;, 把具体数据代进去可得置信区间为&lt;span class=&#34;math inline&#34;&gt;\([0.9146,1.9031]\)&lt;/span&gt;, 显然这样的区间长度比最优的情况长些。&lt;/p&gt;
&lt;p&gt;此外，我们还可以构造其他的枢轴量，比如
&lt;span class=&#34;math display&#34;&gt;\[G_2 = -2\sum_{i=1}^n\log Y_i = -2\sum_{i=1}^n\log X_i+2n\log \theta\sim\chi^2(2n)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;为什么是卡方分布？请查看&lt;a href=&#34;https://hezhijian.netlify.com/post/homework1/&#34;&gt;第一次作业第五题&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;假设存在&lt;span class=&#34;math inline&#34;&gt;\(0\le c&amp;lt;d\)&lt;/span&gt;满足&lt;span class=&#34;math inline&#34;&gt;\(P(c\le G_2\le d)=1-\alpha\)&lt;/span&gt;, 则有可以得到另一个
置信水平为&lt;span class=&#34;math inline&#34;&gt;\(1-\alpha\)&lt;/span&gt;的置信区间
&lt;span class=&#34;math display&#34;&gt;\[\left[e^{(c+2\sum_{i=1}^n\log X_i)/2n},e^{(d+2\sum_{i=1}^n\log X_i)/2n}\right].\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;利用平分法，不妨取&lt;span class=&#34;math inline&#34;&gt;\(c=\chi^2_{\alpha/2}(2n),d=\chi^2_{1-\alpha/2}(2n)\)&lt;/span&gt;. 代入数据得到置信区间为&lt;span class=&#34;math inline&#34;&gt;\([0.5465,3.0631]\)&lt;/span&gt;，这种做法需要用到所有的数据，而且置信区间长度更长。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;总结：从这例子看出，置信区间的选择有很多种，选取不同的枢轴量，所得的区间往往差别很大。如何选择恰当的枢轴量？一个很好的启发就是与点估计量联系起来。比如这道题第一种方式用到了最大值统计量，这个是未知参数的极大似然估计量，所以“好的”置信区间可能与它存在某种联系，比如包含它。然而，遗憾的是，对区间估计问题没有一个准则来得到所谓“好的”置信区间。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;有部分同学使用中心极限定理来得到近似置信区间，这种做法对&lt;span class=&#34;math inline&#34;&gt;\(n=5\)&lt;/span&gt;的小样本问题不合适。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr /&gt;
&lt;p&gt;下面通过R语言来求解具体的区间估计问题，最方便的方法是用函数来实现。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## 单个总体期望的区间估计
# x为数据
# 1-alpha为置信水平
# sigma为总体标准差，默认sigma=NA为未知标准差的情形
# k为输出结果的有效数字，如果k=0意味着不输出结果，默认输出k=6位有效数字的结果
meanCI &amp;lt;- function(x,alpha,sigma=NA,k=6){
  n = length(x)
  mu = mean(x)
  if(is.na(sigma)){
    #方差未知, 用t分布
    len = qt(1-alpha/2,df=n-1)*sd(x)/sqrt(n)
    CI = c(mu-len,mu+len)
  }else{
    #方差已知, 用正态分布
    len = qnorm(1-alpha/2)*sigma/sqrt(n)
    CI = c(mu-len,mu+len)
  }
  if(k&amp;gt;0){#输出结果，保留k位有效数字
    print(paste0(&amp;quot;期望的&amp;quot;,(1-alpha)*100,&amp;quot;%置信区间为[&amp;quot;,signif(CI[1],k),&amp;quot;, &amp;quot;,signif(CI[2],k),&amp;quot;]&amp;quot;))
  }
  return(CI)
}

## 单个总体方差的区间估计
# x为数据
# 1-alpha为置信水平
# mu为总体期望，默认mu=NA为未知期望的情形
# k为输出结果的有效数字，如果k=0意味着不输出结果，默认输出k=6位有效数字的结果
varCI &amp;lt;- function(x,alpha,mu=NA,k=6){
  n = length(x)
  if(is.na(mu)){
    #期望未知, 用chisq(n-1)分布
    CI = (n-1)*var(x)*c(1/qchisq(1-alpha/2,df=n-1),1/qchisq(alpha/2,df=n-1))
  }else{
    #期望已知, 用chisq(n)分布
    CI = sum((x-mu)^2)*c(1/qchisq(1-alpha/2,df=n),1/qchisq(alpha/2,df=n))
  }
  if(k&amp;gt;0){#输出结果，保留k位有效数字
    print(paste0(&amp;quot;方差的&amp;quot;,(1-alpha)*100,&amp;quot;%置信区间为[&amp;quot;,signif(CI[1],k),&amp;quot;, &amp;quot;,signif(CI[2],k),&amp;quot;]&amp;quot;))
  }
  return(CI)
}&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;p&gt;为了解决课本P62第27题，只需要调用&lt;code&gt;varCI&lt;/code&gt;函数就可以，操作如下&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 第27题数据导入
data1 = c(249,254,243,268,253,269,287,241,273,
          306,303,280,260,256,278,344,304,283,310)
alpha = 0.05
CI1 = varCI(data1,alpha)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;方差的95%置信区间为[418.754, 1603.96]&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(paste0(&amp;quot;标准差的&amp;quot;,(1-alpha)*100,&amp;quot;%置信区间为[&amp;quot;,signif(sqrt(CI1[1]),6),&amp;quot;, &amp;quot;,signif(sqrt(CI1[2]),6),&amp;quot;]&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;标准差的95%置信区间为[20.4635, 40.0495]&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;p&gt;为解决课本P62第28题，只需要调用&lt;code&gt;meanCI&lt;/code&gt;函数就可以，操作如下&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 第28题数据导入
data2 = c(40,45,23,40,31,33,49,33,34,43,26,39)
alpha = 0.05
CI2 = meanCI(data2,alpha)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;期望的95%置信区间为[31.4317, 41.235]&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;p&gt;分析R软件的&lt;code&gt;dslabs&lt;/code&gt;包中的身高数据heights, 利用R软件完成以下问题。相关的R语言操作见 &lt;a href=&#34;https://hezhijian.netlify.com/post/ex2/&#34; class=&#34;uri&#34;&gt;https://hezhijian.netlify.com/post/ex2/&lt;/a&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;假设整个总体服从正态分布，求期望和方差的95%置信区间。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;为了判断“正态总体”的假设的合理性，画图比较核估计密度与正态分布密度的差异？&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;假设男生总体与女生总体均服从正态分布（方差相同）且独立，求这两个总体平均水平的差的95%置信区间。可否认为男生总体的平均身高大于女生总体的平均身高？你的理由是什么？&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;为了考察第3问中“男女总体的方差相同”的假设是否合理，不妨求这两个总体的方差比的95%置信区间。并观察该置信区间是否包含1？&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;解&lt;/code&gt;：第一问只需调用前面的两个函数求解即可。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;dslabs&amp;quot;) #事先需要安装该package
attach(heights) #这样可以直接使用height和sex
alpha = 0.05
CI3 = meanCI(height,alpha)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;期望的95%置信区间为[68.076, 68.57]&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;CI4 = varCI(height,alpha)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;方差的95%置信区间为[15.2985, 18.1559]&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;下面解决第二问，可以看出是核估计与正态估计接近。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(lwd = 2,mar=c(4,4,4,2))
plot(density(height,from = 50,to=85),type=&amp;quot;l&amp;quot;,col=&amp;quot;blue&amp;quot;,xlab=&amp;quot;x&amp;quot;,main=&amp;quot;Kernel vs. Normal&amp;quot;)
x = seq(50,85,by=0.001)
y = dnorm(x,mean(height),sd(height))
lines(x,y,col=&amp;quot;red&amp;quot;)
legend(x=50,y=.1,legend=c(&amp;quot;Kernel&amp;quot;,&amp;quot;Normal&amp;quot;),lty=c(1,1),col=c(&amp;quot;blue&amp;quot;,&amp;quot;red&amp;quot;),lwd=c(2,2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/homework5_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;下面解决两个正态总体均值差（方差相同）的区间估计，同样用函数来解决。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## 两个总体期望差的区间估计（已知方差相同）
# x,y为两个样本数据
# 1-alpha为置信水平
# k为输出结果的有效数字，如果k=0意味着不输出结果，默认输出k=6位有效数字的结果
meandiffCI &amp;lt;- function(x,y,alpha,k=6){
  n = length(x)
  m = length(y)
  mu1 = mean(x)
  mu2 = mean(y)
  sw = sqrt(((n-1)*var(x)+(m-1)*var(y))/(n+m-2))
  CI = mu1-mu2+qt(1-alpha/2,m+n-2)*sw*sqrt(1/n+1/m)*c(-1,1)
  if(k&amp;gt;0){#输出结果，保留k位有效数字
    print(paste0(&amp;quot;期望差的&amp;quot;,(1-alpha)*100,&amp;quot;%置信区间为[&amp;quot;,signif(CI[1],k),&amp;quot;, &amp;quot;,signif(CI[2],k),&amp;quot;]&amp;quot;))
  }
  return(CI)
}

M_height = height[sex==&amp;quot;Male&amp;quot;] #男生数据
F_height = height[sex==&amp;quot;Female&amp;quot;] #女生数据
alpha = 0.05
CI5 = meandiffCI(M_height,F_height,alpha)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;期望差的95%置信区间为[3.84807, 4.90259]&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;下面解决两个正态总体方差比的区间估计，同样用函数来解决。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## 两个总体方差比的区间估计（期望未知）
# x,y为两个样本数据
# 1-alpha为置信水平
# k为输出结果的有效数字，如果k=0意味着不输出结果，默认输出k=6位有效数字的结果
vardiffCI &amp;lt;- function(x,y,alpha,k=6){
  n = length(x)
  m = length(y)
  CI = var(x)/var(y)*c(1/qf(1-alpha/2,n-1,m-1),1/qf(alpha/2,n-1,m-1))
  if(k&amp;gt;0){#输出结果，保留k位有效数字
    print(paste0(&amp;quot;方差比的&amp;quot;,(1-alpha)*100,&amp;quot;%置信区间为[&amp;quot;,signif(CI[1],k),&amp;quot;, &amp;quot;,signif(CI[2],k),&amp;quot;]&amp;quot;))
  }
  return(CI)
}

CI6 = vardiffCI(M_height,F_height,alpha)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;方差比的95%置信区间为[0.746698, 1.12527]&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;从中可以看出，方差比的置信区间包含1，可以认为男女两个总体的方差相同。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>密度估计：直方图与核估计</title>
      <link>/post/ex2/</link>
      <pubDate>Fri, 19 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/ex2/</guid>
      <description>&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;案例：身高数据&lt;/h2&gt;
&lt;p&gt;数据来源于R的包&lt;code&gt;dslabs&lt;/code&gt;，第一次使用时需要安装该包，命令为&lt;code&gt;install.packages(&amp;quot;dslabs&amp;quot;)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;参考资料： &lt;a href=&#34;https://simplystatistics.org/2018/01/22/the-dslabs-package-provides-datasets-for-teaching-data-science/&#34;&gt;Some datasets for teaching data science&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;直方图&lt;/h3&gt;
&lt;p&gt;直方图的R命令为：&lt;code&gt;hist(...)&lt;/code&gt;, 查看帮助&lt;code&gt;?hist&lt;/code&gt;看具体参数含义&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;核估计&lt;/h3&gt;
&lt;p&gt;核估计的R命令为：&lt;code&gt;density(...)&lt;/code&gt;, 查看帮助&lt;code&gt;?density&lt;/code&gt;看具体参数含义。注意该命令只是给出估计值的数据，不能直接画图，如果要画图则需要调用画图函数，如&lt;code&gt;plot(...)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;以下代码展示所有身高数据的直方图与和核估计。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;if(!require(dslabs))
  install.packages(&amp;quot;dslabs&amp;quot;)
attach(heights) #此命令用于使用该包里面的身高数据heights
par(mar=c(2,2,1,1)) #调整图形边距
#直方图
hist(height,breaks=10,ylim=c(0,.115),col = &amp;quot;lightblue&amp;quot;, border = &amp;quot;pink&amp;quot;,freq=FALSE,main=&amp;quot;Histogram vs. Kernel density&amp;quot;)
#添加核估计数据
lines(density(height,from = 50,to=85),col=&amp;quot;red&amp;quot;,lwd=2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/ex2_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;下面代码比较男生和女生数据的核估计&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;female_height = height[sex==&amp;quot;Female&amp;quot;]#提取女生数据
male_height = height[sex==&amp;quot;Male&amp;quot;]#提取男生数据
par(mar=c(2,2,1,1))
#画男生数据
plot(density(male_height,from = 50,to=85),col=&amp;quot;red&amp;quot;,lwd=2,ylim=c(0,.14),main=&amp;quot;Male vs. Female&amp;quot;)
#添加女生数据
lines(density(female_height,from = 50,to=85),col=&amp;quot;blue&amp;quot;,lwd=2)
#画出图例说明
legend(74,0.12,legend = c(&amp;quot;Male&amp;quot;,&amp;quot;Female&amp;quot;),lty = c(1,1),col=c(&amp;quot;red&amp;quot;,&amp;quot;blue&amp;quot;),lwd=c(2,2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/ex2_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;结论：身高数据可以近似看成正态分布，而且男生、女生两个总体的均值有差异，男生身高平均水平大于女生身高的平均水平。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>第四次作业</title>
      <link>/post/homework4/</link>
      <pubDate>Fri, 12 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/homework4/</guid>
      <description>&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt;为来自参数为&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;的Poisson分布的样本. 在下列选项中选出用于估计参数&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;的无偏估计量。&lt;strong&gt;答案：ABCE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A. &lt;span class=&#34;math inline&#34;&gt;\(\bar X\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;B. &lt;span class=&#34;math inline&#34;&gt;\(S_n^{*2}=\frac{1}{n-1}\sum_{i=1}^{n}(X_i-\bar X)^2\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;C. &lt;span class=&#34;math inline&#34;&gt;\(\frac 1 {n-1}\sum_{i=1}^{n-1}X_i\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;D. &lt;span class=&#34;math inline&#34;&gt;\(S_n^2=\frac{1}{n}\sum_{i=1}^n(X_i-\bar X)^2\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;E. &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}2 \bar X + \frac 12 S_n^{*2}\)&lt;/span&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt;为来自参数为&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;的Poisson分布的样本, 已知&lt;span class=&#34;math inline&#34;&gt;\(\bar X\)&lt;/span&gt;是未知参数&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;的完全统计量。在下列选项中选出用于估计参数&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;的最有效的估计量。&lt;strong&gt;答案：A&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A. &lt;span class=&#34;math inline&#34;&gt;\(\bar X\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;B. &lt;span class=&#34;math inline&#34;&gt;\(S_n^{*2}=\frac{1}{n-1}\sum_{i=1}^n(X_i-\bar X)^2\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;C. &lt;span class=&#34;math inline&#34;&gt;\(\frac 1 {n-1}\sum_{i=1}^{n-1}X_i\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;D. &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}2 \bar X + \frac 12 S_n^{*2}\)&lt;/span&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X,\dots,X_n\)&lt;/span&gt;为来自参数为&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;的Poisson分布的样本，求&lt;span class=&#34;math inline&#34;&gt;\(\lambda^2\)&lt;/span&gt;的无偏估计。已知&lt;span class=&#34;math inline&#34;&gt;\(\bar X\)&lt;/span&gt;是参数&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;的完全统计量，能否找到&lt;span class=&#34;math inline&#34;&gt;\(\lambda^2\)&lt;/span&gt;的最小方差无偏估计量？&lt;/p&gt;
&lt;p&gt;&lt;code&gt;解&lt;/code&gt;: 1. &lt;span class=&#34;math inline&#34;&gt;\(\lambda^2\)&lt;/span&gt;的无偏估计有很多种答案：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;因为&lt;span class=&#34;math inline&#34;&gt;\(E[X]=Var[\lambda]=\lambda\)&lt;/span&gt;, 所以&lt;span class=&#34;math inline&#34;&gt;\(E[X^2]=\lambda+\lambda^2=E[X]+\lambda^2\)&lt;/span&gt;，由矩法得到一种无偏估计量：
&lt;span class=&#34;math display&#34;&gt;\[\hat{\lambda^2}_1 = \frac{1}{n}\sum_{i=1}^n(X_i^2-X_i)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;因为&lt;span class=&#34;math inline&#34;&gt;\(E[\bar X]=\lambda\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[E(\bar X^2) = Var(\bar X)+(E[\bar X])^2=\lambda/n+\lambda^2=E[\bar X]/n+\lambda^2,\]&lt;/span&gt;
于是可以得到一种无偏估计量：
&lt;span class=&#34;math display&#34;&gt;\[\hat{\lambda^2}_2 = (\bar X)^2-\bar X/n\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;当然还可以构造无穷多种无偏估计量：
&lt;span class=&#34;math display&#34;&gt;\[\hat{\lambda^2}_3 = \alpha \hat{\lambda^2}_1+(1-\alpha)\hat{\lambda^2}_2,\forall \alpha\in [0,1].\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;虽然无偏统计量有很多，但是最小方差无偏估计量是唯一的（在概率意义下）。由于&lt;span class=&#34;math inline&#34;&gt;\(\bar X\)&lt;/span&gt;是充分完全统计量，所以由B-L-S定理知，&lt;span class=&#34;math inline&#34;&gt;\(\hat{\lambda^2}_2\)&lt;/span&gt;是最小方差无偏的。&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;有一部分同学想通过&lt;span class=&#34;math inline&#34;&gt;\(\psi(\bar X)=E[\hat{\lambda^2}_1|\bar X]\)&lt;/span&gt;的得到最小无偏估计量，思路是对的，但是如何计算&lt;span class=&#34;math inline&#34;&gt;\(\psi(\bar X)\)&lt;/span&gt;就没那么容易了。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;为了解决这个问题，我们需要计算在给定&lt;span class=&#34;math inline&#34;&gt;\(\bar X= t\)&lt;/span&gt;下，样本&lt;span class=&#34;math inline&#34;&gt;\((X_1,\dots,X_n)\)&lt;/span&gt;的条件分布。容易计算样本的联合分布为
&lt;span class=&#34;math display&#34;&gt;\[P(X_1=x_1,\dots,X_n=x_n)=\prod_{i=1}^nP(X_i=x_i) = \prod_{i=1}^n \frac{e^{-\lambda}\lambda^{x_i}}{x_i!}=\frac{e^{-n\lambda}\lambda^{n\bar x}}{\prod_{i=1}^nx_i!}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;由于Possion分布的可加性，有&lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^n X_i\sim Possion(n\lambda)\)&lt;/span&gt;, 所以
&lt;span class=&#34;math display&#34;&gt;\[P(\bar X=t)=P(\sum_{i=1}^n X_i=nt)=\frac{e^{-n\lambda}(n\lambda)^{nt}}{(nt)!}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;于是，给定&lt;span class=&#34;math inline&#34;&gt;\(\bar X= t\)&lt;/span&gt;下，样本&lt;span class=&#34;math inline&#34;&gt;\((X_1,\dots,X_n)\)&lt;/span&gt;的条件分布为：当&lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^nx_i=nt\)&lt;/span&gt;时，
&lt;span class=&#34;math display&#34;&gt;\[P(X_1=x_1,\dots,X_n=x_n|\bar X=t)=\frac{\frac{e^{-n\lambda}\lambda^{nt}}{\prod_{i=1}^nx_i!}}{\frac{e^{-n\lambda}(n\lambda)^{nt}}{(nt)!}}=\frac{(nt)!}{n^{nt}\prod_{i=1}^nx_i!}, \]&lt;/span&gt;
其他情况下，该条件概率为0. 我们发现
&lt;span class=&#34;math display&#34;&gt;\[\psi(t)=E[\hat{\lambda^2}_1|\bar X=t]=E[\frac{1}{n}\sum_{i=1}^nX_i^2-\bar X|\bar X=t]=E[n(\bar X)^2-\frac{1}{n}\sum_{i\neq j}X_iX_j-\bar X|\bar X=t]=nt^2-t-\frac{1}{n}E[\sum_{i\neq j}X_iX_j|\bar X=t].\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;由于对称性，&lt;span class=&#34;math inline&#34;&gt;\(E[\sum_{i\neq j}X_iX_j|\bar X=t]=n(n-1)E[X_1X_2|\bar X=t]\)&lt;/span&gt;, 所以只需计算&lt;span class=&#34;math inline&#34;&gt;\(E[X_1X_2|\bar X=t]\)&lt;/span&gt;即可：
&lt;span class=&#34;math display&#34;&gt;\[E[X_1X_2|\bar X=t]=\sum_{\vec x:\sum_{i=1}^nx_i=nt}\frac{x_1x_2(nt)!}{n^{nt}\prod_{i=1}^nx_i!}=\frac{(nt)!}{n^{nt}}\sum_{\vec x:\sum_{i=1}^nx_i=nt}\frac{x_1x_2}{\prod_{i=1}^nx_i!}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;由于
&lt;span class=&#34;math display&#34;&gt;\[\sum_{\vec x:\sum_{i=1}^nx_i=nt}\frac{x_1x_2}{\prod_{i=1}^nx_i!}=\sum_{\vec x:\sum_{i=1}^nx_i=nt,x_1\ge 1,x_2\ge 1}\frac{1}{(x_1-1)!(x_2-1)!\prod_{i=3}^nx_i!}=\sum_{\vec x:\sum_{i=1}^nx_i=nt-2}\frac{1}{\prod_{i=1}^nx_i!}=\frac{n^{nt-2}}{(nt-2)!}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以，
&lt;span class=&#34;math display&#34;&gt;\[\psi(t)=nt^2-t-(n-1)E[X_1X_2|\bar X=t]=nt^2-t-(n-1)\frac{(nt)!}{n^{nt}}\frac{n^{nt-2}}{(nt-2)!}=t^2-\frac{t}{n}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;这表明，&lt;span class=&#34;math inline&#34;&gt;\(\psi(\bar X)=(\bar X)^2-\bar X/n\)&lt;/span&gt;, 也就是&lt;span class=&#34;math inline&#34;&gt;\(\hat{\lambda^2}_2\)&lt;/span&gt;. 两种方式得到的最小方差无偏估计量是一致的。这也印证了最小方差无偏估计量是唯一的。显然第一种方式比较简单，第二种方式需要求条件期望，这个比较复杂，一般情况下不容易求解。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt;为&lt;span class=&#34;math inline&#34;&gt;\(N(\mu,\sigma^2)\)&lt;/span&gt;分布的样本，参数&lt;span class=&#34;math inline&#34;&gt;\(\mu,\sigma^2\)&lt;/span&gt;未知。证明样本方差&lt;span class=&#34;math inline&#34;&gt;\(S_n^2\)&lt;/span&gt;与修正样本方差&lt;span class=&#34;math inline&#34;&gt;\(S_n^{*2}\)&lt;/span&gt;均为&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;的弱相合估计量。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;解&lt;/code&gt;： 由抽样分布定理知，&lt;span class=&#34;math inline&#34;&gt;\(\frac{\sum_{i=1}^n (X_i-\bar X)^2}{\sigma^2}\sim \chi^2(n-1)\)&lt;/span&gt;, 所以
&lt;span class=&#34;math display&#34;&gt;\[Var[\sum_{i=1}^n (X_i-\bar X)^2]=2(n-1)\sigma^4\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;于是，&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Var[S_n^2]=Var[\sum_{i=1}^n (X_i-\bar X)^2]/n^2=\frac{2(n-1)\sigma^4}{n^2}\to 0\text{ as }n\to \infty\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Var[S_n^{*2}]=Var[\sum_{i=1}^n (X_i-\bar X)^2]/(n-1)^2=\frac{2\sigma^4}{n-1}\to 0\text{ as }n\to \infty\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;由于&lt;span class=&#34;math inline&#34;&gt;\(E[S_n^{*2}]=\sigma^2,\lim_{n\to\infty}E[S_n^2]=\sigma^2\)&lt;/span&gt;, 由弱相合性判别条件知，它们都是弱相合的。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt;为&lt;span class=&#34;math inline&#34;&gt;\(N(\mu,\sigma^2)\)&lt;/span&gt;分布的样本，参数&lt;span class=&#34;math inline&#34;&gt;\(\mu,\sigma^2\)&lt;/span&gt;未知。样本方差&lt;span class=&#34;math inline&#34;&gt;\(S_n^2\)&lt;/span&gt;与修正样本方差&lt;span class=&#34;math inline&#34;&gt;\(S_n^{*2}\)&lt;/span&gt;作为&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;的两种估计量，哪个更有效？由B-L-S定理知，&lt;span class=&#34;math inline&#34;&gt;\(S_n^{*2}\)&lt;/span&gt;是最小方差无偏估计量，这是否与你所得的结论矛盾？由此你能得到什么启发？&lt;/p&gt;
&lt;p&gt;&lt;code&gt;解&lt;/code&gt;: 由于&lt;span class=&#34;math inline&#34;&gt;\(S_n^{*2}\)&lt;/span&gt;是无偏的，所以均方误差
&lt;span class=&#34;math display&#34;&gt;\[M(S_n^{*2}) = Var[S_n^{*2}]=\frac{2\sigma^4}{n-1}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;对&lt;span class=&#34;math inline&#34;&gt;\(S_n^{2}\)&lt;/span&gt;, 其均方误差为
&lt;span class=&#34;math display&#34;&gt;\[M(S_n^{*2}) = Var[S_n^{2}]+(E[S_n^2]-\sigma^2)^2=\frac{2(n-1)\sigma^4}{n^2}+(\frac{(n-1)\sigma^2}{n}-\sigma^2)^2=\frac{(2n-1)\sigma^4}{n^2}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;又
&lt;span class=&#34;math display&#34;&gt;\[\frac{M(S_n^{*2})}{M(S_n^{2})}=\frac{2n^2}{(n-1)(2n-1)}&amp;gt;1\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以，&lt;span class=&#34;math inline&#34;&gt;\(S_n^{2}\)&lt;/span&gt;比&lt;span class=&#34;math inline&#34;&gt;\(S_n^{*2}\)&lt;/span&gt;有效。这与“&lt;span class=&#34;math inline&#34;&gt;\(S_n^{*2}\)&lt;/span&gt;是最小方差无偏估计量”不矛盾，因为&lt;span class=&#34;math inline&#34;&gt;\(S_n^{2}\)&lt;/span&gt;是有偏估计量。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;启发：无偏估计量不一定是最有效的。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr /&gt;
&lt;p&gt;设&lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots,X_n\)&lt;/span&gt;为总体&lt;span class=&#34;math inline&#34;&gt;\(N(\mu,\sigma^2)\)&lt;/span&gt;, 其中&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;已知，&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;未知。证明&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;的估计量
&lt;span class=&#34;math display&#34;&gt;\[T(X_1,\dots,X_n)=\frac 1n\sum_{i=1}^n(X_i-\mu)^2\]&lt;/span&gt;
的方差达到C-R不等式的下界。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;解&lt;/code&gt;: 令&lt;span class=&#34;math inline&#34;&gt;\(\theta=\sigma^2,f(x;\theta)\)&lt;/span&gt;为总体密度函数。于是，
&lt;span class=&#34;math display&#34;&gt;\[\log f(x;\theta)= \log  \frac{1}{\sqrt{2\pi}\sqrt{\theta}}e^{-\frac{(x-\mu)^2}{2\theta}}=-(1/2)\log(2\pi\theta)-\frac{(x-\mu)^2}{2\theta}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{d\log f(x;\theta)}{d\theta}=-\frac{1}{2\theta}+\frac{(x-\mu)^2}{2\theta^2}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以，Fisher信息量为：
&lt;span class=&#34;math display&#34;&gt;\[I(\theta)=E[(\frac{d\log f(X;\theta)}{d\theta})^2]=\frac{1}{4\theta^2}E[(\frac{(X-\mu)^2}{\theta}-1)^2]=\frac{1}{2\theta^2}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以，C-R不等式下界为:
&lt;span class=&#34;math display&#34;&gt;\[\frac{1}{nI(\theta)}=\frac{2\sigma^4}{n}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;因为&lt;span class=&#34;math inline&#34;&gt;\(nT/\sigma^2\sim \chi^2(n)\)&lt;/span&gt;, 所以
&lt;span class=&#34;math display&#34;&gt;\[Var[nT/\sigma^2]=2n\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;于是，&lt;span class=&#34;math inline&#34;&gt;\(Var[T]=2\sigma^4/n\)&lt;/span&gt;达到C-R不等式下界。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 5: Hierarchial models</title>
      <link>/post/bayes_chap05/</link>
      <pubDate>Sun, 07 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/bayes_chap05/</guid>
      <description>&lt;div id=&#34;introduction-to-hierarchial-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction to hierarchial models&lt;/h2&gt;
&lt;p&gt;Many statistical applications involve multiple parameters (say, &lt;span class=&#34;math inline&#34;&gt;\(\theta_1,\dots,\theta_J\)&lt;/span&gt;) that can be regarded as related or connected in some way by the structure of the problem.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;for the group &lt;span class=&#34;math inline&#34;&gt;\(j\in 1{:}J\)&lt;/span&gt;, we have the observed data &lt;span class=&#34;math inline&#34;&gt;\(y_{ij}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(i=1,\dots,n_j\)&lt;/span&gt; from the population distribution with unknown parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;we use a prior distribution in which the &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt;’s are viewed as a sample from a common &lt;em&gt;population distribution&lt;/em&gt;, say &lt;span class=&#34;math inline&#34;&gt;\(p(\theta|\phi)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; is known as &lt;em&gt;hyperparameters&lt;/em&gt;. Assume that &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt; are iid, i.e.,
&lt;span class=&#34;math display&#34;&gt;\[p(\theta|\phi)=\prod_{j=1}^Jp(\theta_j|\phi)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;hierarchical-model-for-rats-experiment&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hierarchical model for Rats experiment&lt;/h2&gt;
&lt;p&gt;The experiment is used to estimate the probability &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; of tumor in a population of female laboratory rats of type ‘F344’ that receive a zero dose of the drug. The data show that 4 out of 14 rats developed a kind of tumor.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;assume a binomial model for the number of tumors&lt;/li&gt;
&lt;li&gt;select a prior from the conjugate family, i.e., &lt;span class=&#34;math inline&#34;&gt;\(\theta\sim Beta(\alpha,\beta)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;the posterior is therefore &lt;span class=&#34;math inline&#34;&gt;\(Beta(\alpha+1,\beta+10)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The question is how to determine the hyperparameters &lt;span class=&#34;math inline&#34;&gt;\(\phi=(\alpha,\beta)\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;historical data are available on previous experiments on similar groups of rats: in the jth historical experiments, let the number of rats with tumors be &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt; and the total number of rats be &lt;span class=&#34;math inline&#34;&gt;\(n_j\)&lt;/span&gt;, the parameters for the populations are &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(j=1,\dots,70\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;for current experiment, let &lt;span class=&#34;math inline&#34;&gt;\(y_{71},n_{71},\theta_{71}\)&lt;/span&gt; be the associated notations.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;historical-data-for-the-70-historical-experiments&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Historical data for the 70 historical experiments&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  1  1  2
## [24]  2  2  2  2  2  2  2  2  1  5  2  5  3  2  7  7  3  3  2  9 10  4  4
## [47]  4  4  4  4  4 10  4  4  4  5 11 12  5  5  6  5  6  6  6  6 16 15 15
## [70]  9  4&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 20 20 20 20 20 20 20 19 19 19 19 18 18 17 20 20 20 20 19 19 18 18 25
## [24] 24 23 20 20 20 20 20 20 10 49 19 46 27 17 49 47 20 20 13 48 50 20 20
## [47] 20 20 20 20 20 48 19 19 19 22 46 49 20 20 23 19 22 20 20 20 52 46 47
## [70] 24 14&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;viewed-as-separate-models-using-uniform-priors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Viewed as separate models using uniform priors&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;separate_model.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;viewed-as-a-pooled-model-using-uniform-prior&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Viewed as a pooled model using uniform prior&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;pool_model.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-the-historical-data-to-estimate-the-hyperparameters&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using the historical data to estimate the hyperparameters&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the sample mean and standard deviation of the 70 values &lt;span class=&#34;math inline&#34;&gt;\(y_i/n_i\)&lt;/span&gt; are 0.136 and 0.103&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;let &lt;span class=&#34;math inline&#34;&gt;\(E[\theta]=\frac{\alpha}{\alpha+\beta}=0.136\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Var[\theta]=\frac{E[\theta](1-E[\theta])}{\alpha+\beta+1}=0.103\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\alpha}=1.4,\ \hat{\beta}=8.6\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;for the current exeriment, the posterior for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(Beta(5.4,18.6)\)&lt;/span&gt;, posterior mean is &lt;span class=&#34;math inline&#34;&gt;\(0.223\)&lt;/span&gt;, standard deviation is 0.083.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are several logical and practical problems with the approach of directly estimating a prior distribution from existing data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the data will be used twice for inference about the first 70 experiments – overestimate our precision&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the point estimate for &lt;span class=&#34;math inline&#34;&gt;\(\alpha,\beta\)&lt;/span&gt; seems arbitrary that necessarily ignores some posterior uncertainty&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;this is not the logic of Bayesian inference&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-full-bayesian-treatment-of-the-hierarchical-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The full Bayesian treatment of the hierarchical model&lt;/h2&gt;
&lt;p&gt;Suppose the hyperparameters &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; has its own prior distribution &lt;span class=&#34;math inline&#34;&gt;\(p(\phi)\)&lt;/span&gt;, which is called &lt;em&gt;hyperprior distribution&lt;/em&gt;. The appropriate Bayesian posterior distribution is of the vector &lt;span class=&#34;math inline&#34;&gt;\((\phi,\theta)\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the joint prior distribution is
&lt;span class=&#34;math display&#34;&gt;\[p(\phi,\theta)=p(\phi)p(\theta|\phi)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the joint posterior distribution is
&lt;span class=&#34;math display&#34;&gt;\[p(\phi,\theta|y)\propto p(\phi,\theta)p(y|\phi,\theta)=p(\phi)p(\theta|\phi)p(y|\theta)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Previously, we assumed &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; was known, which is unrealistic; now we include the uncertainty in &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; in the model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fully-bayesian-analysis-of-conjugate-hierarchical-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fully Bayesian analysis of conjugate hierarchical models&lt;/h2&gt;
&lt;p&gt;Consider the setting in which &lt;span class=&#34;math inline&#34;&gt;\(p(\theta|\phi)\)&lt;/span&gt; is conjugate to the likelihood &lt;span class=&#34;math inline&#34;&gt;\(p(y|\theta)\)&lt;/span&gt;. For this case, it is easy to determine analytically &lt;span class=&#34;math display&#34;&gt;\[p(\theta|\phi,y)\propto p(\theta|\phi)p(y|\theta)\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the joint posterior density:
&lt;span class=&#34;math display&#34;&gt;\[p(\phi,\theta|y)\propto p(\phi)p(\theta|\phi)p(y|\theta)\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;the marginal posterior density &lt;span class=&#34;math inline&#34;&gt;\(p(\phi|y)\)&lt;/span&gt; can be computed via
&lt;span class=&#34;math display&#34;&gt;\[p(\phi|y)=\int p(\phi,\theta|y)d \theta\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{or }p(\phi|y)=\frac{p(\phi,\theta|y)}{p(\theta|\phi,y)}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;application-to-the-model-for-rat-tumors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Application to the model for rat tumors&lt;/h2&gt;
&lt;p&gt;The binomial model:
&lt;span class=&#34;math display&#34;&gt;\[y_j\sim Bin(n_j,\theta_j),\ j=1,\dots,J=71\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt; are assumed to be independent samples from a beta distribution:
&lt;span class=&#34;math display&#34;&gt;\[\theta_j\sim Beta(\alpha,\beta)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The joint posterior density is
&lt;span class=&#34;math display&#34;&gt;\[p(\theta,\alpha,\beta|y)\propto p(\alpha,\beta)p(\theta|\alpha,\beta)p(y|\theta)\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\propto p(\alpha,\beta)\prod_{j=1}^J\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\theta_j^{\alpha-1}(1-\theta_j)^{\beta-1}\prod_{j=1}^J\theta_j^{y_j}(1-\theta_j)^{n_j-y_j}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(\theta|\alpha,\beta,y)=\prod_{j=1}^J\frac{\Gamma(\alpha+\beta+n_j)}{\Gamma(\alpha+y_j)\Gamma(\beta+n_j-y_j)}\theta_j^{\alpha+y_i-1}(1-\theta_j)^{\beta+n_j-y_j-1}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;application-to-the-model-for-rat-tumors-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Application to the model for rat tumors&lt;/h2&gt;
&lt;p&gt;The marginal posterior density:
&lt;span class=&#34;math display&#34;&gt;\[p(\alpha,\beta|y)\propto p(\alpha,\beta)\prod_{j=1}^J\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\frac{\Gamma(\alpha+y_j)\Gamma(\beta+n_j-y_j)}{\Gamma(\alpha+\beta+n_j)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Choosing a noninformative hyperprior distribution:
&lt;span class=&#34;math display&#34;&gt;\[p(\alpha,\beta)\propto (\alpha+\beta)^{-5/2}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This implies that &lt;span class=&#34;math inline&#34;&gt;\((\alpha/(\alpha+\beta),(\alpha+\beta)^{-1/2})\)&lt;/span&gt; is uniformly distributed.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the prior mean is &lt;span class=&#34;math inline&#34;&gt;\(\alpha/(\alpha+\beta)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;the prior variance is approximately &lt;span class=&#34;math inline&#34;&gt;\((\alpha+\beta)^{-1}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-of-the-marginal-posterior-density&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plot of the marginal posterior density&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;alphabeta.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;compare-the-separate-model-and-hierarchical-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Compare the separate model and hierarchical model&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;hier_sep.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hierarchical-model-based-on-normal-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hierarchical model based on normal distribution&lt;/h2&gt;
&lt;p&gt;Consider &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; independent experiments, with experiment &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; estimating &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt; form &lt;span class=&#34;math inline&#34;&gt;\(n_j\)&lt;/span&gt; independent distributed data points &lt;span class=&#34;math inline&#34;&gt;\(y_{ij}\)&lt;/span&gt;, each with known error variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;, that is
&lt;span class=&#34;math display&#34;&gt;\[y_{ij}|\theta_j\stackrel{iid}{\sim} N(\theta_j,\sigma^2), \text{ for }i=1,\dots,n_j;\ j=1,\dots,J\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;denote the sample mean of each group &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; as
&lt;span class=&#34;math display&#34;&gt;\[\bar{y}_{\cdot j}=\frac 1{n_j}\sum_{i=1}^{n_j}y_{ij}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;let &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_j=\sigma^2/n_j\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the likelihood for each &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[\bar{y}_{\cdot j}|\theta_j\sim N(\theta_j,\sigma_j^2)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;for the convenience of conjugacy, assume the paramerters &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt; are drawn from a normal distribution with hyperparameters &lt;span class=&#34;math inline&#34;&gt;\(\mu,\tau\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(\theta_1,\dots,\theta_J|\mu,\tau)=\prod_{j=1}^J N(\theta_j|\mu,\tau^2)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;assign noninformative uniform hyperprior density to &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(\mu,\tau)=p(\mu|\tau)p(\tau)\propto p(\tau)\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;prior distribution for &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(p(\tau)\propto 1\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the joint posterior density is
&lt;span class=&#34;math display&#34;&gt;\[p(\theta,\mu,\tau|y)\propto p(\mu,\tau)p(\theta|\mu,\tau)p(y|\theta)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(\theta,\mu,\tau|y)\propto p(\mu,\tau)\prod_{j=1}^J N(\theta_j|\mu,\tau^2)\prod_{j=1}^JN(\bar{y}_{\cdot j}|\theta_j,\sigma_j^2)\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the conditional posterior distirbution:
&lt;span class=&#34;math display&#34;&gt;\[\theta_j|\mu,\tau,y\sim N(\hat{\theta}_j,V_j)\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where
&lt;span class=&#34;math display&#34;&gt;\[\hat{\theta}_j=\frac{\frac 1{\sigma^2}\bar{y}_{\cdot j}+\frac 1{\tau^2}\mu}{\frac 1{\sigma^2}+\frac 1{\tau^2}},\ V_j=\frac{1}{\frac 1{\sigma^2}+\frac 1{\tau^2}}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the marginal posterior density can be computed in a simple way
&lt;span class=&#34;math display&#34;&gt;\[p(\mu,\tau|y)\propto p(\mu,\tau)p(y|\mu,\tau)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bar{y}_{\cdot j}|\mu,\tau\sim N(\mu,\sigma_j^2+\tau^2)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(\mu,\tau|y)\propto p(\mu,\tau)\prod_{j=1}^JN(\bar{y}_{\cdot j}|\mu,\sigma_j^2+\tau^2)\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;posterior distribution of &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\mu|\tau,y\sim N(\hat{\mu},V_{\mu})\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where
&lt;span class=&#34;math display&#34;&gt;\[\hat{\mu}=\frac{\sum_{j=1}^J \frac 1{\sigma_j^2+\tau^2}\bar{y}_{\cdot j}}{\sum_{j=1}^J \frac 1{\sigma_j^2+\tau^2}},\ V_{\mu}^{-1}=\sum_{j=1}^J \frac 1{\sigma_j^2+\tau^2}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;posterior distribution of &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[p(\tau|y)=\frac{p(\mu,\tau|y)}{p(\mu|\tau,y)}\propto \frac{p(\tau)\prod_{j=1}^JN(\bar{y}_{\cdot j}|\mu,\sigma_j^2+\tau^2)}{N(\mu|\hat{\mu},V_{\mu})}\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(\tau|y)\propto p(\tau)V_{\mu}^{1/2}\prod_{j=1}^J(\sigma_j^2+\tau^2)^{-1/2}\exp\left(-\frac{(\bar{y}_{\cdot j}-\hat{\mu})^2}{2(\sigma_j^2+\tau^2)}\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-parallel-experiments-in-eight-schools&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example: parallel experiments in eight schools&lt;/h2&gt;
&lt;p&gt;A study was performanced for the Educational Testing Service to analyze the effects of special coaching programs on test scores. Seperate randomized experiments were performed to estimate the effects of coaching programs for the SAT-V (Verbal).&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;33%&#34; /&gt;
&lt;col width=&#34;33%&#34; /&gt;
&lt;col width=&#34;33%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;School&lt;/th&gt;
&lt;th&gt;Estiamted treatment effect &lt;span class=&#34;math inline&#34;&gt;\(y_j\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;Standard error of effect estimate &lt;span class=&#34;math inline&#34;&gt;\(\sigma_j\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;A&lt;/td&gt;
&lt;td&gt;28&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;B&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;C&lt;/td&gt;
&lt;td&gt;-3&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;D&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;E&lt;/td&gt;
&lt;td&gt;-1&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;F&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;G&lt;/td&gt;
&lt;td&gt;18&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;H&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;18&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;comparisons&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comparisons&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;8schools.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-the-posterior-summaries&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plot the posterior summaries&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;8schools2.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 4: Asymptotics and connections to non-Bayesian approaches</title>
      <link>/post/bayes_chap04/</link>
      <pubDate>Sat, 06 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/bayes_chap04/</guid>
      <description>&lt;div id=&#34;large-sample-theory&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Large-sample theory&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Assumptions and notations&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;true distribution: &lt;span class=&#34;math inline&#34;&gt;\(y_i\stackrel {iid}{\sim} f(\cdot)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\theta\in\Theta\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;prior distribution: &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;model distribution: &lt;span class=&#34;math inline&#34;&gt;\(p(y_i|\theta)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Kullback-Leibler divergence&lt;/em&gt;: a measure of ‘discrepancy’ between the model and the true distribution
&lt;span class=&#34;math display&#34;&gt;\[KL(\theta)= E\left[\log\left(\frac{f(y_i)}{p(y_i|\theta)}\right)\right]=\int \log\left(\frac{f(y_i)}{p(y_i|\theta)}\right)f(y_i)dy_i\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\theta_0\)&lt;/span&gt;: the &lt;strong&gt;unique minimizer&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(KL(\theta)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;if &lt;span class=&#34;math inline&#34;&gt;\(f(y_i) = p(y_i|\theta)\)&lt;/span&gt; then &lt;span class=&#34;math inline&#34;&gt;\(\theta=\theta_0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;convergence-of-the-posterior-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Convergence of the posterior distribution&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Discrete parmeter space&lt;/strong&gt;: If the parameter space &lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt; is finite and &lt;span class=&#34;math inline&#34;&gt;\(P(\theta=\theta_0)&amp;gt;0\)&lt;/span&gt;, then
&lt;span class=&#34;math display&#34;&gt;\[P(\theta=\theta_0|y)\to 1\text{ as }n\to \infty,\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\theta_0=\arg_{\theta\in\Theta} KL(\theta)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Continuous parmeter space&lt;/strong&gt;: If &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is defined on a compace set &lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is a neighborhood of &lt;span class=&#34;math inline&#34;&gt;\(\theta_0\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(P(\theta\in A)&amp;gt;0\)&lt;/span&gt;, then
&lt;span class=&#34;math display&#34;&gt;\[P(\theta\in A|y)\to 1\text{ as }n\to \infty,\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\theta_0=\arg_{\theta\in\Theta} KL(\theta)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;See the proofs in Appendix B.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;normal-approximations-to-the-posterior-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Normal approximations to the posterior distribution&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat \theta\)&lt;/span&gt;: the posterior mode&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Taylor series expansion of &lt;span class=&#34;math inline&#34;&gt;\(\log p(\theta|y)\)&lt;/span&gt; gives
&lt;span class=&#34;math display&#34;&gt;\[\log(\theta|y) = \log p(\hat \theta|y)-\frac 12 (\theta-\hat\theta)^\top I(\hat \theta) (\theta-\hat\theta) + \cdots \]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(I(\theta)\)&lt;/span&gt; is the &lt;em&gt;observed&lt;/em&gt; information
&lt;span class=&#34;math display&#34;&gt;\[I(\theta)=-\frac{d^2}{d\theta^2}\log p(\theta|y)\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Normal approximation: &lt;span class=&#34;math inline&#34;&gt;\(p(\theta|y)\approx N(\hat\theta,[I(\hat\theta)]^{-1})\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Fisher information&lt;/em&gt;:
&lt;span class=&#34;math display&#34;&gt;\[J(\theta)=-E_f\left[\frac{d^2}{d\theta^2}\log p(y_j|\theta)\right]\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;convergence-of-the-posterior-distribution-to-normality&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Convergence of the posterior distribution to normality&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: Under some regularity conditions (notably that &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; not be on the boundary of &lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt;), as &lt;span class=&#34;math inline&#34;&gt;\(n\to \infty\)&lt;/span&gt;, the posterior distribution of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; approaches normality with mean &lt;span class=&#34;math inline&#34;&gt;\(\theta_0\)&lt;/span&gt; and variance &lt;span class=&#34;math inline&#34;&gt;\([nJ(\theta_0)]^{-1}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\theta_0=\arg_{\theta\in\Theta} KL(\theta)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; is the Fisher information.&lt;/p&gt;
&lt;p&gt;Oberved that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat\theta\to \theta_0\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(n\to \infty\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(I(\hat\theta)=-\frac{d^2}{d\theta^2}\log p(\hat\theta)-\sum_{i=1}^n\frac{d^2}{d\theta^2}\log p(y_i|\hat\theta)\approx nJ(\theta_0)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(J(\theta_0)=\frac{d^2}{d\theta^2} KL(\theta_0)&amp;gt;0\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;counterexamples-to-the-theorems&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Counterexamples to the theorems&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;underidentified models: &lt;span class=&#34;math inline&#34;&gt;\(p(y|\theta)\)&lt;/span&gt; is equal for a range of values of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;nonindentified parameters: for example, consider the model,
&lt;span class=&#34;math display&#34;&gt;\[\left(
\begin{matrix}
u\\
v
\end{matrix}
\right)\sim N \left( \left(\begin{matrix}
0\\
0
\end{matrix}
\right),\left(\begin{matrix}
1&amp;amp;\rho\\
\rho &amp;amp; 1
\end{matrix}
\right)\right)\]&lt;/span&gt;
only one of &lt;span class=&#34;math inline&#34;&gt;\(u,v\)&lt;/span&gt; is observed from each pair &lt;span class=&#34;math inline&#34;&gt;\((u,v)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;number of parameters increasing with sample sizes: new latent parameters with each data point&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;point-estimation-consistency-and-efficiency&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Point estimation, consistency, and efficiency&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;point estimations&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;posterior mode &lt;span class=&#34;math inline&#34;&gt;\(\hat\theta(y)=\arg \max_{\theta\in\Theta} p(\theta|y)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;posterior mean &lt;span class=&#34;math inline&#34;&gt;\(\hat\theta(y)=E[\theta|y]=\int \theta p(\theta|y)d \theta\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;posterior median &lt;span class=&#34;math inline&#34;&gt;\(\hat\theta(y)=F^{-1}_{\theta|y}(0.5)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;consistency&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\hat\theta(y)\to \theta_0\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(n\to \infty\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;asymptotic unbiasedness&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(E[\hat\theta|\theta_0]\to\theta_0\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(n\to \infty\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;efficiency&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[\text{eff}(\hat\theta)=\frac{\inf_T E[(T(y)-\theta_0)^2|\theta_0]}{E[(\hat\theta-\theta_0)^2|\theta_0]}\le 1\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;asymptotically efficient&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\text{eff}(\hat\theta)\to 1\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(n\to \infty\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
