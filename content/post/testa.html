---
title: "《数理统计》试卷"
date: "2019-01-17"
categories: ["课件"]
math: TRUE
Summary: "《数理统计》试卷"
tags: ["课件", "数理统计"]
---



<p>Part I: Each problem is worth 3 points.</p>
<ol style="list-style-type: decimal">
<li>Let <span class="math inline">\(X_1,X_2,\dots,X_6\)</span> be a simple random sample taken from <span class="math inline">\(N(0,2^2)\)</span>. Denote
<span class="math display">\[Y = (X_1+X_2)^2+(X_3+X_4)^2+(X_5+X_6)^2.\]</span>
If <span class="math inline">\(kY\sim \chi^2(3)\)</span>, then <span class="math inline">\(k=1/8\)</span>.</li>
</ol>
<hr />
<ol start="2" style="list-style-type: decimal">
<li><p>Let <span class="math inline">\(X_1,X_2,X_3\)</span> be a simple random sample taken from <span class="math inline">\(N(\mu,\sigma^2)\)</span>. If <span class="math inline">\(\hat\mu = \frac{1}{2} X_1+cX_2+\frac{1}{6}X_3\)</span> is an unibased estimate of <span class="math inline">\(\mu\)</span>, then <span class="math inline">\(c=1/3\)</span>.</p></li>
<li><p>Let <span class="math inline">\(X_1,X_2,X_3\)</span> be a simple random sample taken from <span class="math inline">\(B(1,p)\)</span>. For testing the hypothesis <span class="math inline">\(H_0:p=1/2\ vs.\ H_1:p=3/4\)</span>, we use a rejection region:
<span class="math display">\[W=\{(x_1,x_2,x_3):x_1+x_2+x_3\ge 2\}.\]</span>
The power of the test is 27/32?</p></li>
<li><p>Let <span class="math inline">\(X_1,\dots,X_n\)</span> be a simple random sample taken from <span class="math inline">\(N(\mu,1)\)</span>, and let <span class="math inline">\(S_n^2=\frac 1n\sum_{i=1}^n(X_i-\bar X)^2\)</span> be the sample variance. Then <span class="math inline">\(Var[S_n^2]=2(n-1)/n^2\)</span>.</p></li>
<li><p>If the usual <span class="math inline">\(95\%\)</span> confidence interval for the mean of normal population was <span class="math inline">\([0.12,0.22]\)</span>, the method of moments estimate of the mean would be 0.17?</p></li>
</ol>
<hr />
<p>Part II: Multiple Choice Problems (one or more than one items may be true). Each problem is worth 3 points.</p>
<ol style="list-style-type: decimal">
<li>The parameters <span class="math inline">\(\theta,\lambda,\alpha,\beta\)</span> are unknown in the following densities. Which of the following probability distributions belong to the exponential family? ( BC )</li>
</ol>
<p>A. <span class="math inline">\(f(x;\theta,\lambda) = \frac \theta\lambda\left(\frac{x}{\lambda}\right)^{\theta-1}e^{-(x/\lambda)^\theta}1\{x&gt; 0\}\)</span></p>
<p>B. <span class="math inline">\(f(x;\alpha,\beta) = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1}1\{0&lt;x&lt;1\}\)</span>, where <span class="math inline">\(\Gamma(\cdot)\)</span> is the gamma function.</p>
<p>C. <span class="math inline">\(f(x;\lambda) = \frac{\lambda^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\lambda x}1\{x&gt; 0\}\)</span></p>
<p>D. <span class="math inline">\(f(x;\theta) = \frac{2}{\sqrt{2\pi}}e^{-\frac{(x-\theta)^2}{2}}1\{x\ge \theta\}\)</span></p>
<hr />
<ol start="2" style="list-style-type: decimal">
<li>Let <span class="math inline">\(X_1,\dots,X_n\)</span> be the simple random sample taken from the normal distribution <span class="math inline">\(N(\mu,\sigma^2)\)</span>, where <span class="math inline">\(\mu,\sigma^2\)</span> are unknown parameters. Which of the following are sufficient statistics for <span class="math inline">\(\theta=(\mu,\sigma^2)\)</span>? ( AB )</li>
</ol>
<p>A. <span class="math inline">\(T_1 = (X_1,\dots,X_n)\)</span></p>
<p>B. <span class="math inline">\(T_2 = (\sum_{i=1}^n X_i,\sum_{i=1}^n X_i^2)\)</span></p>
<p>C. <span class="math inline">\(T_3 = (\sum_{i=1}^n |X_i|,\sum_{i=1}^n X_i^2)\)</span></p>
<p>D. <span class="math inline">\(T_4 = \frac{1}{n}\sum_{i=1}^n X_i\)</span></p>
<hr />
<ol start="3" style="list-style-type: decimal">
<li>Which of the following statements are true? ( BC )</li>
</ol>
<p>A. If the <span class="math inline">\(p\)</span>-value is 0.05, the corresponding test will be rejected at the significance level 0.03.</p>
<p>B. If a test rejects at significance level 0.05, then the <span class="math inline">\(p\)</span>-value is less than or equal to 0.05.</p>
<p>C. If the significance level of a test is decreased, the power of the test would be expected to decrease.</p>
<p>D. A type II error occurs when the test statistic falls in the rejection region of the test and the null is true.</p>
<hr />
<ol start="4" style="list-style-type: decimal">
<li>Let <span class="math inline">\(\hat\beta_0,\hat\beta_1\)</span> be the least squares etstimators for the simple linear model <span class="math inline">\(y_i = \beta_0+\beta_1x_i+\epsilon_i,\ i=1,\dots,n\)</span>, where <span class="math inline">\(\epsilon_i\stackrel{iid}{\sim} N(0,\sigma^2)\)</span>. Which of the following statements are true? ( BCD )</li>
</ol>
<p>A. <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> are independent.</p>
<p>B. <span class="math inline">\(\hat\beta_0-\hat\beta_1\)</span> is normally distributed.</p>
<p>C. The more spread out the <span class="math inline">\(x_i\)</span> are the better we can estimate the slope <span class="math inline">\(\beta_1\)</span>.</p>
<p>D. <span class="math inline">\(\bar y = \hat\beta_0+\hat\beta_1 \bar x\)</span>, where <span class="math inline">\(\bar x = \frac 1 n\sum_{i=1}^n x_i,\ \bar y = \frac 1 n\sum_{i=1}^n y_i\)</span>.</p>
<hr />
<ol start="5" style="list-style-type: decimal">
<li>Let <span class="math inline">\(X_1,\dots,X_n\)</span> be a simple random sample taken from <span class="math inline">\(N(2,3^2)\)</span>, and let <span class="math inline">\(\bar X\)</span> be the sample mean. Which of the following are true? ( D )</li>
</ol>
<p>A. <span class="math inline">\(\frac{\bar X -2}{3/\sqrt{n}}\sim t(n)\)</span></p>
<p>B. <span class="math inline">\(\frac 1 9\sum_{i=1}^n (X_i-2)^2\sim F(n,1)\)</span></p>
<p>C. <span class="math inline">\(\frac{\bar X-2}{\sqrt{3}/\sqrt{n}}\sim N(0,1)\)</span></p>
<p>D. <span class="math inline">\(\frac 1 9\sum_{i=1}^n(X_i-2)^2\sim \chi^2(n)\)</span></p>
<hr />
<p>Part III. (12 points)</p>
<p>Let <span class="math inline">\(X_1,\dots,X_n\)</span> be a simple random sample taken from the density</p>
<p><span class="math display">\[f(x;\theta)=\frac{2x}{\theta^2},\quad 0\le x\le \theta.\]</span></p>
<ol style="list-style-type: decimal">
<li><p>Find an expression for <span class="math inline">\(\hat\theta_L\)</span>, the maximum likelihood estimator (MLE) for <span class="math inline">\(\theta\)</span>.</p></li>
<li><p>Find an expression for <span class="math inline">\(\hat\theta_M\)</span>, the method of moments estimator for <span class="math inline">\(\theta\)</span>.</p></li>
<li><p>For the two estimators <span class="math inline">\(\hat\theta_L\)</span> and <span class="math inline">\(\hat\theta_M\)</span>, which one is more efficient in terms of mean squared error (MSE)?</p></li>
</ol>
<p><code>Solution</code>:</p>
<ol style="list-style-type: decimal">
<li>The likelihood function is</li>
</ol>
<p><span class="math display">\[L(\theta) = \prod_{i=1}^n f(x_i;\theta) = \frac{2^n}{\theta^n}\left(\prod_{i=1}^n x_i\right) 1\{x_{(n)}\le \theta\}.\]</span></p>
<p>To maximize <span class="math inline">\(L(\theta)\)</span>, we need to choose <span class="math inline">\(\theta\ge x_{(n)}\)</span> so that
<span class="math inline">\(L(\theta) = A\theta^{-n}\)</span>, where <span class="math inline">\(A=2^n\prod_{i=1}^n x_i\)</span> does not depend on <span class="math inline">\(\theta\)</span>. So the MLE is <span class="math inline">\(\hat\theta_L = X_{(n)}\)</span>.</p>
<ol start="2" style="list-style-type: decimal">
<li>First, compute the first order moment:</li>
</ol>
<p><span class="math display">\[E[X] = \int_0^\theta xf(x;\theta)dx = \int_0^\theta \frac{2x^2}{\theta^2}dx=\frac{2\theta}{3}.\]</span></p>
<p>This implies that <span class="math inline">\(\theta = 3E[X]/2\)</span>. The method of moments estimator <span class="math inline">\(\hat\theta_M=3\bar X/2\)</span>.</p>
<ol start="3" style="list-style-type: decimal">
<li>The density for <span class="math inline">\(X_{(n)}\)</span> is given by</li>
</ol>
<p><span class="math display">\[f_{X_{(n)}}(x;\theta) = nF^{n-1}(x)f(x;\theta)=n\frac{x^{2(n-1)}}{\theta^{2(n-1)}}\frac{2x}{\theta^2}=\frac{2nx^{2n-1}}{\theta^{2n}},\quad 0\le x\le \theta.\]</span></p>
<p>The first and second order moments for <span class="math inline">\(X_{(n)}\)</span> are</p>
<p><span class="math display">\[E[X_{(n)}] = \int_0^\theta \frac{2nx^{2n}}{\theta^{2n}}dx = \frac{2n\theta}{2n+1},\]</span></p>
<p><span class="math display">\[E[X_{(n)}^2] = \int_0^\theta \frac{2nx^{2n+1}}{\theta^{2n}}dx = \frac{n\theta^2}{n+1}.\]</span></p>
<p>The MSE for <span class="math inline">\(\hat\theta_L\)</span> is given by</p>
<p><span class="math display">\[
\begin{align}
MSE(\hat\theta_L)&amp;=E[(\hat\theta_L-\theta)^2]=E[X_{(n)}^2]-2\theta E[X_{(n)}]+\theta^2\\
&amp;=\frac{n\theta^2}{n+1}-\frac{4n\theta^2}{2n+1}+\theta^2\\
&amp;=\frac{\theta^2}{(n+1)(2n+1)}.
\end{align}
\]</span>
The second order moment for <span class="math inline">\(X\)</span> is</p>
<p><span class="math display">\[E[X^2] = \int_{0}^\theta \frac{2x^3}{\theta^2}dx=\frac{\theta^2}{2}.\]</span></p>
<p>The MSE for <span class="math inline">\(\hat\theta_M\)</span> is given by</p>
<p><span class="math display">\[
\begin{align}
MSE(\hat\theta_M)&amp;=Var[\hat\theta_M]=\frac{9Var[X]}{4n}\\
&amp;=\frac{9}{4n}(E[X^2]-E[X]^2)\\
&amp;=\frac{9}{4n}\left(\frac{\theta^2}{2}-\frac{4\theta^2}{9}\right)= \frac{\theta^2}{8n}.
\end{align}
\]</span></p>
<p>It is easy to see that when <span class="math inline">\(n\ge 3\)</span>, <span class="math inline">\(MSE(\hat\theta_L)&lt;MSE(\hat\theta_M)\)</span>; otherwise, <span class="math inline">\(MSE(\hat\theta_L)&gt;MSE(\hat\theta_M)\)</span>.</p>
<hr />
<p>Part IV. (10 points)</p>
<p>Let <span class="math inline">\(X_1,\dots,X_n\)</span> be a simple random sample taken from an exponential distribution <span class="math inline">\(Exp(\lambda)\)</span>, whose density is given by
<span class="math display">\[f(x;\lambda) = \lambda e^{-\lambda x}1\{x\ge 0\},\ \lambda&gt;0.\]</span>
Derive a likelihood ratio test of the hypothesis
<span class="math display">\[H_0:\lambda=1\ vs.\ H_1:\lambda=2.\]</span>
What is the definition of uniformly most powerful (UMP)? Is the test UMP against the alternative <span class="math inline">\(H_1:\lambda&gt;1\)</span>?</p>
<p><code>Solution</code>: The likelihood function is
<span class="math display">\[L(\lambda)=\prod_{i=1}^n (\lambda e^{-\lambda x_i}) = \lambda^ne^{-\lambda n\bar x}.\]</span></p>
<p>The likelihood ratio is given by
<span class="math display">\[\lambda(\vec x)= \frac{L(2)}{L(1)}=\frac{2^ne^{-2 n\bar x}}{e^{- n\bar x}}=2^ne^{-n\bar x}.\]</span></p>
<p>Choose the test statistic <span class="math inline">\(T(\vec x) = 2n\bar x\)</span>. When <span class="math inline">\(\lambda=1\)</span>, <span class="math inline">\(T(\vec X)\sim \chi^2(2n)\)</span>. Also,
<span class="math inline">\(\lambda(\vec x) = 2^ne^{-T(\vec x)/2}.\)</span> The rejection region is of the form <span class="math inline">\(W=\{T(\vec x)&lt;C\}\)</span>. We thus have <span class="math inline">\(C=\chi_{\alpha}^2(2n)\)</span>.</p>
<p>A rejection region <span class="math inline">\(W\)</span> is said to be UMP if for any rejection region <span class="math inline">\(W&#39;\)</span> with the type I error probability is no more than <span class="math inline">\(\alpha\)</span>, the power of the test associated with <span class="math inline">\(W&#39;\)</span> is no larger than that of the rejection region <span class="math inline">\(W\)</span>.</p>
<p>Consider the test of the hypothesis
<span class="math display">\[H_0:\lambda=1\ vs.\ H_1:\lambda=\lambda_0&gt;1.\]</span>
Following the same procedure above, the likelihood ratio test gives the same rejection region W. So the test derived before is also UMP for the alternative <span class="math inline">\(H_1:\lambda&gt;1\)</span> by using the N-P lemma.</p>
<hr />
<p>Part V.</p>
<p>A medical researcher believes that women typically
have lower serum cholesterol (血清胆固醇) than men. To test this
hypothesis, he took a sample of 476 men between the ages
of nineteen and forty-four and found their mean serum
cholesterol to be 189.0 mg/dl with a sample standard deviation
of 34.2. A group of 592 women in the same age range
averaged 177.2 mg/dl and had a sample standard deviation
of 33.3. Is the lower average for the women statistically
significant? Set the significant level <span class="math inline">\(\alpha\)</span> =0.05. What assumptions are made when conducting the test? (<span class="math inline">\(u_{0.95}=1.644854\)</span>, <span class="math inline">\(t_{0.95}(1066)=1.646284\)</span>, <span class="math inline">\(t_{0.95}(1068)=1.646282\)</span>, <span class="math inline">\(u_{0.975}=1.959964\)</span>, <span class="math inline">\(t_{0.975}(1066)=1.962192\)</span>, <span class="math inline">\(t_{0.975}(1068)=1.962188\)</span>)</p>
<p><code>Solution</code>: Let <span class="math inline">\(X_i\)</span> be the serum cholesterol for men, <span class="math inline">\(i=1,\dots,n=476\)</span>, let <span class="math inline">\(Y_j\)</span> be be the serum cholesterol for women, <span class="math inline">\(j=1,\dots,m=592\)</span>. We now have <span class="math inline">\(\bar x = 189.0\)</span>, <span class="math inline">\(s_{1n}=34.2\)</span>, <span class="math inline">\(\bar y = 177.2\)</span>, <span class="math inline">\(s_{2m}=33.3\)</span>. Suppose that <span class="math inline">\(X_i\stackrel{iid}{\sim} N(\mu_1,\sigma^2)\)</span> and <span class="math inline">\(Y_i\stackrel{iid}{\sim} N(\mu_2,\sigma^2)\)</span>. We are testing</p>
<p><span class="math display">\[H_0:\mu_1\le \mu_2,\ vs.\ H_1:\mu_1&gt;\mu_2.\]</span></p>
<p>We use the t-test. The test statistic is</p>
<p><span class="math display">\[T = \frac{\bar X-\bar Y}{S_w\sqrt{\frac 1 n+\frac 1 m}},\]</span>
where <span class="math inline">\(S_w^2 = (nS_{1n}^2+mS_{2m}^2)/(n+m-2)\)</span>. The rejection region is given by <span class="math inline">\(W = \{T&gt;t_{1-\alpha}(n+m-2)\}\)</span>.
The observed test statistic is <span class="math display">\[t=\frac{189.0-177.2}{33.74\sqrt{\frac 1 {476}+\frac 1 {592}}}=5.68&gt;t_{0.95}(1066)=1.65.\]</span></p>
<p>We therefore reject the null. The lower average for the women is statistically significant.</p>
<p>The assumptions are</p>
<ol style="list-style-type: decimal">
<li>normally distributed for both groups</li>
<li>the two grouds are independent</li>
<li>their variances are the same</li>
</ol>
<hr />
<p>Part VI:</p>
<p>Let <span class="math inline">\(X_1,\dots,X_n\)</span> be a simple random sample taken from the uniform distribution <span class="math inline">\(U(\theta,0)\)</span>, where <span class="math inline">\(\theta&lt;0\)</span>.</p>
<p>(a). Derive a <span class="math inline">\(100(1-\alpha)\%\)</span> confidence interval for <span class="math inline">\(\theta\)</span>.</p>
<p>(b). There is a duality between confidence intervals and
hypothesis tests. Use the result in part (a) to derive a test at significant level <span class="math inline">\(\alpha\)</span> of the hypothesis
<span class="math display">\[H_0: \theta = \theta_0\ vs.\ H_1:\theta \neq \theta_0,\]</span>
where <span class="math inline">\(\theta_0&lt;0\)</span> is fixed.</p>
<p><code>Solution</code>: Let <span class="math inline">\(G = X_{(1)}/\theta\)</span>. The CDF for <span class="math inline">\(G\)</span>
is given by</p>
<p><span class="math display">\[F_G(x) = P(G\le x) = P(X_{(1)}/\theta\le x) = P(X_{(1)}\ge \theta x) = \prod_{i=1}^nP(X_i\ge \theta x) = x^n,\ 0&lt; x&lt; 1.\]</span></p>
<p>Let<span class="math inline">\(a,b\in \mathbb{R}\)</span> such that <span class="math inline">\(P(a\le G\le b)=1-\alpha\)</span>. Then the CI for <span class="math inline">\(\theta\)</span> is</p>
<p><span class="math display">\[CI=\left[\frac{X_{(1)}}{a},\frac{X_{(1)}}{b}\right].\]</span></p>
<p>For simplicity, we take <span class="math inline">\(a,b\)</span> such that <span class="math inline">\(P(G\le a) = P(G\ge b) = \alpha/2\)</span>. This implies <span class="math inline">\(a = (\alpha/2)^{1/n},\ b= (1-\alpha/2)^{1/n}\)</span>.</p>
<p>Or you can take <span class="math inline">\(P(G\le a) = \alpha,P(G\le b)=1\)</span> so that <span class="math inline">\(a=\alpha^{1/n}, b=1\)</span>.</p>
<p>You can also use other statistics, such as <span class="math inline">\(G=X_{n}/\theta\)</span> or <span class="math inline">\(G=-2\log(\sum_{i=1}^n X_i/\theta)\)</span>. The answer is not unique.</p>
<p>Form part (a), we have</p>
<p><span class="math display">\[P_{\theta}\left(\theta\in CI\right) = 1-\alpha\ \forall\theta&lt;0.\]</span></p>
<p>We therefore choose the rejection region</p>
<p><span class="math display">\[W = \{\theta_0\notin CI\}.\]</span></p>
<p>It is easy to see that <span class="math inline">\(P_{\theta_0}(\theta_0\notin CI) = \alpha\)</span>.</p>
<p>Part VII:</p>
<p>Consider the linear model
<span class="math display">\[y_i=\beta_0+\beta_1x_i+\epsilon_i,\ \epsilon_i\stackrel{iid}{\sim} N(0,\sigma^2),\ i=1,\dots,n.\]</span>
Suppose that all the fixed <span class="math inline">\(x_i\)</span> are not equal and <span class="math inline">\(n\ge 3\)</span>.</p>
<p>(a). Derive a maximum likelihood estimator (MLE) <span class="math inline">\(\hat\sigma_L^2\)</span> for <span class="math inline">\(\sigma^2\)</span>.</p>
<p>(b). Let <span class="math inline">\(T_k=k\hat\sigma_L^2\)</span> be an estimate of <span class="math inline">\(\sigma^2\)</span>. Find a <span class="math inline">\(k\in \mathbb{R}\)</span> such that <span class="math inline">\(T_k\)</span> is an unbiased estimate of <span class="math inline">\(\sigma^2\)</span>.
Show that the unbiased estimate is not the optimal choice by taking account of mean squared error (MSE), and
the most efficient <span class="math inline">\(T_k\)</span> takes place at <span class="math inline">\(k=1\)</span>, i.e., the MLE <span class="math inline">\(\hat\sigma_L^2\)</span>.</p>
<p><code>Solution</code>: It is easy to see that <span class="math display">\[\hat\sigma_L^2=\frac{S_e^2}{n},\]</span>
where <span class="math inline">\(S_e^2 = \frac 1n\sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)^2\)</span>, and <span class="math inline">\(\hat\beta_0,\hat\beta_1\)</span> are the LSE for <span class="math inline">\(\beta_0,\beta_1\)</span>.
It is known that <span class="math inline">\(S_e^2/\sigma^2\sim \chi^2(n-2)\)</span>. This gives
<span class="math inline">\(E[S_e^2]=(n-2)\sigma^2\)</span> and <span class="math inline">\(Var[S_e^2] = 2(n-2)\sigma^4\)</span>.</p>
<p>As a result, <span class="math inline">\(E[T_k] = kE[S_e^2/n]=\frac{k(n-2)}{n}\sigma^2\)</span>. If <span class="math inline">\(T_k\)</span> is unbiased, then <span class="math inline">\(k = n/(n-2)\)</span>.
On the other hand, <span class="math display">\[Var[T_k] = \frac{k^2}{n^2}Var[S_e^2] = \frac{2(n-2)k^2}{n^2}\sigma^4.\]</span></p>
<p>The MSE of <span class="math inline">\(T_k\)</span> is given by</p>
<p><span class="math display">\[M(k) = E[(T_k-\sigma^2)^2] = (E[T_k]-\sigma^2)^2+Var[T_k]=\frac{(n-2)(k-1)^2+2}{n}\sigma^4\]</span></p>
<p>whose minimum takes place at <span class="math inline">\(k=1\)</span>.</p>
<hr />
<p>Part VIII:</p>
<p>Consider the multiple linear regression model
<span class="math display">\[y_i = \beta_0+\beta_1 x_{i1}+\beta_2x_{i2}+\dots+\beta_{p-1}x_{i,p-1} +\epsilon_i,\]</span>
where <span class="math inline">\(i=1,\dots,n\)</span> and <span class="math inline">\(n&gt;p\ge 2\)</span>.</p>
<p>(a). Find the least squares estimates (LSE) of <span class="math inline">\(\beta_0,\dots,\beta_{p-1}\)</span> via the matrix formalism. What assumptions are required for ensuring a unique solution of the LSE?</p>
<p>(b). Show that the the residuals sum to zero. Are the standard assumptions of <span class="math inline">\(E[\epsilon_i]=0\)</span> for <span class="math inline">\(i=1,\dots,n\)</span> required to establish the statement?</p>
<p>(c). Suppose that <span class="math inline">\(\epsilon_i\stackrel{iid}{\sim} N(0,\sigma^2)\)</span>, where <span class="math inline">\(\sigma&gt;0\)</span> is an unknown parameter. Define <span class="math inline">\(\alpha = \sum_{i=1}^{p-1} \beta_i^2\)</span>. Use the generalized likelihood ratio method to test the hypothesis</p>
<p><span class="math display">\[H_0: \alpha = 0\ vs.\ H_1:\alpha&gt;0.\]</span>
If the coefficient of determination <span class="math inline">\(R^2=0.95\)</span>, <span class="math inline">\(p = 3\)</span> and <span class="math inline">\(n=13\)</span>, is the null rejected at the significant level <span class="math inline">\(\alpha =0.05\)</span>? (<span class="math inline">\(F_{0.95}(2,10)=4.10,F_{0.95}(3,10)=3.71,t_{0.95}(10)=1.81\)</span>)</p>
<p><code>Solution</code>:</p>
<p>(a). The model is <span class="math inline">\(Y=X\beta+\epsilon\)</span>, and the LSE is <span class="math inline">\(\hat\beta = (X^\top X)^{-1} X^\top Y\)</span>. It is requried that
that <span class="math inline">\(\text{rank} (X) = p\)</span>.</p>
<p>(b). <span class="math inline">\(\hat\epsilon = Y-X\hat \beta = Y-X(X^\top X)^{-1} X^\top Y=(I_n-P)Y\)</span></p>
<p><span class="math display">\[\hat \epsilon^\top X = Y^\top (I_n-P)X = 0.\]</span></p>
<p>As a result, we have <span class="math inline">\(\hat \epsilon^\top 1 = \sum_{i=1}^n \hat\epsilon_i=0\)</span>. We do not require any assumption on <span class="math inline">\(\epsilon_i\)</span>.</p>
<p>(C). The test statistic is</p>
<p><span class="math display">\[F =\frac{S_R^2/(p-1)}{S_e^2/(n-p)}=\frac{R^2/(p-1)}{(1-R^2)/(n-p)}=\frac{0.95/2}{(1-0.95)/10}=95&gt;F_{0.95}(2,10)=4.1.\]</span>
We therefore reject the null.</p>
