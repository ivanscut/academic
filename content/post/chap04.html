---
title: "第四章：线性回归"
date: "2018-11-15"
categories: ["课件"]
Summary: "第四章：线性回归(Linear regression)"
tags: ["课件", "数理统计"]
---



<div id="simple-linear-models" class="section level2">
<h2>Simple linear models</h2>
<p>The linear model is given by
<span class="math display">\[y_i=\beta_0+\beta_1x_i+\epsilon_i,\ i=1,\dots,n.\]</span></p>
<ul>
<li><span class="math inline">\(\epsilon_i\)</span> are random (need some assumptions)</li>
<li><span class="math inline">\(x_i\)</span> are <strong>fixed</strong> (<em>independent/predictor</em> variable)</li>
<li><span class="math inline">\(y_i\)</span> are random (<em>dependent/response</em> variable)</li>
<li><span class="math inline">\(\beta_0\)</span> is the <em>intercept</em></li>
<li><span class="math inline">\(\beta_1\)</span> is the <em>slope</em></li>
</ul>
<div id="least-square-estimators" class="section level3">
<h3>Least square estimators</h3>
<p>Choose <span class="math inline">\(\beta_0,\beta_1\)</span> to minimize
<span class="math display">\[Q(\beta_0,\beta_1) = \sum_{i=1}^n(y_i-\beta_0-\beta_1x_i)^2.\]</span></p>
<p>The minimizers <span class="math inline">\(\hat\beta_0,\hat\beta_1\)</span> satisfy
<span class="math display">\[
\begin{cases}
\frac{\partial Q}{\partial \beta_0} = -2\sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)=0\\
\frac{\partial Q}{\partial \beta_1} = -2\sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)x_i=0
\end{cases}
\]</span></p>
<p>This gives
<span class="math display">\[\hat\beta_1 = \frac{\sum_{i=1}^n(y_i-\bar y)x_i}{\sum_{i=1}^n(x_i-\bar x)x_i},\ \hat\beta_0=\bar y-\hat\beta_1\bar x.\]</span></p>
<p>Define <span class="math display">\[\ell_{xx} = \sum_{i=1}^n(x_i-\bar x)^2,\]</span> <span class="math display">\[\ell_{yy} = \sum_{i=1}^n(y_i-\bar y)^2,\]</span> <span class="math display">\[\ell_{xy} = \sum_{i=1}^n(x_i-\bar x)(y_i-\bar y).\]</span>
We thus have
<span class="math display">\[\hat\beta_1 = \frac{\sum_{i=1}^n(y_i-\bar y)(x_i-\bar x)}{\sum_{i=1}^n(x_i-\bar x)(x_i-\bar x)}=\frac{\ell_{xy}}{\ell_{xx}}=\frac{1}{\ell_{xx}}\sum_{i=1}^n(x_i-\bar x)y_i.\]</span></p>
<p><code>Regression function</code>: <span class="math inline">\(\hat y=\hat\beta_0+\hat\beta_1x\)</span>.</p>
</div>
<div id="expected-values" class="section level3">
<h3>Expected values</h3>
<p><code>Assumption A1</code>: <span class="math inline">\(E[\epsilon_i]=0,i=1,\dots,n\)</span>.</p>
<p><code>Theorem 1</code>: Under Assumption A1, <span class="math inline">\(\hat\beta_0,\hat\beta_1\)</span> are unbiased estimators for <span class="math inline">\(\beta_0,\beta_1\)</span>, respectively.</p>
<p><code>Proof</code>:
<span class="math display">\[
\begin{align}
E[\hat \beta_1] &amp;= \frac{1}{\ell_{xx}}\sum_{i=1}^n(x_i-\bar x)E[y_i]\\
&amp;=\frac{1}{\ell_{xx}}\sum_{i=1}^n(x_i-\bar x)(\beta_0+\beta_1x_i)\\
&amp;=\frac{\beta_0}{\ell_{xx}}\sum_{i=1}^n(x_i-\bar x)+\frac{\beta_1}{\ell_{xx}}\sum_{i=1}^n(x_i-\bar x)x_i\\
&amp;=\frac{\beta_1}{\ell_{xx}}\sum_{i=1}^n(x_i-\bar x)(x_i-\bar x)\\
&amp;=\beta_1
\end{align}
\]</span></p>
<p><span class="math display">\[
\begin{align}
E[\hat \beta_0] &amp;= E[\bar y-\hat\beta_1\bar x]=E[\bar y]-\beta_1\bar x=\beta_0+\beta_1\bar x-\beta_1\bar x=\beta_0.
\end{align}
\]</span></p>
</div>
<div id="variances" class="section level3">
<h3>Variances</h3>
<p><code>Assumption A2</code>: <span class="math inline">\(Cov(\epsilon_i,\epsilon_j)=\sigma^21\{i=j\}\)</span>.</p>
<p><code>Theorem 2</code>: Under Assumption A2, we have
<span class="math display">\[Var[\hat\beta_0] = \left(\frac 1n+\frac{\bar x^2}{\ell_{xx}}\right)\sigma^2,\]</span></p>
<p><span class="math display">\[Var[\hat\beta_1] =\frac{\sigma^2}{\ell_{xx}},\]</span></p>
<p><span class="math display">\[Cov(\hat\beta_0,\hat\beta_1) = \frac{-\bar x}{\ell_{xx}}\sigma^2.\]</span></p>
<p><code>Proof</code>: Since <span class="math inline">\(Cov(\epsilon_i,\epsilon_j)=0\)</span> for any <span class="math inline">\(i\neq j\)</span>, <span class="math inline">\(Cov(y_i,y_j)=0\)</span>. We thus have
<span class="math display">\[
\begin{align}
Var[\hat\beta_1] &amp;= \frac{1}{\ell_{xx}^2}\sum_{i=1}^n(x_i-\bar x)^2Var[y_i]\\
&amp;= \frac{\sigma^2}{\ell_{xx}^2}\sum_{i=1}^n(x_i-\bar x)^2=\frac{\sigma^2}{\ell_{xx}}.
\end{align}
\]</span></p>
<p>We next show that <span class="math inline">\(Cov(\bar y,\hat \beta_1)=0\)</span>.
<span class="math display">\[
\begin{align}
Cov(\bar y,\hat \beta_1) &amp;= Cov\left(\frac{1}{n}\sum_{i=1}^n y_i,\frac{1}{\ell_{xx}}\sum_{i=1}^n(x_i-\bar x)y_i\right)\\
&amp;=\frac{1}{n\ell_{xx}}Cov\left(\sum_{i=1}^n y_i,\sum_{i=1}^n(x_i-\bar x)y_i\right)\\
&amp;=\frac{1}{n\ell_{xx}}\sum_{i=1}^nCov(y_i,(x_i-\bar x)y_i)\\
&amp;=\frac{1}{n\ell_{xx}}\sum_{i=1}^n(x_i-\bar x)\sigma^2 = 0.
\end{align}
\]</span></p>
<p><span class="math display">\[Var[\hat\beta_0] = Var[\bar y-\hat\beta_1\bar x]=Var[\bar y]+Var[\hat \beta_1\bar x]=\frac{\sigma^2}{n}+\frac{\bar x^2\sigma^2}{\ell_{xx}}.\]</span></p>
<p><span class="math display">\[Cov(\hat\beta_0,\hat\beta_1) = Cov(\bar y-\hat\beta_1\bar x,\hat\beta_1) = -\bar x Var[\hat\beta_1]=\frac{-\bar x}{\ell_{xx}}\sigma^2.\]</span></p>
<blockquote>
<p>So bigger <span class="math inline">\(n\)</span> is better. Get a bigger sample size if you can. Smaller <span class="math inline">\(\sigma\)</span> is better. The most interesting one is that bigger <span class="math inline">\(\ell_{xx}\)</span> is better. The more spread out the <span class="math inline">\(x_i\)</span> are the better we can
estimate the slope <span class="math inline">\(\beta_1\)</span>. When you’re picking the <span class="math inline">\(x_i\)</span>, if you can spread them out more, then it is more informative.</p>
</blockquote>
</div>
<div id="estimation-of-sigma2" class="section level3">
<h3>Estimation of <span class="math inline">\(\sigma^2\)</span></h3>
<p>For Assumption A2, it is common that the variance <span class="math inline">\(\sigma^2\)</span> is unknown.
The next theorem gives an unbiased estimate of <span class="math inline">\(\sigma^2\)</span>.</p>
<p><code>Definition</code>: The sum of squared errors (SSE) is defined by
<span class="math display">\[S_e^2 = \sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)^2.\]</span></p>
<p><code>Theorem 3</code>: Let
<span class="math display">\[\hat{\sigma}^2 := \frac{Q(\hat \beta_0,\hat\beta_1)}{n-2}=\frac{\sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)^2}{n-2}=\frac{S_e^2}{n-2}.\]</span>
Under Assumptions A1 and A2, we have <span class="math inline">\(E[\hat\sigma^2]=\sigma^2\)</span>.</p>
<p><code>Proof</code>: Let <span class="math inline">\(\hat y_i = \hat\beta_0+\hat\beta_1x_i=\bar y+\hat\beta_1(x_i-\bar x)\)</span>.</p>
<p><span class="math display">\[
\begin{align}
E[Q(\hat \beta_0,\hat\beta_1)] &amp;= \sum_{i=1}^nE[(y_i-\hat y_i)^2]=\sum_{i=1}^nVar[y_i-\hat y_i]+(E[y_i]-E[\hat y_i])^2\\
&amp;=\sum_{i=1}^n[Var[y_i]+Var[\hat y_i]-2Cov(y_i,\hat y_i)].
\end{align}
\]</span></p>
<p><span class="math display">\[
\begin{align}
Var[\hat y_i]&amp;= Var[\hat\beta_0+\hat\beta_1x_i]=Var[\bar y+\hat\beta_1(x_i-\bar x)]\\
&amp;=Var[\bar y]+(x_i-\bar x)^2Var[\hat\beta_1]\\
&amp;=\frac{\sigma^2}{n}+\frac{(x_i-\bar x)^2\sigma^2}{\ell_{xx}}.
\end{align}\]</span></p>
<p><span class="math display">\[
\begin{align}
Cov(y_i,\hat y_i)  &amp;= Cov(\beta_0+\beta_1x_i+\epsilon_i, \bar y+\hat\beta_1(x_i-\bar x))\\
&amp;=Cov(\epsilon_i,\bar y)+(x_i-\bar x)Cov(\epsilon_i,\hat\beta_1)\\
&amp;=\frac{\sigma^2}{n}+\frac{(x_i-\bar x)^2\sigma^2}{\ell_{xx}}.
\end{align}
\]</span></p>
<p>As a result, we have
<span class="math display">\[E[Q(\hat \beta_0,\hat\beta_1)] = \sum_{i=1}^n \left[\sigma^2-\frac{\sigma^2}{n}-\frac{(x_i-\bar x)^2\sigma^2}{\ell_{xx}}\right]=(n-2)\sigma^2.\]</span></p>
</div>
<div id="normal-distributions" class="section level3">
<h3>Normal distributions</h3>
<p><code>Assumption B</code>: <span class="math inline">\(\epsilon_i\stackrel{iid}{\sim}N(0,\sigma^2),i=1,\dots,n\)</span>.</p>
<blockquote>
<p>Assumption B includes Assumptions A1 and A2.</p>
</blockquote>
<p><code>Theorem 4</code>: Under Assumption B, we have</p>
<p>(1). <span class="math inline">\(\hat\beta_0\sim N(\beta_0,(\frac 1n+\frac{\bar x^2}{\ell_{xx}})\sigma^2)\)</span></p>
<p>(2). <span class="math inline">\(\hat\beta_1\sim N(\beta_1,\frac{\sigma^2}{\ell_{xx}})\)</span></p>
<p>(3). <span class="math inline">\(\frac{(n-2)\hat\sigma^2}{\sigma^2}=\frac{S_e^2}{\sigma^2}\sim \chi^2(n-2)\)</span></p>
<p>(4). <span class="math inline">\(\hat\sigma^2\)</span> is independent of <span class="math inline">\((\hat\beta_0,\hat\beta_1)\)</span>.</p>
<p><code>Proof</code>: Under Assumption B, <span class="math inline">\(y_i=\beta_0+\beta_1x_i+\epsilon_i\sim N(\beta_0+\beta_1x_i,\sigma^2)\)</span> independently. Both <span class="math inline">\(\hat\beta_0,\hat\beta_1\)</span> are linear combinations of <span class="math inline">\(y_i\)</span>s. Consequently, they are normally distributed. We have known their expected values and variances from Theorems 1 and 2. The claims (1) and (2) are thus verified. The proofs of claims (3) and (4) are deferred to the general case.</p>
<blockquote>
<p>It is <span class="math inline">\(n-2\)</span> degrees of freedom because we have fit two parameters to the <span class="math inline">\(n\)</span> data points.</p>
</blockquote>
</div>
<div id="confidence-intervals-and-hypothesis-tests" class="section level3">
<h3>Confidence intervals and hypothesis tests</h3>
<p>For known <span class="math inline">\(\sigma\)</span> we can make tests and confidence
intervals using
<span class="math display">\[\frac{\hat\beta_1-\beta_1}{\sigma/\sqrt{\ell_{xx}}}\sim N(0,1).\]</span></p>
<p>The <span class="math inline">\(100(1-\alpha)\%\)</span> confidence interval for <span class="math inline">\(\beta_1\)</span> is given by <span class="math inline">\(\hat\beta_1\pm u_{1-\alpha/2}\sigma/\sqrt{\ell_{xx}}\)</span>. For testing
<span class="math display">\[H_0:\beta_1=\beta_1^*\ vs.\ H_1:\beta_1\neq\beta_1^*,\]</span>
we reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(|\hat\beta_1-\beta_1^*|&gt;u_{1-\alpha/2}\sigma/\sqrt{\ell_{xx}}\)</span> with the most popular hypothesized value being <span class="math inline">\(\beta_1^*=0\)</span> (i.e., the regession function is <strong>significant</strong> or not at significance level <span class="math inline">\(\alpha\)</span>.)</p>
<p>In the more realistic setting of unknown <span class="math inline">\(\sigma\)</span>, so long as <span class="math inline">\(n \ge 3\)</span>, using claims (2-4) gives
<span class="math display">\[\frac{\hat\beta_1-\beta_1}{\hat{\sigma}/\sqrt{\ell_{xx}}}\sim t(n-2).\]</span>
The <span class="math inline">\(100(1-\alpha)\%\)</span> confidence interval for <span class="math inline">\(\beta_1\)</span> is <span class="math inline">\(\hat\beta_1\pm t_{1-\alpha/2}(n-2)\hat{\sigma}/\sqrt{\ell_{xx}}\)</span>. For testing
<span class="math display">\[H_0:\beta_1=\beta_1^*\ vs.\ H_1:\beta_1\neq\beta_1^*,\]</span>
we reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(|\hat\beta_1-\beta_1^*|&gt;t_{1-\alpha/2}(n-2)\hat\sigma/\sqrt{\ell_{xx}}\)</span>.</p>
<p>For drawing inferences about <span class="math inline">\(\beta_0\)</span>, we can use <span class="math display">\[\frac{\hat\beta_0-\beta_0}{\sigma\sqrt{1/n+\bar x^2/\ell_{xx}}}\sim N(0,1),\]</span>
<span class="math display">\[\frac{\hat\beta_0-\beta_0}{\hat\sigma\sqrt{1/n+\bar x^2/\ell_{xx}}}\sim t(n-2).\]</span></p>
<p>The <span class="math inline">\(100(1-\alpha)\%\)</span> confidence interval for <span class="math inline">\(\sigma^2\)</span> is
<span class="math display">\[\left[\frac{(n-2)\hat\sigma^2}{\chi_{1-\alpha/2}^2(n-2)},\frac{(n-2)\hat\sigma^2}{\chi_{\alpha/2}^2(n-2)}\right]=\left[\frac{S_e^2}{\chi_{1-\alpha/2}^2(n-2)},\frac{S_e^2}{\chi_{\alpha/2}^2(n-2)}\right].\]</span></p>
</div>
<div id="case-study-1" class="section level3">
<h3>Case study 1</h3>
<p>A manufacturer of air conditioning units is having assembly problems due to
the failure of a connecting rod (连接杆) to meet finished-weight specifications. Too many
rods are being completely tooled, then rejected as overweight. To reduce that
cost, the company’s quality-control department wants to quantify the relationship
between the weight of the <strong>finished rod</strong>, <span class="math inline">\(y\)</span>, and that of the <strong>rough casting</strong> (毛坯铸件), <span class="math inline">\(x\)</span>. Castings likely to produce rods that are too heavy can then be discarded
before undergoing the final (and costly) tooling process. The data are displayed below.</p>
<pre class="r"><code>rod = data.frame(
        id = seq(1:25),
        rough_weight = c(2.745, 2.700, 2.690, 2.680, 2.675, 2.670, 2.665, 
                         2.660, 2.655, 2.655, 2.650, 2.650, 2.645, 2.635,
                         2.630, 2.625, 2.625, 2.620, 2.615, 2.615, 2.615, 
                         2.610, 2.590, 2.590, 2.565),
     finished_weight = c(2.080, 2.045, 2.050, 2.005, 2.035, 2.035, 2.020, 
                         2.005, 2.010, 2.000, 2.000, 2.005, 2.015, 1.990, 
                         1.990, 1.995, 1.985, 1.970, 1.985, 1.990, 1.995, 
                         1.990, 1.975, 1.995, 1.955)
)
knitr::kable(rod,&quot;html&quot;,caption = &quot;rough weight vs. finished weight&quot;)</code></pre>
<table>
<caption>
<span id="tab:unnamed-chunk-1">Table 1: </span>rough weight vs. finished weight
</caption>
<thead>
<tr>
<th style="text-align:right;">
id
</th>
<th style="text-align:right;">
rough_weight
</th>
<th style="text-align:right;">
finished_weight
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2.745
</td>
<td style="text-align:right;">
2.080
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2.700
</td>
<td style="text-align:right;">
2.045
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
2.690
</td>
<td style="text-align:right;">
2.050
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
2.680
</td>
<td style="text-align:right;">
2.005
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
2.675
</td>
<td style="text-align:right;">
2.035
</td>
</tr>
<tr>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
2.670
</td>
<td style="text-align:right;">
2.035
</td>
</tr>
<tr>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
2.665
</td>
<td style="text-align:right;">
2.020
</td>
</tr>
<tr>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
2.660
</td>
<td style="text-align:right;">
2.005
</td>
</tr>
<tr>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
2.655
</td>
<td style="text-align:right;">
2.010
</td>
</tr>
<tr>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
2.655
</td>
<td style="text-align:right;">
2.000
</td>
</tr>
<tr>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
2.650
</td>
<td style="text-align:right;">
2.000
</td>
</tr>
<tr>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
2.650
</td>
<td style="text-align:right;">
2.005
</td>
</tr>
<tr>
<td style="text-align:right;">
13
</td>
<td style="text-align:right;">
2.645
</td>
<td style="text-align:right;">
2.015
</td>
</tr>
<tr>
<td style="text-align:right;">
14
</td>
<td style="text-align:right;">
2.635
</td>
<td style="text-align:right;">
1.990
</td>
</tr>
<tr>
<td style="text-align:right;">
15
</td>
<td style="text-align:right;">
2.630
</td>
<td style="text-align:right;">
1.990
</td>
</tr>
<tr>
<td style="text-align:right;">
16
</td>
<td style="text-align:right;">
2.625
</td>
<td style="text-align:right;">
1.995
</td>
</tr>
<tr>
<td style="text-align:right;">
17
</td>
<td style="text-align:right;">
2.625
</td>
<td style="text-align:right;">
1.985
</td>
</tr>
<tr>
<td style="text-align:right;">
18
</td>
<td style="text-align:right;">
2.620
</td>
<td style="text-align:right;">
1.970
</td>
</tr>
<tr>
<td style="text-align:right;">
19
</td>
<td style="text-align:right;">
2.615
</td>
<td style="text-align:right;">
1.985
</td>
</tr>
<tr>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
2.615
</td>
<td style="text-align:right;">
1.990
</td>
</tr>
<tr>
<td style="text-align:right;">
21
</td>
<td style="text-align:right;">
2.615
</td>
<td style="text-align:right;">
1.995
</td>
</tr>
<tr>
<td style="text-align:right;">
22
</td>
<td style="text-align:right;">
2.610
</td>
<td style="text-align:right;">
1.990
</td>
</tr>
<tr>
<td style="text-align:right;">
23
</td>
<td style="text-align:right;">
2.590
</td>
<td style="text-align:right;">
1.975
</td>
</tr>
<tr>
<td style="text-align:right;">
24
</td>
<td style="text-align:right;">
2.590
</td>
<td style="text-align:right;">
1.995
</td>
</tr>
<tr>
<td style="text-align:right;">
25
</td>
<td style="text-align:right;">
2.565
</td>
<td style="text-align:right;">
1.955
</td>
</tr>
</tbody>
</table>
<p>Consider the linear model
<span class="math display">\[y_i=\beta_0+\beta_1x_i+\epsilon_i,\ \epsilon_i\stackrel{iid}{\sim}N(0,\sigma^2).\]</span></p>
<p>The observed data gives <span class="math inline">\(\bar x = 2.643\)</span>, <span class="math inline">\(\bar y=2.0048\)</span>, <span class="math inline">\(\ell_{xx}=0.0367\)</span>, <span class="math inline">\(\ell_{xy}=0.023565\)</span>, <span class="math inline">\(\hat\sigma = 0.0113\)</span>.
The least square estimates are
<span class="math display">\[\hat\beta_1=\frac{\ell_{xy}}{\ell_{xx}}=\frac{0.023565}{0.0367}=0.642,\ \hat\beta_0=\bar y-\hat\beta_1\bar x=0.308.\]</span></p>
<p>The regession function <span class="math inline">\(\hat y = 0.308+0.642 x\)</span>; see the blue line given below.</p>
<pre class="r"><code>attach(rod)
par(mar=c(4,4,1,0.5))
plot(rough_weight,finished_weight,type=&quot;p&quot;,pch=16,
     xlab = &quot;Rough Weight&quot;,ylab = &quot;Finished Weight&quot;)
lm.rod = lm(finished_weight~rough_weight)
abline(coef(lm.rod),col=&quot;blue&quot;)</code></pre>
<p><img src="/post/chap04_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<pre class="r"><code>summary(lm.rod) #output the results</code></pre>
<pre><code>## 
## Call:
## lm(formula = finished_weight ~ rough_weight)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.023558 -0.008242  0.001074  0.008179  0.024231 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   0.30773    0.15608   1.972   0.0608 .  
## rough_weight  0.64210    0.05905  10.874 1.54e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.01131 on 23 degrees of freedom
## Multiple R-squared:  0.8372, Adjusted R-squared:  0.8301 
## F-statistic: 118.3 on 1 and 23 DF,  p-value: 1.536e-10</code></pre>
</div>
<div id="assessing-the-fit" class="section level3">
<h3>Assessing the Fit</h3>
<p>As an aid in assessing the quality of the fit, we will make extensive use of the residuals,
which are the differences between the observed and fitted values:
<span class="math display">\[\hat \epsilon_i = y_i-\hat\beta_0-\hat\beta_1x_i,\ i=1,\dots,n.\]</span></p>
<p>It is most useful to examine the residuals graphically. Plots of the residuals versus the
<span class="math inline">\(x\)</span> values may reveal systematic misfit or ways in which the data do not conform to
the fitted model. Ideally, the residuals should show no relation to the <span class="math inline">\(x\)</span> values, and the plot should look like a horizontal blur. The residuals for case study 1 are plotted below.</p>
<pre class="r"><code>par(mar=c(4,4,2,1))
plot(lm.rod$fitted.values,lm.rod$residuals,&quot;p&quot;,
     xlab=&quot;Fitted values&quot;,ylab = &quot;Residuals&quot;)</code></pre>
<p><img src="/post/chap04_files/figure-html/unnamed-chunk-3-1.png" width="672" />
Standardized Residuals are graphed below. The key command is <code>rstandard</code>.</p>
<pre class="r"><code>par(mar=c(4,4,2,1))
plot(lm.rod$fitted.values,rstandard(lm.rod),&quot;p&quot;,
     xlab=&quot;Fitted values&quot;,ylab = &quot;Standardized Residuals&quot;)
abline(h=c(-2,2),lty=c(5,5))</code></pre>
<p><img src="/post/chap04_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
<div id="drawing-inferences-about-ey" class="section level3">
<h3>Drawing Inferences about <span class="math inline">\(E[y]\)</span></h3>
<p>For given <span class="math inline">\(x\)</span>, we want to estimate the expected value of <span class="math inline">\(y\)</span>, i.e., <span class="math inline">\(E[y]=\beta_0+\beta_1x.\)</span> A natural unbiased estimate is <span class="math inline">\(\hat y = \hat\beta_0+\hat\beta_1x\)</span>. From the proof of Theorem 3, we have the variance
<span class="math display">\[Var[\hat y] = \left(\frac{1}{n}+\frac{(x-\bar x)^2}{\ell_{xx}}\right)\sigma^2.\]</span>
Under Assumption B, by Theorem 4, we have
<span class="math display">\[\hat y\sim N(\beta_0+\beta_1x,(1/n+(x-\bar x)^2/\ell_{xx})\sigma^2),\]</span>
<span class="math display">\[\frac{\hat y-E[\hat y]}{\hat{\sigma}\sqrt{1/n+(x-\bar x)/\ell_{xx}}}\sim t(n-2)\]</span>
We thus have the following results.</p>
<p><code>Theorem 5</code>: Suppose Assumption B is satisfied. Then we have
<span class="math display">\[\hat y = \hat\beta_0+\hat\beta_1x \sim N(\beta_0+\beta_1x,[1/n+(x-\bar x)^2/\ell_{xx}]\sigma^2).\]</span>
A <span class="math inline">\(100(1−\alpha)\%\)</span> confidence interval for <span class="math inline">\(E[y]=\beta_0+\beta_1x\)</span> is
given by
<span class="math display">\[\hat y\pm t_{1-\alpha/2}(n-2)\hat{\sigma}\sqrt{\frac{1}{n}+\frac{(x-\bar x)^2}{\ell_{xx}}}.\]</span></p>
<blockquote>
<p>Notice from the formula in Theorem 5 that the width of a confidence
interval for <span class="math inline">\(E[y]\)</span> increases as the value of <span class="math inline">\(x\)</span> becomes more extreme. That
is, we are better able to predict the location of the regression line for an <span class="math inline">\(x\)</span>-value
close to <span class="math inline">\(\bar x\)</span> than we are for <span class="math inline">\(x\)</span>-values that are either very small or very large.</p>
</blockquote>
<p>For case study 1, we plot the lower and upper limits for the <span class="math inline">\(95\%\)</span> confidence interval for <span class="math inline">\(E[y]\)</span>.</p>
<pre class="r"><code>x = seq(2.5,2.8,by=0.001)
newdata = data.frame(rough_weight= x)
pred_x = predict(lm.rod,newdata,interval = &quot;confidence&quot;)
par(mar=c(4,4,2,1))
matplot(x,pred_x,type=&quot;l&quot;,lty = c(1,5,5),
        col=c(&quot;blue&quot;,&quot;red&quot;,&quot;red&quot;),lwd=2,
        xlab=&quot;Rough Weight&quot;,ylab=&quot;Finished Weight&quot;)
abline(v=mean(rough_weight),lty=5)
points(rough_weight,finished_weight,pch=16)
legend(2.5,2.1,c(&quot;Fitted&quot;,&quot;Lower limit&quot;,&quot;Upper limit&quot;),
       lty = c(1,5,5),col=c(&quot;blue&quot;,&quot;red&quot;,&quot;red&quot;))</code></pre>
<p><img src="/post/chap04_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
</div>
<div id="drawing-inferences-about-future-observations" class="section level3">
<h3>Drawing Inferences about Future Observations</h3>
<p>We now give a <strong>prediction interval</strong> for the future observation <span class="math inline">\(y\)</span> rather than its expected value <span class="math inline">\(E[y]\)</span>. Note that here <span class="math inline">\(y\)</span> is no longer a fixed parameter, which is assumed to be independent of <span class="math inline">\(y_i\)</span>’s. A prediction interval is a range of numbers that
contains <span class="math inline">\(y\)</span> with a specified probability. Consider <span class="math inline">\(y-\hat y\)</span>. If Assumption A1 is satisfied, then
<span class="math display">\[E[y-\hat y] = E[y]-E[\hat y]= 0.\]</span></p>
<p>If Assumption A2 is satisfied, then
<span class="math display">\[Var[y-\hat y] = Var[y]+Var[\hat y]=\left(1+\frac{1}{n}+\frac{(x-\bar x)^2}{\ell_{xx}}\right)\sigma^2.\]</span></p>
<p>If Assumption B is satisfied, <span class="math inline">\(y-\hat y\)</span> is then normally distributed.</p>
<p><code>Theorem 6</code>: Suppose Assumption B is satisfied. Let <span class="math inline">\(y=\beta_0+\beta_1x+\epsilon\)</span>, where <span class="math inline">\(\epsilon\sim N(0,\sigma^2)\)</span> is independent of <span class="math inline">\(\epsilon_i\)</span>’s. A <span class="math inline">\(100(1−\alpha)\%\)</span> prediction interval for <span class="math inline">\(y\)</span> is
given by
<span class="math display">\[\hat y\pm t_{1-\alpha/2}(n-2)\hat{\sigma}\sqrt{1+\frac{1}{n}+\frac{(x-\bar x)^2}{\ell_{xx}}}.\]</span></p>
<p>For case study 1, we plot the lower and upper limits for the <span class="math inline">\(95\%\)</span> prediction interval for <span class="math inline">\(y\)</span>.</p>
<pre class="r"><code>x = seq(2.5,2.8,by=0.001)
newdata = data.frame(rough_weight= x)
pred_x = predict(lm.rod,newdata,interval = &quot;prediction&quot;)
par(mar=c(4,4,2,1))
matplot(x,pred_x,type=&quot;l&quot;,lty = c(1,5,5),
        col=c(&quot;blue&quot;,&quot;red&quot;,&quot;red&quot;),lwd=2,
        xlab=&quot;Rough Weight&quot;,ylab=&quot;Finished Weight&quot;)
abline(v=mean(rough_weight),lty=5)
points(rough_weight,finished_weight,pch=16)
legend(2.5,2.1,c(&quot;Fitted&quot;,&quot;Lower limit&quot;,&quot;Upper limit&quot;),
       lty = c(1,5,5),col=c(&quot;blue&quot;,&quot;red&quot;,&quot;red&quot;))</code></pre>
<p><img src="/post/chap04_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
<div id="how-to-control-y" class="section level3">
<h3>How to control y?</h3>
<p>Consider case study 1 again. Castings likely to produce rods that are too heavy can then be discarded before undergoing the final (and costly) tooling process. The company’s quality-control department wants to produce the rod <span class="math inline">\(y\)</span> with weights no large than 2.05 with probablity no less than 0.95. How to choose the rough casting?</p>
<p>Now we want <span class="math inline">\(y\le y_0=2.05\)</span> with probability <span class="math inline">\(1-\alpha\)</span>. Similarly to Theorem 6, we can construct one-side confidence interval for <span class="math inline">\(y\)</span>, that is
<span class="math display">\[\bigg(-\infty,\hat y+t_{1-\alpha}(n-2)\hat{\sigma}\sqrt{1+\frac{1}{n}+\frac{(x-\bar x)^2}{\ell_{xx}}}\bigg].\]</span>
This implies <span class="math display">\[\hat\beta_0+\hat\beta_1x+t_{1-\alpha}(n-2)\hat{\sigma}\sqrt{1+\frac{1}{n}+\frac{(x-\bar x)^2}{\ell_{xx}}}\le y_0.\]</span></p>
<p><img src="/post/chap04_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
</div>
</div>
<div id="multiple-linear-regression" class="section level2">
<h2>Multiple linear regression</h2>
<p>With problems more complex than fitting a straight line, it is very useful to approach the linear least squares analysis via linear algebra.
Consider a model of the form
<span class="math display">\[y_i=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\dots+\beta_{p-1}x_{i,p-1}+\epsilon_i,\ i=1,\dots,n.\]</span></p>
<p>Let <span class="math inline">\(Y=(y_1,\dots,y_n)^\top\)</span>, <span class="math inline">\(\beta=(\beta_0,\dots,\beta_{p-1})^\top\)</span>, <span class="math inline">\(\epsilon=(\epsilon_1,\dots,\epsilon_n)^\top\)</span>, and let <span class="math inline">\(X\)</span> be the <span class="math inline">\(n\times p\)</span> matrix
<span class="math display">\[
X=
\left[
\begin{matrix}
1 &amp; x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1,p-1}\\
1 &amp; x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2,p-1}\\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \\
1 &amp; x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{n,p-1}\\
\end{matrix}
\right].
\]</span></p>
<p>The model can be rewritten as <span class="math display">\[Y=X\beta+\epsilon,\]</span></p>
<ul>
<li><p>the matrix <span class="math inline">\(X\)</span> is called the <strong>design matrix</strong>,</p></li>
<li><p>assume that <span class="math inline">\(p&lt;n\)</span>.</p></li>
</ul>
<p>The
least squares problem can then be phrased as follows: Find <span class="math inline">\(\beta\)</span> to minimize</p>
<p><span class="math display">\[Q(\beta)=\sum_{i=1}^n(y_i-\beta_0-\beta_1x_{i1}-\dots-\beta_{p-1}x_{i,p-1})^2:=||Y-X\beta||^2,\]</span></p>
<p>where <span class="math inline">\(||\cdot||\)</span> is the Euclidean norm.</p>
<p>Note that <span class="math display">\[Q=(Y-X\beta)^\top(Y-X\beta) = Y^\top Y-2Y^\top X\beta+\beta^\top X^\top X\beta.\]</span>
If we differentiate <span class="math inline">\(Q\)</span> with respect to each <span class="math inline">\(\beta_i\)</span> and set the derivatives equal to zero, we see that the minimizers <span class="math inline">\(\hat\beta_0,\dots,\hat\beta_{p-1}\)</span> satisfy</p>
<p><span class="math display">\[\frac{\partial Q}{\partial \beta_i}=-2(Y^\top X)_i+2(X^{\top}X)_{i\cdot}\hat\beta=0.\]</span></p>
<p>We thus arrive at the so-called <strong>normal equations</strong>: <span class="math display">\[X^\top X\hat\beta = X^\top Y.\]</span></p>
<p>If the design matrix <span class="math inline">\(X^\top X\)</span> is <strong>nonsingular</strong>, the formal solution is
<span class="math display">\[\hat\beta = (X^\top X)^{-1}X^\top Y.\]</span></p>
<p>The following lemma gives a criterion for the existence and uniqueness of solutions
of the normal equations.</p>
<p><code>Lemma 1</code>: The design matrix <span class="math inline">\(X^\top X\)</span> is nonsingular if and only if <span class="math inline">\(\mathrm{rank}(X)=p\)</span>.</p>
<p><code>Proof</code>: First suppose that <span class="math inline">\(X^\top X\)</span> is singular. There exists a nonzero vector <span class="math inline">\(u\)</span> such that
<span class="math inline">\(X^\top Xu = 0\)</span>. Multiplying the left-hand side of this equation by <span class="math inline">\(u^\top\)</span>, we have <span class="math inline">\(0=u^\top X^\top Xu=(Xu)^\top (Xu)\)</span>
So <span class="math inline">\(Xu=0\)</span>, the columns of <span class="math inline">\(X\)</span> are linearly dependent, and the rank of <span class="math inline">\(X\)</span> is less
than <span class="math inline">\(p\)</span>.</p>
<p>Next, suppose that the rank of <span class="math inline">\(X\)</span> is less than <span class="math inline">\(p\)</span> so that there exists a nonzero
vector <span class="math inline">\(u\)</span> such that <span class="math inline">\(Xu = 0\)</span>. Then <span class="math inline">\(X^\top Xu = 0\)</span>, and hence <span class="math inline">\(X^\top X\)</span> is singular.</p>
<blockquote>
<p>In what follows, we assume that <span class="math inline">\(\mathrm{rank}(X)=p&lt;n\)</span>.</p>
</blockquote>
<div id="expected-values-and-variances" class="section level3">
<h3>Expected values and variances</h3>
<p><code>Assumption A</code>: Assume that <span class="math inline">\(E[\epsilon]=0\)</span> and <span class="math inline">\(Var[\epsilon]=\sigma^2I_n\)</span>.</p>
<p><code>Theorem 7</code>: Suppose that Assumption A is satisfied and <span class="math inline">\(\mathrm{rank}(X)=p&lt;n\)</span>, we have</p>
<p>(1). <span class="math inline">\(E[\hat\beta]=\beta,\)</span></p>
<p>(2). <span class="math inline">\(Var[\hat\beta]=\sigma^2(X^\top X)^{-1}\)</span>.</p>
<p><code>Proof</code>:</p>
<p><span class="math display">\[
\begin{align}
E[\hat\beta]&amp;= E[(X^\top X)^{-1}X^{\top}Y] \\
&amp;= (X^\top X)^{-1}X^{\top}E[Y]\\
&amp;=(X^\top X)^{-1}X^{\top}(X\beta)=\beta.
\end{align}\]</span></p>
<p><span class="math display">\[
\begin{align}
Var[\hat\beta] &amp;= Var[(X^\top X)^{-1}X^{\top}Y]\\
&amp;=(X^\top X)^{-1}X^{\top}Var(Y)X(X^\top X)^{-1}\\
&amp;=\sigma^2(X^\top X)^{-1}.
\end{align}
\]</span></p>
<p>We used the fact that <span class="math inline">\(Var(AY) = AVar(Y)A^\top\)</span> for any fixed matrix <span class="math inline">\(A\)</span>, and <span class="math inline">\(X^\top X\)</span> and therefore <span class="math inline">\((X^\top X)^{-1}\)</span> are symmetric.</p>
</div>
<div id="estimation-of-sigma2-1" class="section level3">
<h3>Estimation of <span class="math inline">\(\sigma^2\)</span></h3>
<p><code>Definition</code>:</p>
<ul>
<li><p><strong>The fitted values</strong>: <span class="math inline">\(\hat Y = X\hat\beta\)</span></p></li>
<li><p><strong>The vector of residuals</strong>: <span class="math inline">\(\hat\epsilon = Y-\hat Y\)</span></p></li>
<li><p><strong>The sum of squared errors (SSE)</strong>: <span class="math inline">\(S_e^2=Q(\hat\beta)=||Y-\hat Y||^2=||\hat\epsilon||^2\)</span></p></li>
</ul>
<p>Note that</p>
<p><span class="math display">\[\hat Y = X\hat\beta=X(X^\top X)^{-1}X^\top Y=:PY\]</span></p>
<ul>
<li><strong>The projection matrix</strong>: <span class="math inline">\(P = X(X^\top X)^{-1}X^\top\)</span></li>
</ul>
<p>The vector of residuals is then <span class="math inline">\(\hat\epsilon=(I_n-P)Y\)</span>. Two useful properties of <span class="math inline">\(P\)</span> are given in the following lemma.</p>
<p><code>Lemma 2</code>: Let <span class="math inline">\(P\)</span> be defined as before. Then
<span class="math display">\[P = P^\top=P^2\]</span></p>
<p><span class="math display">\[I_n-P = (I_n-P)^\top=(I_n-P)^2.\]</span></p>
<p><code>Note</code>: We may think
geometrically of the fitted values, <span class="math inline">\(\hat Y=X\hat\beta=PY\)</span>, as being the projection of <span class="math inline">\(Y\)</span> onto the subspace
spanned by the columns of <span class="math inline">\(X\)</span>.</p>
<p>The sum of squared residuals is then
<span class="math display">\[
\begin{align}
S_e^2 := ||\hat \epsilon||^2 = Y^\top(I_n-P)^\top(I_n-P)Y=Y^\top(I_n-P)Y.
\end{align}
\]</span></p>
<p><code>Definition</code>: Let <span class="math inline">\(A\)</span> be an <span class="math inline">\(n\times n\)</span> matrix. The trace of the matrix <span class="math inline">\(A\)</span> is defined as <span class="math inline">\(tr(A) = \sum_{i=1}^n a_{ii}\)</span>, where <span class="math inline">\(a_{ii}\)</span> are the elements on the main diagonal.</p>
<p><code>Lemma 3</code>: If <span class="math inline">\(A\)</span> is an <span class="math inline">\(m \times n\)</span> matrix and <span class="math inline">\(B\)</span> is an <span class="math inline">\(n \times m\)</span> matrix, then <span class="math display">\[tr(AB)=tr(BA).\]</span> This is the cyclic property of the trace.</p>
<p>Using Lemma 3, we have</p>
<p><span class="math display">\[
\begin{align}
E[S_e^2]&amp;= E[Y^\top(I_n-P)Y]=E[tr(Y^\top(I_n-P)Y)] \\&amp;= E[tr((I_n-P)YY^\top)]=tr((I_n-P)E[YY^\top])\\
&amp;=tr((I_n-P)(Var[Y]+E[Y]E[Y^\top]))\\
&amp;=tr((I_n-P)(\sigma^2 I_n))+tr((I_n-P)X\beta\beta^\top X^\top)\\
&amp;=\sigma^2(n-tr(P))
\end{align}
\]</span>
where we used <span class="math inline">\((I_n-P)X=X-X(X^\top X)^{-1}X^\top X=0\)</span>. Using the cyclic property of the trace again gives</p>
<p><span class="math display">\[
\begin{align}
tr(P)&amp;= tr(X(X^\top X)^{-1}X^\top)\\
&amp;=tr(X^\top X(X^\top X)^{-1})=tr(I_p)=p.
\end{align}
\]</span></p>
<p>We therefore have <span class="math inline">\(E[S_e^2]=(n-p)\sigma^2\)</span>.</p>
<p><code>Theorem 8</code>: Suppose that Assumption A is satisfied and <span class="math inline">\(\mathrm{rank}(X)=p&lt;n\)</span>,
<span class="math display">\[\hat\sigma^2 = \frac{S_e^2}{n-p}\]</span></p>
<p>is an unbiased estimate of <span class="math inline">\(\sigma^2\)</span>.</p>
</div>
<div id="normal-distribution" class="section level3">
<h3>Normal distribution</h3>
<p><code>Assumption B</code>: Assume that <span class="math inline">\(\epsilon\sim N(0,\sigma^2I_n)\)</span>.</p>
<p><code>Theorem 9</code>: Suppose that Assumption B is satisfied and <span class="math inline">\(\mathrm{rank}(X)=p&lt;n\)</span>, we have</p>
<p>(1). <span class="math inline">\(\hat\beta \sim N(\beta, \sigma^2(X^\top X)^{-1})\)</span>,</p>
<p>(2). <span class="math inline">\(\frac{(n-p)\hat\sigma^2}{\sigma^2}=\frac{S_e^2}{\sigma^2}\sim \chi^2(n-p)\)</span>,</p>
<p>(3). <span class="math inline">\(\hat\epsilon\)</span> is independent of <span class="math inline">\(\hat Y\)</span>,</p>
<p>(4). <span class="math inline">\(S_e^2\)</span> (or equivalently <span class="math inline">\(\hat\sigma^2\)</span>) is independent of <span class="math inline">\(\hat\beta\)</span>.</p>
<p><code>Proof</code>: If Assumption B is satisfied, then <span class="math inline">\(Y = X\beta+\epsilon\sim N(X\beta,\sigma^2_n)\)</span>. Recall that <span class="math inline">\(\hat\beta = (X^\top X)^{-1}X^\top Y\)</span> is normally distributed. The mean and covariance are given in Theorem 7 since Assumption A is satisfied.</p>
<p>Let <span class="math inline">\(\xi_1,\dots,\xi_p\)</span> be the orthogonal basis of the subspace <span class="math inline">\(\mathrm{span}(X)\)</span> of <span class="math inline">\(\mathbb{R}^n\)</span> generated by the <span class="math inline">\(p\)</span> columns of the matrix <span class="math inline">\(X\)</span>, and <span class="math inline">\(\xi_{p+1},\dots,\xi_n\)</span> be the orthogonal basis of the orthogonal complement <span class="math inline">\(\mathrm{span}(X)^\perp\)</span>. Since <span class="math inline">\(\hat Y = X\hat\beta\in \mathrm{span}(X)\)</span>, there exists <span class="math inline">\(z_1,\dots,z_p\)</span> such that
<span class="math display">\[\hat Y = \sum_{i=1}^p z_i\xi_i.\]</span></p>
<p>On the other hand, <span class="math inline">\(\hat Y^\top \hat \epsilon = (PY)^\top (I_n-P)Y = Y^\top P^\top (I_n-P)Y\)</span>. By Lemma 2, we have
<span class="math display">\[P^\top (I_n-P)=P-P^2 = 0.\]</span>
As a result, <span class="math inline">\(\hat Y^\top \hat \epsilon = 0\)</span>, implying <span class="math inline">\(\hat \epsilon\in \mathrm{span}(X)^\perp\)</span>. So there exsits <span class="math inline">\(z_{p+1},\dots,z_n\)</span> such that
<span class="math display">\[\hat \epsilon = \sum_{i=p+1}^n z_i\xi_i.\]</span>
Let <span class="math inline">\(U = (\xi_1,\dots,\xi_n)\)</span>, then <span class="math inline">\(U\)</span> is an orthogonal matrix, and let <span class="math inline">\(z=(z_1,\dots,z_n)^\top\)</span>. We thus have
<span class="math display">\[Y = \hat Y+\hat\epsilon =\sum_{i=1}^nz_i\xi_i=Uz.\]</span></p>
<p>Therefore, <span class="math display">\[z=U^{-1}Y=U^\top Y\sim N(U^\top X\beta,U^\top(\sigma^2 I_n)U)=N(U^\top X\beta,\sigma^2 I_n).\]</span></p>
<p>This implies that <span class="math inline">\(z_i\)</span> are independently normally distributed. So <span class="math inline">\(\hat Y\)</span> and <span class="math inline">\(\hat\epsilon\)</span> are independent. We next prove that <span class="math inline">\(E[z_i]=0\)</span> for all <span class="math inline">\(i&gt;p\)</span>. Let <span class="math inline">\(A=(\xi_{p+1},\dots,\xi_{n})\)</span>, then</p>
<p><span class="math display">\[A(z_{p+1},\dots,z_{n})^\top = \hat\epsilon=(I_n-P)Y.\]</span></p>
<p>This gives
<span class="math display">\[E[(z_{p+1},\dots,z_{n})^\top] = E[A^\top (I_n-P)Y]=A^\top (I_n-P)E[Y]=A^\top (I_n-P)X\beta=0.\]</span></p>
<p>Consequently, <span class="math inline">\(z_i\stackrel{iid}{\sim} N(0,\sigma^2),i=p+1,\dots,n\)</span>, implying <span class="math display">\[S_e^2/\sigma^2 = \hat\epsilon^\top\hat\epsilon/\sigma^2 = \sum_{i=p+1}^n (z_i/\sigma)^2\sim \chi^2(n-p).\]</span></p>
<p>Note that <span class="math inline">\(S_e^2\)</span> is a function of <span class="math inline">\(\hat\epsilon_i\)</span> and <span class="math inline">\(\hat \beta = (X^\top X)^{-1}X^\top Y =(X^\top X)^{-1} X^\top \hat Y\)</span> are linear conbinations of <span class="math inline">\(\hat y_i\)</span>. So <span class="math inline">\(S_e^2\)</span> and <span class="math inline">\(\hat\beta\)</span> are independent.</p>
</div>
<div id="confidence-intervals-and-hypothesis-tests-1" class="section level3">
<h3>Confidence intervals and hypothesis tests</h3>
<p>Let <span class="math inline">\(C=(X^\top X)^{-1}\)</span> with entries <span class="math inline">\(c_{ij}\)</span>. Suppose that Assumption B is satisfied and <span class="math inline">\(\mathrm{rank}(X)=p&lt;n\)</span>. If <span class="math inline">\(\sigma^2\)</span> is known, for each <span class="math inline">\(\beta_i\)</span>, the <span class="math inline">\(100(1-\alpha)\%\)</span> CI is</p>
<p><span class="math display">\[\hat\beta_i \pm u_{1-\alpha/2}\sigma\sqrt{c_{ii}}.\]</span></p>
<p>If <span class="math inline">\(\sigma^2\)</span> is unknown, for each <span class="math inline">\(\beta_i\)</span>, the <span class="math inline">\(100(1-\alpha)\%\)</span> CI is</p>
<p><span class="math display">\[\hat\beta_i \pm t_{1-\alpha/2}(n-p)\hat\sigma\sqrt{c_{ii}}.\]</span>
For testing hypothesis <span class="math inline">\(H_0:\beta_i= 0\ vs.\ H_1:\beta_i\neq 0\)</span>, the rejection region is
<span class="math display">\[W = \{|\beta_i|&gt;t_{1-\alpha/2}(n-p)\hat\sigma\sqrt{c_{ii}}\}.\]</span></p>
<p>The <span class="math inline">\(100(1-\alpha)\%\)</span> CI for <span class="math inline">\(\sigma^2\)</span> is <span class="math display">\[\left[\frac{(n-p)\hat\sigma^2}{\chi_{1-\alpha/2}^2(n-p)},\frac{(n-p)\hat\sigma^2}{\chi_{\alpha/2}^2(n-2)}\right]=\left[\frac{S_e^2}{\chi_{1-\alpha/2}^2(n-p)},\frac{S_e^2}{\chi_{\alpha/2}^2(n-p)}\right].\]</span></p>
</div>
<div id="significance-tests" class="section level3">
<h3>Significance tests</h3>
<p>Consider the hypothesis test:</p>
<p><span class="math display">\[H_0:\beta_1=\dots=\beta_{p-1}=0\ vs.\ H_1: \beta_{i^*}\neq 0\text{ for some }i^*\ge 1.\]</span></p>
<p><code>Definition</code>:</p>
<ul>
<li><strong>The total sum of squares (SST)</strong>:</li>
</ul>
<p><span class="math display">\[S_T^2 = \sum_{i=1}^n(y_i-\bar Y)^2\]</span></p>
<ul>
<li><strong>The sum of squares due to regression (SSR)</strong>:</li>
</ul>
<p><span class="math display">\[S_R^2  = \sum_{i=1}^n(\hat y_i-\bar Y)^2\]</span></p>
<ul>
<li><strong>The sum of squared errors (SSE)</strong>:</li>
</ul>
<p><span class="math display">\[S_e^2 = \sum_{i=1}^n(y_i-\hat y_i)^2\]</span></p>
<p>The relationship is
<span class="math display">\[S_T^2=S_R^2+S_e^2.\]</span></p>
<p>It is easy to see that</p>
<p><span class="math display">\[
\begin{align}
S_T^2&amp;=\sum_{i=1}^n (y_i-\bar Y)^2 = \sum_{i=1}^n (y_i-\hat y_i+\hat y_i-\bar Y)^2\\
&amp;=\sum_{i=1}^n [(y_i-\hat y_i)^2+(\hat y_i-\bar Y)^2+2(y_i-\hat y_i)(\hat y_i-\bar Y)]\\
&amp;=S_R^2+S_e^2 +2\sum_{i=1}^n (y_i-\hat y_i)(\hat y_i-\bar Y)
\end{align}
\]</span></p>
<p>Using Lemma 2, we have
<span class="math display">\[
\begin{align}
\sum_{i=1}^n (y_i-\hat y_i)(\hat y_i-\bar Y) &amp;= \hat\epsilon^\top\hat Y-\bar Y\sum_{i=1}^n \hat\epsilon_i\\
&amp;= [(I_n-P)Y]^\top PY-\bar Y(1,1,\dots,1) (I_n-P)Y\\
&amp;=Y^\top (I_n-P)PY - \bar Y[(1,1,\dots,1)-(1,1,\dots,1)P]Y\\
&amp;= 0.
\end{align}
\]</span></p>
<p>This is because <span class="math inline">\(PX = X\)</span> and the first column of <span class="math inline">\(X\)</span> is <span class="math inline">\((1,1,\dots,1)^\top\)</span>. This implies that <span class="math inline">\(P(1,1,\dots,1)^\top = (1,1,\dots,1)^\top\)</span>.</p>
<p><code>Theorem 10</code>: Suppose that Assumption B is satisfied and <span class="math inline">\(\mathrm{rank}(X)=p&lt;n\)</span>, we have</p>
<p>(1). <span class="math inline">\(S_R^2,S_e^2,\bar Y\)</span> are independent, and</p>
<p>(2). if the null <span class="math inline">\(H_0:\beta_1=\dots=\beta_{p-1}=0\)</span> is true, <span class="math inline">\(S_R^2/\sigma^2\sim\chi^2(p-1)\)</span>.</p>
<p><code>Proof</code>: Using the same notations in Theorem 9. Since <span class="math inline">\((1,\dots,1)^\top\in \mathrm{span}(X)\)</span>, we set <span class="math inline">\(\xi_1 = (1/\sqrt{n},1/\sqrt{n},\dots,1/\sqrt{n})^\top\)</span>. Recall that
<span class="math display">\[Y = \sum_{i=1}^n z_i\xi_i = Uz.\]</span>
The average <span class="math inline">\(\bar Y = (1/n,\dots,1/n)Y = (1/n,\dots,1/n)Uz=z_1/\sqrt{n},\)</span> where we used the fact that <span class="math inline">\(U\)</span> is orthogonal matrix and the first colum of <span class="math inline">\(U\)</span> is <span class="math inline">\(\xi_1 = (1/\sqrt{n},1/\sqrt{n},\dots,1/\sqrt{n})^\top\)</span>.</p>
<p>Recall that <span class="math inline">\(\hat Y = \sum_{i=1}^p z_i\xi_i = (z_1/\sqrt{n},z_1/\sqrt{n},\dots,z_1/\sqrt{n})^\top+\sum_{i=2}^p z_i\xi_i\)</span>.
This implies that
<span class="math display">\[(\hat y_1-\bar Y,\dots,\hat y_n-\bar Y)^\top = \sum_{i=2}^p z_i\xi_i.\]</span>
As a result, <span class="math inline">\(S_R^2 = \sum_{i=2}^p z_i^2\)</span>. Recall that <span class="math inline">\(S_e^2=\sum_{i=p+1}^n z_i^2\)</span>. Since <span class="math inline">\(z_i\)</span> are independent, <span class="math inline">\(S_R^2,S_e^2,\bar Y\)</span> are independent.</p>
<p>If <span class="math inline">\(\beta_1=\dots=\beta_{p-1}=0\)</span>, we have</p>
<p><span class="math display">\[E[z] = U^\top X\beta = U^\top (\beta_0,\dots,\beta_0)^\top.\]</span></p>
<p><span class="math display">\[E[z^\top]=\beta_0(1,1,\dots,1) U = \beta_0(\sqrt{n},0,\dots,0).\]</span>
We therefore have <span class="math inline">\(z_i\sim N(0,\sigma^2)\)</span> for <span class="math inline">\(i=2,\dots,n\)</span>. So <span class="math display">\[S_R^2/\sigma^2 = \sum_{i=2}^p (z_i/\sigma)^2\sim \chi^2(p-1).\]</span></p>
<p>We now use generalized likelihood ratio (GLR) test. The likelihood function for <span class="math inline">\(Y\)</span> is given by</p>
<p><span class="math display">\[L(\beta,\sigma^2) = (2\pi \sigma^2)^{-n/2} e^{-\frac{||Y-X\beta||^2}{2\sigma^2}}.\]</span></p>
<p>The maximizers for <span class="math inline">\(L(\beta,\sigma^2)\)</span> over the parameter space <span class="math inline">\(\Theta=\{(\beta,\sigma^2)|\beta\in \mathbb{R}^p,\sigma^2&gt;0\}\)</span> are</p>
<p><span class="math display">\[\hat\beta = (X^\top X)^{-1}X^\top Y,\ \hat\sigma^2 = \frac{||Y-X\hat \beta||}{n}=\frac{S_e^2}{n}.\]</span></p>
<p>The maximizers for <span class="math inline">\(L(\beta,\sigma^2)\)</span> over the sub-space <span class="math inline">\(\Theta_0=\{(\beta,\sigma^2)|\beta_i=0,i\ge 1,\sigma^2&gt;0\}\)</span> are</p>
<p><span class="math display">\[\hat\beta^* = (\bar Y,0,\dots,0)^\top,\ \hat\sigma^{*2} = \frac{||Y-X\hat \beta^*||}{n}=\frac{S_T^2}{n}.\]</span></p>
<p>The likelihood ratio is then given by</p>
<p><span class="math display">\[\lambda = \frac{\sup_{\theta\in\Theta}L(\beta,\sigma^2)}{\sup_{\theta\in\Theta_0}L(\beta,\sigma^2)} = \frac{L(\hat\beta,\hat\sigma^2)}{L(\hat\beta^*,\hat\sigma^{*2})}= \left(\frac{S_T^2}{S_e^2}\right)^{n/2}= \left(1+\frac{S_R^2}{S_e^2}\right)^{n/2}.\]</span></p>
<p>By Theorems 9 and 10, if the null is true we have
<span class="math display">\[F=\frac{S_R^2/(p-1)}{S_e^2/(n-p)}\sim F(p-1,n-p).\]</span>
We take <span class="math inline">\(F\)</span> as the test statistic. The rejection region is <span class="math inline">\(W=\{F&gt;C\}\)</span>, where the critical value <span class="math inline">\(C = F_{1-\alpha}(p-1,n-p)\)</span> so that <span class="math inline">\(\sup_{\theta\in\Theta_0}P_\theta(F&gt;C)=\alpha\)</span>.</p>
<p><code>Definition</code>: The <strong>coefficient of determination</strong> is sometimes used as a crude measure of the strength of a relationship that has been
fit by least squares. This coefficient is defined as</p>
<p><span class="math display">\[R^2 =\frac{S_R^2}{S_T^2}=\frac{\sum_{i=1}^n(\hat y_i-\bar y)^2}{\sum_{i=1}^n(y_i-\bar y)^2}.\]</span>
It can be interpreted as the proportion of the variability of the dependent variable that
can be explained by the independent variables.</p>
<p>It is easy to see that</p>
<p><span class="math display">\[F = \frac{S_T^2 R^2/(p-1)}{S_T^2(1-R^2)/(n-p)}=\frac{ R^2/(p-1)}{(1-R^2)/(n-p)}.\]</span></p>
<p>For the simple linear model <span class="math inline">\(p=2\)</span>, we have</p>
<p><span class="math display">\[S_R^2 = \sum_{i=1}^n(\hat y_i-\bar y)^2 = \hat\beta_1^2\sum_{i=1}^n(x_i-\bar x)^2 = \frac{\ell_{xy}^2}{\ell_{xx}}.\]</span>
This gives
<span class="math display">\[R^2 = \frac{\ell_{xy}^2}{\ell_{xx}\ell_{yy}} = \rho^2,\]</span>
where <span class="math inline">\(\rho = \ell_{xy}/\sqrt{\ell_{xx}\ell_{yy}}\)</span> is the <strong>correlation coefficient</strong> between <span class="math inline">\(x_i\)</span> and <span class="math inline">\(y_i\)</span>.</p>
</div>
</div>
<div id="case-study-2" class="section level2">
<h2>Case study 2</h2>
<p>It is found that the systolic pressure is linked to the weight and the age. We now have the following data.</p>
<pre class="r"><code>blood=data.frame(
weight=c(76.0,91.5,85.5,82.5,79.0,80.5,74.5,79.0,85.0,76.5,82.0,95.0,92.5),
age=c(50,20,20,30,30,50,60,50,40,55,40,40,20),
pressure=c(120,141,124,126,117,125,123,125,132,123,132,155,147))
knitr::kable(blood,format=&quot;html&quot;)</code></pre>
<table>
<thead>
<tr>
<th style="text-align:right;">
weight
</th>
<th style="text-align:right;">
age
</th>
<th style="text-align:right;">
pressure
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
76.0
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
120
</td>
</tr>
<tr>
<td style="text-align:right;">
91.5
</td>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
141
</td>
</tr>
<tr>
<td style="text-align:right;">
85.5
</td>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
124
</td>
</tr>
<tr>
<td style="text-align:right;">
82.5
</td>
<td style="text-align:right;">
30
</td>
<td style="text-align:right;">
126
</td>
</tr>
<tr>
<td style="text-align:right;">
79.0
</td>
<td style="text-align:right;">
30
</td>
<td style="text-align:right;">
117
</td>
</tr>
<tr>
<td style="text-align:right;">
80.5
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
125
</td>
</tr>
<tr>
<td style="text-align:right;">
74.5
</td>
<td style="text-align:right;">
60
</td>
<td style="text-align:right;">
123
</td>
</tr>
<tr>
<td style="text-align:right;">
79.0
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
125
</td>
</tr>
<tr>
<td style="text-align:right;">
85.0
</td>
<td style="text-align:right;">
40
</td>
<td style="text-align:right;">
132
</td>
</tr>
<tr>
<td style="text-align:right;">
76.5
</td>
<td style="text-align:right;">
55
</td>
<td style="text-align:right;">
123
</td>
</tr>
<tr>
<td style="text-align:right;">
82.0
</td>
<td style="text-align:right;">
40
</td>
<td style="text-align:right;">
132
</td>
</tr>
<tr>
<td style="text-align:right;">
95.0
</td>
<td style="text-align:right;">
40
</td>
<td style="text-align:right;">
155
</td>
</tr>
<tr>
<td style="text-align:right;">
92.5
</td>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
147
</td>
</tr>
</tbody>
</table>
<pre class="r"><code>plot(blood)</code></pre>
<p><img src="/post/chap04_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<pre class="r"><code>lm.blood=lm(pressure~weight+age,data=blood)
summary(lm.blood)</code></pre>
<pre><code>## 
## Call:
## lm(formula = pressure ~ weight + age, data = blood)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.0404 -1.0183  0.4640  0.6908  4.3274 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -62.96336   16.99976  -3.704 0.004083 ** 
## weight        2.13656    0.17534  12.185 2.53e-07 ***
## age           0.40022    0.08321   4.810 0.000713 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.854 on 10 degrees of freedom
## Multiple R-squared:  0.9461, Adjusted R-squared:  0.9354 
## F-statistic: 87.84 on 2 and 10 DF,  p-value: 4.531e-07</code></pre>
<p>The regression function is</p>
<p><span class="math display">\[\hat y = -62.96336 + 2.13656 x_1+ 0.40022 x_2.\]</span></p>
<pre class="r"><code>n = length(blood$weight)
X = cbind(intercept=rep(1,n),weight=blood$weight,age=blood$age)
C = solve(t(X)%*%X)
SSE = sum(lm.blood$residuals^2) # sum of squared errors
# SST = var(blood$pressure)*(n-1)
# SSR = SST-SSE
# Fstat = SSR/(3-1)/(SSE/(n-3))
cov = SSE/(n-3)*C
knitr::kable(cov,format=&quot;html&quot;,caption = &quot;Estimated Covariance Matrix&quot;)</code></pre>
<table>
<caption>
<span id="tab:unnamed-chunk-9">Table 2: </span>Estimated Covariance Matrix
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
intercept
</th>
<th style="text-align:right;">
weight
</th>
<th style="text-align:right;">
age
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
intercept
</td>
<td style="text-align:right;">
288.991861
</td>
<td style="text-align:right;">
-2.9499280
</td>
<td style="text-align:right;">
-1.1174334
</td>
</tr>
<tr>
<td style="text-align:left;">
weight
</td>
<td style="text-align:right;">
-2.949928
</td>
<td style="text-align:right;">
0.0307450
</td>
<td style="text-align:right;">
0.0102176
</td>
</tr>
<tr>
<td style="text-align:left;">
age
</td>
<td style="text-align:right;">
-1.117433
</td>
<td style="text-align:right;">
0.0102176
</td>
<td style="text-align:right;">
0.0069243
</td>
</tr>
</tbody>
</table>
<p>Similarly to the simple linear model, we can construct the confidence intervals and prediction intervals for <span class="math inline">\(E[y]\)</span> and <span class="math inline">\(y\)</span>, respectively.</p>
<pre class="r"><code>newdata = data.frame(
        age = rep(31,100),
        weight = seq(70,100,length.out = 100)
)
CI = predict(lm.blood,newdata,interval = &quot;confidence&quot;)
Pred = predict(lm.blood,newdata,interval = &quot;prediction&quot;)
par(mar=c(4,4,2,1))
matplot(newdata$weight,cbind(CI,Pred[,-1]),type=&quot;l&quot;,lty = c(1,5,5,2,2),
        col=c(&quot;blue&quot;,&quot;red&quot;,&quot;red&quot;,&quot;brown&quot;,&quot;brown&quot;),lwd=2,
        xlab=&quot;Weight&quot;,ylab=&quot;Pressure&quot;,main = &quot;Age = 31&quot;)
legend(70,160,c(&quot;Fitted&quot;,&quot;Confidence&quot;,&quot;Prediction&quot;),
       lty = c(1,5,2),col=c(&quot;blue&quot;,&quot;red&quot;,&quot;brown&quot;))</code></pre>
<p><img src="/post/chap04_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<pre class="r"><code>newdata = data.frame(
        weight = rep(85,41),
        age = seq(20,60)
)
CI = predict(lm.blood,newdata,interval = &quot;confidence&quot;)
Pred = predict(lm.blood,newdata,interval = &quot;prediction&quot;)
par(mar=c(4,4,2,1))
matplot(newdata$age,cbind(CI,Pred[,-1]),type=&quot;l&quot;,lty = c(1,5,5,2,2),
        col=c(&quot;blue&quot;,&quot;red&quot;,&quot;red&quot;,&quot;brown&quot;,&quot;brown&quot;),lwd=2,
        xlab=&quot;Age&quot;,ylab=&quot;Pressure&quot;,main = &quot;Weight = 85&quot;)
legend(20,150,c(&quot;Fitted&quot;,&quot;Confidence&quot;,&quot;Prediction&quot;),
       lty = c(1,5,2),col=c(&quot;blue&quot;,&quot;red&quot;,&quot;brown&quot;))</code></pre>
<p><img src="/post/chap04_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
</div>
<div id="extension-to-general-models" class="section level2">
<h2>Extension to general models</h2>
<p><strong>Inherently Linear models</strong>:</p>
<p><span class="math display">\[\begin{align}
f(y) &amp;= \beta_0+\beta_1 g_1(x_1,\dots,x_{p-1})+\dots\\&amp;+\beta_{k-1} g_{k-1}(x_1,\dots,x_{p-1})+\epsilon
\end{align}\]</span></p>
<p>Let <span class="math inline">\(y^*=f(y),\ x_i^*=g_i(x_1,\dots,x_{p-1})\)</span>. The transformed model is linear</p>
<p><span class="math display">\[y^*=\beta_0+\beta_1 x_1^*+\dots+\beta_{k-1} x_{k-1}^{*}+\epsilon.\]</span></p>
</div>
<div id="examples" class="section level2">
<h2>Examples</h2>
<ul>
<li><p>Polynomial models: <span class="math display">\[y = \beta_0+\beta_1x+\beta_2x^2+\beta_{p-1}x^p+\epsilon\]</span></p></li>
<li><p>Interaction models: <span class="math display">\[y = \beta_0+\beta_1x_1+\beta_2x_2^2+\beta_{3}x_1x_2+\epsilon\]</span></p></li>
<li><p>Multiplicative models: <span class="math display">\[y = \gamma_1X_1^{\gamma_2}X_2^{\gamma3}\epsilon^*\]</span></p></li>
<li><p>Exponential models: <span class="math display">\[y = \exp\{\beta_0+\beta_1x_1+\beta_2x_2\}+\epsilon^*\]</span></p></li>
</ul>
</div>
<div id="examples-1" class="section level2">
<h2>Examples</h2>
<ul>
<li><p>Reciprocal models: <span class="math display">\[y=\frac{1}{\beta_0+\beta_1x+\beta_2x^2+\beta_{p-1}x^p+\epsilon}\]</span></p></li>
<li><p>Semilog models: <span class="math display">\[y = \beta_0+\beta_1\log(x)+\epsilon\]</span></p></li>
<li><p>Logit models: <span class="math display">\[\log\left(\frac{y}{1-y}\right) = \beta_0+\beta_1 x+\epsilon\]</span></p></li>
<li><p>Probit models: <span class="math inline">\(\Phi^{-1}(y) = \beta_0+\beta_1 x+\epsilon\)</span>, where <span class="math inline">\(\Phi\)</span> is the CDF of <span class="math inline">\(N(0,1)\)</span>.</p></li>
</ul>
</div>
<div class="section level2">
<h2>回归诊断</h2>
<p>回归分析都是基于误差项的假定进行的，最常见的假设<span class="math display">\[\epsilon_i\stackrel{iid}{\sim}N(0,\sigma^2).\]</span></p>
<ul>
<li><p>如何考察数据基本上满足这些假设？自然从残差的角度来解决问题，这种方法叫<strong>残差分析</strong>。</p></li>
<li><p>研究那些数据对统计推断（估计、检验、预测和控制）有较大影响的点，这样的点叫做<strong>影响点</strong>。剔除那些有较强影响的异常/离群(outlier)数据，这就是所谓的影响分析(influence analysis).</p></li>
</ul>
<p>残差的定义为
<span class="math display">\[\hat\epsilon = Y-\hat Y\]</span></p>
</div>
<div class="section level2">
<h2>残差的性质</h2>
<p>在假设<span class="math inline">\(\epsilon\sim N(0,\sigma^2I_n)\)</span>下，</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\hat\epsilon \sim N(0,\sigma^2(I_n-P))\)</span></p></li>
<li><p><span class="math inline">\(Cov(\hat Y,\hat\epsilon) = 0\)</span></p></li>
<li><p><span class="math inline">\(1^\top\hat\epsilon = 0\)</span></p></li>
</ol>
<p>从中可以看出，<span class="math inline">\(Var[\hat\epsilon_i] = \sigma^2(1-p_{ii})\)</span>, 其中<span class="math inline">\(p_{ij}\)</span>为投影矩阵的元素。该方差与<span class="math inline">\(\sigma^2\)</span>以及<span class="math inline">\(p_{ii}\)</span>有关，因此直接比较残差<span class="math inline">\(\hat\epsilon_i\)</span>是不恰当的。</p>
<p>为此，将残差标准化：</p>
<p><span class="math display">\[\frac{\hat\epsilon_i-E[\hat\epsilon_i]}{\sqrt{Var[\hat\epsilon_i}]}= \frac{\hat\epsilon_i}{\sigma\sqrt{1-p_{ii}}},\ i=1,\dots,n\]</span></p>
</div>
<div class="section level2">
<h2>学生化残差</h2>
<p>由于<span class="math inline">\(\sigma\)</span>是未知的，所以用<span class="math inline">\(\hat\sigma\)</span>来代替，其中<span class="math inline">\(\hat\sigma^2 = S_e^2/(n-p)\)</span>. 于是得到学生化(studentized residuals)</p>
<p><span class="math display">\[t_i = \frac{\hat\epsilon_i}{\hat{\sigma}\sqrt{1-p_{ii}}}\]</span></p>
<ul>
<li><p><span class="math inline">\(t_i\)</span>虽然是<span class="math inline">\(\hat\epsilon_i\)</span>的学生化，但它的分布并不服从<span class="math inline">\(t\)</span>分布，它的分布通常比较复杂</p></li>
<li><p><span class="math inline">\(t_1,\dots,t_n\)</span>通常是不独立的</p></li>
<li><p>在实际应用中，可以近似认为：<span class="math inline">\(t_1,\dots,t_n\)</span>是相互独立，服从<span class="math inline">\(N(0,1)\)</span>分布</p></li>
<li><p>在实际应用中使用的残差图就是根据上述假定来对模型合理性进行诊断的。</p></li>
</ul>
</div>
<div class="section level2">
<h2>残差图</h2>
<p>残差图：以残差为纵坐标，其他的量（一般为拟合值<span class="math inline">\(\hat y_i\)</span>）为横坐标的散点图。</p>
<p>由于可以近似认为：<span class="math inline">\(t_1,\dots,t_n\)</span>是相互独立，服从<span class="math inline">\(N(0,1)\)</span>分布，所以可以把它们看作来自<span class="math inline">\(N(0,1)\)</span>的iid样本</p>
<p>根据标准正态的性质，大概有<span class="math inline">\(95\%\)</span>的<span class="math inline">\(t_i\)</span>落入区间<span class="math inline">\([-2,2]\)</span>中。由于<span class="math inline">\(\hat Y\)</span>与<span class="math inline">\(\hat\epsilon\)</span>不相关，所以<span class="math inline">\(\hat y_i\)</span>与学生化残差<span class="math inline">\(t_i\)</span>的相关性也很小。</p>
<p>这样在残差图中，点<span class="math inline">\((\hat y_i,t_i),i=1,\dots,n\)</span>大致应该落在宽度为4的水平带<span class="math inline">\(|t_i|\le 2\)</span>的区域内，且<strong>不呈现任何趋势</strong>。</p>
</div>
<div id="1" class="section level2">
<h2>残差图（1）</h2>
<div class="figure"><span id="fig:unnamed-chunk-12"></span>
<img src="error1.png" alt="正常的残差图" width="90%" />
<p class="caption">
Figure 1: 正常的残差图
</p>
</div>
</div>
<div id="2" class="section level2">
<h2>残差图（2）</h2>
<div class="figure"><span id="fig:unnamed-chunk-13"></span>
<img src="error2.png" alt="误差随着横坐标的增加而增加" width="90%" />
<p class="caption">
Figure 2: 误差随着横坐标的增加而增加
</p>
</div>
</div>
<div id="3" class="section level2">
<h2>残差图（3）</h2>
<div class="figure"><span id="fig:unnamed-chunk-14"></span>
<img src="error3.png" alt="误差随着横坐标的增加而减少" width="90%" />
<p class="caption">
Figure 3: 误差随着横坐标的增加而减少
</p>
</div>
</div>
<div id="4" class="section level2">
<h2>残差图（4）</h2>
<div class="figure"><span id="fig:unnamed-chunk-15"></span>
<img src="error4.png" alt="误差中间大，两端小" width="90%" />
<p class="caption">
Figure 4: 误差中间大，两端小
</p>
</div>
</div>
<div id="5" class="section level2">
<h2>残差图（5）</h2>
<div class="figure"><span id="fig:unnamed-chunk-16"></span>
<img src="error5.png" alt="回归函数可能非线性，或者误差相关或者漏掉重要的自变量" width="90%" />
<p class="caption">
Figure 5: 回归函数可能非线性，或者误差相关或者漏掉重要的自变量
</p>
</div>
</div>
<div id="6" class="section level2">
<h2>残差图（6）</h2>
<div class="figure"><span id="fig:unnamed-chunk-17"></span>
<img src="error6.png" alt="回归函数可能非线性" width="90%" />
<p class="caption">
Figure 6: 回归函数可能非线性
</p>
</div>
</div>
<div class="section level2">
<h2>残差图诊断的思路</h2>
<ul>
<li>如果残差图中显示误差方差不相等(heterogeneity, 方差非齐性)，可以对变量做适当的变换，使得变换后的相应变量具有近似相等的方差(homogeneity, 方差齐性)。最著名的方法是<strong>Box-Cox变换</strong>，见综述论文：</li>
</ul>
<p>R. M. Sakia. The Box-Cox Transformation Technique: A Review. The Statistician, 41: 169-178, 1992.</p>
<ul>
<li>如果残差图中显示非线性，可适当增加自变量的二次项或者交叉项。具体问题具体分析。</li>
</ul>
</div>
<div id="outlier" class="section level2">
<h2>离群值(outlier)</h2>
<p>产生离群值的原因：</p>
<ol style="list-style-type: decimal">
<li><p>主观原因：收集和记录数据时出现错误</p></li>
<li><p>客观原因：重尾分布（比如，<span class="math inline">\(t\)</span>分布）和混合分布</p></li>
</ol>
<p>离群值的简单判断：</p>
<ol style="list-style-type: decimal">
<li><p>数据散点图</p></li>
<li><p>学生化残差图，如果<span class="math inline">\(|t_i|&gt;3\)</span> (或者2.5,2)，则对应的数据判定为离群值。</p></li>
<li><p>离群值的统计检验方法，M-估计(Maximum likelihood type estimators)</p></li>
</ol>
</div>
<div class="section level2">
<h2>案例</h2>
<p>Anscombe在1973年构造了4组数据，每组数据都是由11对点<span class="math inline">\((x_i,y_i)\)</span>组成，试分析4组数据是否通过回归方程的检验。</p>
<pre class="r"><code>data(&quot;anscombe&quot;)
knitr::kable(anscombe,format = &quot;html&quot;)</code></pre>
<table>
<thead>
<tr>
<th style="text-align:right;">
x1
</th>
<th style="text-align:right;">
x2
</th>
<th style="text-align:right;">
x3
</th>
<th style="text-align:right;">
x4
</th>
<th style="text-align:right;">
y1
</th>
<th style="text-align:right;">
y2
</th>
<th style="text-align:right;">
y3
</th>
<th style="text-align:right;">
y4
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
8.04
</td>
<td style="text-align:right;">
9.14
</td>
<td style="text-align:right;">
7.46
</td>
<td style="text-align:right;">
6.58
</td>
</tr>
<tr>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
6.95
</td>
<td style="text-align:right;">
8.14
</td>
<td style="text-align:right;">
6.77
</td>
<td style="text-align:right;">
5.76
</td>
</tr>
<tr>
<td style="text-align:right;">
13
</td>
<td style="text-align:right;">
13
</td>
<td style="text-align:right;">
13
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
7.58
</td>
<td style="text-align:right;">
8.74
</td>
<td style="text-align:right;">
12.74
</td>
<td style="text-align:right;">
7.71
</td>
</tr>
<tr>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
8.81
</td>
<td style="text-align:right;">
8.77
</td>
<td style="text-align:right;">
7.11
</td>
<td style="text-align:right;">
8.84
</td>
</tr>
<tr>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
8.33
</td>
<td style="text-align:right;">
9.26
</td>
<td style="text-align:right;">
7.81
</td>
<td style="text-align:right;">
8.47
</td>
</tr>
<tr>
<td style="text-align:right;">
14
</td>
<td style="text-align:right;">
14
</td>
<td style="text-align:right;">
14
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
9.96
</td>
<td style="text-align:right;">
8.10
</td>
<td style="text-align:right;">
8.84
</td>
<td style="text-align:right;">
7.04
</td>
</tr>
<tr>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
7.24
</td>
<td style="text-align:right;">
6.13
</td>
<td style="text-align:right;">
6.08
</td>
<td style="text-align:right;">
5.25
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
19
</td>
<td style="text-align:right;">
4.26
</td>
<td style="text-align:right;">
3.10
</td>
<td style="text-align:right;">
5.39
</td>
<td style="text-align:right;">
12.50
</td>
</tr>
<tr>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
10.84
</td>
<td style="text-align:right;">
9.13
</td>
<td style="text-align:right;">
8.15
</td>
<td style="text-align:right;">
5.56
</td>
</tr>
<tr>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
4.82
</td>
<td style="text-align:right;">
7.26
</td>
<td style="text-align:right;">
6.42
</td>
<td style="text-align:right;">
7.91
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
5.68
</td>
<td style="text-align:right;">
4.74
</td>
<td style="text-align:right;">
5.73
</td>
<td style="text-align:right;">
6.89
</td>
</tr>
</tbody>
</table>
</div>
<div class="section level2">
<h2>回归结果</h2>
<pre class="r"><code>coef.list = list()
for(i in 1:4)
{
  ff = as.formula(paste0(&quot;y&quot;,i,&quot;~&quot;,&quot;x&quot;,i))
  lmi = lm(ff,data = anscombe)
  coef.list = c(coef.list, list(summary(lmi)$coef))
}

print(coef.list)</code></pre>
<pre><code>## [[1]]
##              Estimate Std. Error  t value    Pr(&gt;|t|)
## (Intercept) 3.0000909  1.1247468 2.667348 0.025734051
## x1          0.5000909  0.1179055 4.241455 0.002169629
## 
## [[2]]
##             Estimate Std. Error  t value    Pr(&gt;|t|)
## (Intercept) 3.000909  1.1253024 2.666758 0.025758941
## x2          0.500000  0.1179637 4.238590 0.002178816
## 
## [[3]]
##              Estimate Std. Error  t value    Pr(&gt;|t|)
## (Intercept) 3.0024545  1.1244812 2.670080 0.025619109
## x3          0.4997273  0.1178777 4.239372 0.002176305
## 
## [[4]]
##              Estimate Std. Error  t value    Pr(&gt;|t|)
## (Intercept) 3.0017273  1.1239211 2.670763 0.025590425
## x4          0.4999091  0.1178189 4.243028 0.002164602</code></pre>
</div>
<div class="section level2">
<h2>回归直线</h2>
<pre class="r"><code>par(mfrow = c(2,2),mar=c(4,4,1,1)+.1,oma=c(0,0,2,0))
for(i in 1:4)
{
  ff = as.formula(paste0(&quot;y&quot;,i,&quot;~&quot;,&quot;x&quot;,i))
  lmi = lm(ff,data = anscombe)
  plot(ff,data = anscombe,col=&quot;red&quot;,pch = 21,bg=&quot;orange&quot;,
       cex = 1.2,xlim=c(3,19),ylim=c(3,13))
  abline(coef(lmi),col=&quot;blue&quot;)
}</code></pre>
<p><img src="/post/chap04_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
</div>
<div class="section level2">
<h2>残差</h2>
<pre class="r"><code>par(mfrow = c(2,2),mar=c(4,4,1,1)+.1,oma=c(0,0,2,0))
for(i in 1:4)
{
  ff = as.formula(paste0(&quot;y&quot;,i,&quot;~&quot;,&quot;x&quot;,i))
  lmi = lm(ff,data = anscombe)
  plot(lmi$fitted.values,rstandard(lmi),pch = 21,bg=&quot;orange&quot;,
       cex = 1.2,xlim=c(3,15),ylim=c(-4,4),xlab = &quot;fitted values&quot;,
       ylab = &quot;standardized residuals&quot;)
  abline(h=c(-3,3),lty=2)
}</code></pre>
<p><img src="/post/chap04_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
</div>
<div class="section level2">
<h2>异常值</h2>
<pre class="r"><code>lm1 = lm(y1~x1,data = anscombe)
x = c(anscombe$x1, 18) # 人为添加异常数据
y = c(anscombe$y1,30) # 人为添加异常数据
lm.xy = lm(y~x)
plot(x,y,pch = 21,bg=c(rep(&quot;black&quot;,11),&quot;red&quot;),ylim=c(0,50))
abline(coef(lm.xy),lty=2,col=&quot;red&quot;)
abline(coef(lm1),lty=2,col=&quot;blue&quot;)</code></pre>
<p><img src="/post/chap04_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
</div>
<div id="-1" class="section level2">
<h2>残差图</h2>
<pre class="r"><code>par(mfrow = c(1,2),mar=c(4,4,1,1)+.1,oma=c(0,0,2,0))
plot(c(fitted(lm1),fitted(lm.xy)),c(rstandard(lm1),rstandard(lm.xy)),
     ylim=c(-3,3),pch = c(rep(21,11),rep(22,12)),
     bg = c(rep(&quot;blue&quot;,11),rep(&quot;red&quot;,12)),
     xlab = &quot;fitted values&quot;,
     ylab=&quot;standardized residuals&quot;)
abline(h=c(-2,2),lty=2)
plot(lm.xy,3) #画出残差图</code></pre>
<p><img src="/post/chap04_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
</div>
<div class="section level2">
<h2>回归分析的其他内容</h2>
<ul>
<li><p>共线性(collinear)分析，岭回归(ridge regression)方法</p></li>
<li><p>变量选择方法：向前/后回归法、逐步回归法、完全子集法、交叉核实(cross validation)法，见课本P208-214</p></li>
<li><p>现代变量选择方法：LASSO (Least Absolute Shrinkage &amp; Selection Operator)</p></li>
</ul>
</div>
<div id="lasso" class="section level2">
<h2>LASSO</h2>
<p><img src="rob.jpg" /></p>
<p>Professor Rob Tibshirani: <a href="https://statweb.stanford.edu/~tibs/" class="uri">https://statweb.stanford.edu/~tibs/</a></p>
<p>LASSO是斯坦福大学统计系Tibshirani于1996年发表的著名论文“<a href="http://statweb.stanford.edu/~tibs/lasso/lasso.pdf">Regreesion shrinkage and selection via the LASSO</a>” (Journal of Royal Statistical Society, Seriers B, 58, 267-288)中所提出的一种变量选择方法。</p>
</div>
<div class="section level2">
<h2>回归分析与因果分析</h2>
<p>即使建立了回归关系式并且统计检验证明相关关系成立，也只能说明研究的变量是统计相关的，而<strong>不能就此断定变量之间有因果关系</strong>。</p>
<p>案例(Ice Cream Causes Polio)：小儿麻痹症疫苗发明前，美国北卡罗来纳州卫生部研究人员通过分析冰淇淋消费量和小儿麻痹症的关系发现当冰淇淋消费量增加时，小儿麻痹疾病也增加。州卫生部发生警告反对吃冰淇淋来试图阻止这种疾病的传播。</p>
</div>
<div class="section level2">
<h2>没有观察的混杂因素——温度</h2>
<p>Polio and ice cream consumption both increase in the summertime. Summer is when the polio virus thrived.</p>
<div class="figure"><span id="fig:unnamed-chunk-24"></span>
<img src="ice.jpg" alt="The danger of mixing up causality and correlation" width="60%" />
<p class="caption">
Figure 7: The danger of mixing up causality and correlation
</p>
</div>
</div>
