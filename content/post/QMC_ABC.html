---
title: "Quasi-Monte Carlo in ABC"
categories: ["slides"]
date: "2018-11-18"
tags: ["slides", "BDA"]
Summary: Quasi-Monte Carlo in ABC
---



<div id="ingredients-for-abc" class="section level2">
<h2>Ingredients for ABC</h2>
<ul>
<li><p>summary statistic <span class="math inline">\(S(y):\mathbb{R}^n\to \mathbb{R}^d\)</span></p></li>
<li><p>kernel function <span class="math inline">\(k(u_1,\dots,u_d)\)</span></p></li>
<li><p>bandwidth <span class="math inline">\(h&gt;0\)</span></p></li>
<li><p>proposal density <span class="math inline">\(g(\theta)\)</span></p></li>
</ul>
<p>ABC approximation</p>
<p><span class="math display">\[\pi_{ABC}(\theta|s_{obs})\propto \pi(\theta)\int \pi(s|\theta)K((s-s_{obs})/h) d s\to \pi(\theta|s_{obs})\]</span></p>
<p>If <span class="math inline">\(\pi(\theta|s_{obs})\approx \pi(\theta|y)\)</span>, then ABC density is a proper approximation of the posterior <span class="math inline">\(\pi(\theta|y)\)</span>.</p>
</div>
<div id="abc-convergence-rates" class="section level2">
<h2>ABC convergence rates</h2>
<p>Consider the estimation of <span class="math inline">\(\mu=E[a(\theta)|s_{obs}]\)</span>. The acceptance-rejection (AR) based ABC estimate is given by
<span class="math display">\[\hat\mu=\frac{1}{N}\sum_{i=1}^Na(\theta^{(i)}).\]</span></p>
<p>Under regular conditions, ABC bias is</p>
<p><span class="math display">\[\mathrm{bias}=|E_{ABC}[a(\theta)|s_{obs}]-\mu|=O(h^2),\]</span></p>
<p>and the acceptance probability is <span class="math inline">\(R = O(h^{d}),\)</span></p>
<p>and Monte Carlo variance is</p>
<p><span class="math display">\[\mathrm{variance}=\frac{\sigma^2_{ABC}}{N}=O\left(\frac{1}{C h^d}\right),\]</span></p>
<p>where <span class="math inline">\(C\)</span> is the complexity. Overall, the MSE is</p>
<p><span class="math display">\[\mathrm{MSE} = \mathrm{bias}^2+\mathrm{vaiance} = O(h^4)+O\left(\frac{1}{Ch^d}\right).\]</span></p>
<p>For a given <span class="math inline">\(C&gt;0\)</span>,</p>
<ul>
<li><p>the optimal <span class="math inline">\(h^*=O(C^{-1/(4+d)})\)</span></p></li>
<li><p>the optimal <span class="math inline">\(\mathrm{MSE}^*=O(C^{-4/(d+4)})\)</span></p></li>
</ul>
<p>This reveals that ABC suffers from the <strong>curse of dimensionality</strong>.</p>
</div>
<div id="improving-the-sampling-efficiency" class="section level2">
<h2>Improving the sampling efficiency</h2>
<p>A possible way to improve ABC efficiency is to accelerate the Monte Carlo. Monte Carlo error is</p>
<p><span class="math display">\[\text{MC error}=\frac{\sigma}{\sqrt{N}}\]</span></p>
<p>Some variance reduction techniques are proposed to reduce <span class="math inline">\(\sigma\)</span>, such as</p>
<ul>
<li>importance sampling</li>
<li>antithetic variates</li>
<li>control variates</li>
<li>hybrid strategies</li>
</ul>
<p>On the other hand, quasi-Monte Carlo is used to <strong>improve the rate of convergence</strong> (<span class="math inline">\(1/\sqrt{N}\)</span>) rather than the constant <span class="math inline">\(\sigma\)</span>.</p>
</div>
<div id="quasi-monte-carlo-a-review" class="section level2">
<h2>Quasi-Monte Carlo: A review</h2>
<p>As a start-up setting, let’s consider an intergal over the unit cube <span class="math inline">\([0,1]^d\)</span>:</p>
<p><span class="math display">\[\mu=\int_{[0,1]^d} f(u_1,\dots,u_d)du_1\cdots du_d.\]</span></p>
<p>MC estimate is the average of <span class="math inline">\(N\)</span> iid samples:</p>
<p><span class="math display">\[\hat\mu_N = \frac 1 N\sum_{i=1}^N f(u^{(i)}),\ u^{(i)}\stackrel{iid}{\sim} U[0,1]^d.\]</span></p>
<p>QMC estimate has the same form but uses deterministic sequences</p>
<p><span class="math display">\[\hat\mu_N = \frac 1 N\sum_{i=1}^N f(u^{(i)}),\ u^{(i)}\in[0,1]^d.\]</span></p>
<p>The sequences are clearly constructed with better uniformness, which are known as <strong>low discrepancy sequences (LDSs)</strong>:</p>
<ul>
<li>Halton (1960)</li>
<li>Sobol’ (1967)</li>
<li>Faure (1982)</li>
<li>Niderreiter (1992)</li>
<li>Lattice rules</li>
</ul>
<p><strong>Koksma-Hlawka inequality</strong> gives</p>
<p><span class="math display">\[
|\hat\mu_N-\mu|\le V_{\mathrm{HK}}(f)D^*(u^{(1)},...,u^{(N)})
\]</span></p>
<ul>
<li><p><span class="math inline">\(V_{\mathrm{HK}}(f)\)</span> is the variation in the sense of Hardy and Krause</p></li>
<li><p><span class="math inline">\(D^*\)</span> is the star discrepancy of the points</p></li>
</ul>
<p>If <span class="math inline">\(V_{\mathrm{HK}}(f)&lt;\infty\)</span> and <span class="math inline">\(\{u^{(1)},...,u^{(N)}\}\)</span> is a LDS, then</p>
<p><span class="math display">\[\text{QMC error}=O(N^{-1}(\log N)^d)\]</span></p>
<p>QMC can achieve higher-order rate of convergence, but needs higher smoothness conditions; see Dick (Ann. Stat., 2011).</p>
<p>Some mathematical softwares such as <code>Matlab</code>, <code>R</code> include some common generators of LDSs.</p>
<pre class="r"><code>#  install.packages(&quot;randtoolbox&quot;)
set.seed(7)
library(&quot;randtoolbox&quot;)
par(mfrow=c(2,2))
qmc = sobol(1024,2)
plot(qmc[1:256,],pch=19,xlab=&quot;x&quot;,ylab=&quot;y&quot;,main=&quot;Sobol&#39; points: N = 256&quot;)
plot(qmc,pch=19,xlab=&quot;x&quot;,ylab=&quot;y&quot;,main=&quot;Sobol&#39; points: N = 1024&quot;)
mc = matrix(runif(2048),ncol = 2)
plot(mc[1:256,],pch=19,xlab=&quot;x&quot;,ylab=&quot;y&quot;,main=&quot;MC points: N = 256&quot;)
plot(mc,pch=19,xlab=&quot;x&quot;,ylab=&quot;y&quot;,main=&quot;MC points: N = 1024&quot;)</code></pre>
<p><img src="/post/QMC_ABC_files/figure-html/unnamed-chunk-1-1.png" width="960" /></p>
</div>
<div id="example-1" class="section level2">
<h2>Example 1</h2>
<p>Consider the integral:</p>
<p><span class="math display">\[\mu = \int_{[0,1]^d} \sum_{i=1}^d x_i^2 dx =\frac{d}{3}.\]</span></p>
<pre class="r"><code>myfun = function(x){
  rowSums(x^2)
  #apply(x-0.5,1,prod)
}

m = 16
N = 2^m
d = 8
trueval = d/3
#trueval = 0
qmc = as.matrix(sobol(n=N,dim=d),ncol=d)
fsum = cumsum(myfun(qmc))
ns = 1:N
fmean = fsum[ns]/ns
par(mar=c(4,4,2,1),mfrow=c(2,1))
tt = paste0(&quot;QMC: d = &quot;,d)
plot(ns,fmean,xlab=&quot;N&quot;,ylab=&quot;Mean&quot;,typ=&quot;l&quot;,main=tt)
abline(h=trueval,lty=5,col=&quot;red&quot;)
ns = 2^(0:m)
fmean = fsum[ns]/ns
err = abs(fmean-trueval)
plot(ns,err,xlab=&quot;N&quot;,ylab=&quot;Error&quot;,typ=&quot;b&quot;,log=&quot;xy&quot;,main=tt)
r = 1
lines(ns[c(3,m)], c(err[3],err[3]*(ns[3]/ns[m])^r),col=&quot;red&quot;,lty=5)
legend(500,err[2],legend = c(&quot;QMC errors&quot;,paste0(&quot;N^{&quot;,-r,&quot;}&quot;)),lty = c(1,5),
       col=c(&quot;black&quot;,&quot;red&quot;),pch=c(1,NA),cex=1.2)</code></pre>
<p><img src="/post/QMC_ABC_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
</div>
<div id="randomized-qmc" class="section level2">
<h2>Randomized QMC</h2>
<p>In practice, we use randomized QMC (RQMC), which yields an unbiased estimate.</p>
<ul>
<li><p>random-shift, see Cranley and Patterson (1976)</p></li>
<li><p>scrambled nets, see Owen (1995,1997,1998)</p></li>
<li><p>Survey in L’Ecuyer and Lemieux (2005)</p></li>
</ul>
<pre class="r"><code>set.seed(7)
library(&quot;randtoolbox&quot;)
par(mfrow=c(2,2))
qmc = sobol(1024,2)
rqmc = sobol(1024,2,scrambling=1)
plot(qmc[1:256,],pch=19,xlab=&quot;x&quot;,ylab=&quot;y&quot;,main=&quot;Sobol&#39; points: N = 256&quot;)
plot(qmc,pch=19,xlab=&quot;x&quot;,ylab=&quot;y&quot;,main=&quot;Sobol&#39; points: N = 1024&quot;)
mc = matrix(runif(2048),ncol = 2)
plot(rqmc[1:256,],pch=19,xlab=&quot;x&quot;,ylab=&quot;y&quot;,main=&quot;RQMC points: N = 256&quot;)
plot(rqmc,pch=19,xlab=&quot;x&quot;,ylab=&quot;y&quot;,main=&quot;RQMC points: N = 1024&quot;)</code></pre>
<p><img src="/post/QMC_ABC_files/figure-html/unnamed-chunk-3-1.png" width="960" /></p>
<pre class="r"><code>myfun = function(x){
  d = ncol(x)
  #(rowSums(x)&gt;d/2)-0.5
  rowSums(x^2)-d/3
  #apply(x-0.5,1,prod)
  
}

m = 16
N = 2^m
d = 2
trueval = 0
R = 100
ns = 2^(0:m)
fmean = matrix(0,m+1,R)
tmp = sobol(N,d)##initialization
for(i in 1:R)
{
  rqmc = as.matrix(sobol(N,d,scrambling=1,init=FALSE),ncol=d)
  fsum = cumsum(myfun(rqmc))
  fmean[,i] = fsum[ns]/ns
}

par(mar=c(4,4,2,1),mfrow=c(2,1))
tt = paste0(&quot;RQMC: d = &quot;,d)
plot(ns,rowMeans(fmean),xlab=&quot;N&quot;,ylab=&quot;Mean&quot;,typ=&quot;b&quot;,main=tt)
abline(h=trueval,lty=5,col=&quot;red&quot;)

rmse = apply(fmean,1,sd)

plot(ns,rmse,xlab=&quot;N&quot;,ylab=&quot;rmse&quot;,typ=&quot;b&quot;,log=&quot;xy&quot;,main=tt)
r = 1
lines(ns[c(3,m)], c(rmse[3],rmse[3]*(ns[3]/ns[m])^r),col=&quot;red&quot;,lty=5)
legend(500,rmse[2],legend = c(&quot;RQMC errors&quot;,paste0(&quot;N^{&quot;,-r,&quot;}&quot;)),lty = c(1,5),
       col=c(&quot;black&quot;,&quot;red&quot;),pch=c(1,NA),cex=1.2)</code></pre>
<p><img src="/post/QMC_ABC_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
<div id="using-qmc-in-abc" class="section level2">
<h2>Using QMC in ABC</h2>
<p>The univariate g-and-k distribution is a flexible unimodal distribution that
is able to describe data with significant amounts of skewness and kurtosis. Its density function has no closed form, but
is alternatively defined through its quantile function as:</p>
<p><span class="math display">\[Q(q|A,B,g,k)=A+B\left[1+c\frac{1-\exp\{-gz(q)\}}{1+\exp\{-gz(q)\}}\right](1+z(q)^2)^kz(q)\]</span></p>
<ul>
<li><p><span class="math inline">\(c=0.8,\ B&gt;0, k&gt;-1/2\)</span>, <span class="math inline">\(z(q)=\Phi^{-1}(q)\)</span></p></li>
<li><p>if <span class="math inline">\(g=k=0\)</span>, it is the normal density</p></li>
<li><p><span class="math inline">\(y_{obs}\)</span> of length <span class="math inline">\(1000\)</span> is generated from the <span class="math inline">\(g\)</span>-and-<span class="math inline">\(k\)</span> distribution with parameter <span class="math inline">\(\theta_0=(3,1,2,0.5)\)</span></p></li>
<li><p>prior density</p></li>
</ul>
<p><span class="math display">\[\pi(\theta) = \pi(A)\pi(B)\pi(g)\pi(k) = N(1,5)\times N(0.25,2) \times U(0,10) \times U(0,1)\]</span></p>
<p>Summary statistic (Drovandi and Pettitt, 2011): <span class="math inline">\(S(y) = (S_A,S_B,S_g,S_k)\)</span></p>
<ul>
<li><span class="math inline">\(S_A=E_4\)</span></li>
<li><span class="math inline">\(S_B=E_6-E_2\)</span></li>
<li><span class="math inline">\(S_g=(E_6+E_2-2E_4)/S_B\)</span></li>
<li><span class="math inline">\(S_k = (E_7-E_5+E_3-E_1)/S_B\)</span></li>
<li><span class="math inline">\(E_1\le E_2 \le \cdots \le E_8\)</span> are the octiles of <span class="math inline">\(y\)</span></li>
</ul>
<pre class="r"><code>set.seed(100)
theta0 = c(3,1,2,0.5)

gkmodel &lt;- function(theta,n,z=NA){
  if(is.na(z)[1]){
    z = rnorm(n)
  }
  y = theta[1] + theta[2]*(1+0.8*(1-exp(-theta[3]*z))/
        (1+exp(-theta[3]*z)))*(1+z^2)^theta[4]*z
  return(matrix(y,n,1))
}
n = 1e3
yobs = gkmodel(theta0,n)
## prior density
gkprior &lt;- function(n,u=NA){
  if(is.na(u)[1]){
    u = matrix(runif(n*4),n,4)
  }
  A = qnorm(u[,1])*sqrt(5)+1
  B = qnorm(u[,2])*sqrt(2)+.25
  g = u[,3]*10
  k = u[,4]
  return(cbind(A,B,g,k))
}
gksummary &lt;- function(y){
  sorty = sort(y)
  n = length(y)
  q = sorty[ceiling(n/8*(1:8))]
  s = c(q[4],
        q[6]-q[2],
        (q[6]+q[2]-2*q[4])/(q[6]-q[2]),
        (q[7]-q[5]+q[3]-q[1])/(q[6]-q[2]))
  return(s)
}
ytmp1 = matrix(0,2000,4)
for(i in 1:2000){
  ytmp1[i,] = gksummary(gkmodel(gkprior(1),n))
}
Sigma = var(ytmp1)
invsig = solve(Sigma)
N = 1e5
ytmp = rep(0,N)
theta = matrix(0,N,4)
stmp = rep(0,N)
sobs = gksummary(yobs)
#qmc = sobol(N+1,n+4)
#qmc = qmc[-1,]##initialization
for(i in 1:N){
  theta[i,] = gkprior(1) # matrix(qmc[i,1:4],1,4)
  ypro = gkmodel(theta[i,],n) # qnorm(matrix(qmc[i,-(1:4)],1,n))
  spro = gksummary(ypro)
  diff = matrix(spro-sobs,1,4)
  ytmp[i] = sqrt(sum((ypro-yobs)^2))
  stmp[i] = sqrt(diff%*%invsig%*%t(diff))
}
ysort = sort(ytmp)
ssort = sort(stmp)
effN = N*5e-3
hy = ysort[effN]
hs = ssort[effN]
## draw pairwise scatterplots
theta1 = theta[ytmp&lt;=hy,]
theta2 = theta[stmp&lt;=hs,]
theta = rbind(theta1,theta2,theta0)
colnames(theta) = c(&quot;A&quot;,&quot;B&quot;,&quot;g&quot;,&quot;k&quot;)
pairs(theta,pch=c(rep(20,effN*2),24),
      col=c(rep(&quot;grey&quot;,effN),rep(&quot;black&quot;,effN),&quot;red&quot;),cex=1)</code></pre>
<p><img src="/post/QMC_ABC_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
</div>
