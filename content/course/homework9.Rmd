---
title: 第九次作业

date: 2018-12-18 
lastmod: 2018-12-18 

draft: false
# toc: true
type: docs

linktitle: 第九次作业
menu:
  course: 
    parent: 数理统计
    weight: 14
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Consider the linear model
$$y_i=\beta_0+\beta_1x_i+\epsilon_i,\ \epsilon_i\stackrel{iid}{\sim} N(0,\sigma^2), i=1,\dots,n.$$

1. Derive the maximum likelihood estimators (MLE) for $\beta_0,\beta_1$. Are they consistent with the least square estimators (LSE)?



2. Derive the MLE for $\sigma^2$ and look at its unbiasedness.


3. A very slippery point is whether to treat the $x_i$ as fixed numbers or as random variables. In the class, we treated the predictors $x_i$ as fixed numbers for sake of convenience. Now suppose that the predictors $x_i$ are iid  random variables (independent of $\epsilon_i$) with density $f_X(x;\theta)$ for some parameter $\theta$. Write down the likelihood function for all of our data $(x_i,y_i),i=1,\dots,n$. Derive the MLE for $\beta_0,\beta_1$ and see whether the MLE changes if we work with the setting of random predictors?


`Solution`: Note that $y_i\sim N(\beta_0+\beta_1 x_i,\sigma^2)$ independently. The likelihood function is

$$L(\beta_0,\beta_1,\sigma^2) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y_i-\beta_0-\beta_1x_i)^2}{2\sigma^2}}=(2\pi\sigma^2)^{-n/2}e^{-\frac{Q(\beta_0,\beta_1)}{2\sigma^2}},$$
where $Q(\beta_0,\beta_1) = \sum_{i=1}^n (y_i-\beta_0-\beta_1x_i)^2$.
For given $\sigma^2$, to maximize $L(\beta_0,\beta_1,\sigma^2)$, it suffices to minimize $Q(\beta_0,\beta_1)$. So the MLEs for $\beta_0,\beta_1$ are consistent with the LSEs, i.e.,

$$\hat\beta_1 = \frac{\ell_{xy}}{\ell_{xx}}=\frac{\sum_{i=1}^n (y_i-\bar y)(x_i-\bar x)}{\sum_{i=1}^n(x_i-\bar x)^2},\ \hat\beta_0 = \bar y -\hat\beta_1\bar x.$$

We then choose $\sigma^2$ to maximize $L(\hat\beta_0,\hat\beta_1,\sigma^2) = (2\pi\sigma^2)^{-n/2}e^{-\frac{Q(\hat\beta_0,\hat\beta_1)}{2\sigma^2}}$.
It is easy to see that the maximizer is 
$$\hat\sigma^2_{MLE} = \frac{Q(\hat\beta_0,\hat\beta_1)}{n}=\frac{S_e^2}{n}.$$

We have proved that $E[S_e^2] = (n-2)\sigma^2$. So $E[\hat\sigma^2_{MLE}] = \frac{n-2}{n}\sigma^2$, which is NOT an unbiased estimate of $\sigma^2$.

If $x_i$ are random variables with density $f_X(x;\theta)$. The likelihood function for $(x_i,y_i)$ is 

$$L(\beta_0,\beta_1,\sigma^2,\theta) =\prod_{i=1}^nf_X(x_i;\theta) f(y_i|x_i) = \prod_{i=1}^nf_X(x_i;\theta) e^{-\frac{(y_i-\beta_0-\beta_1x_i)^2}{2\sigma^2}} =(2\pi\sigma^2)^{-n/2}e^{-\frac{Q(\beta_0,\beta_1)}{2\sigma^2}}\prod_{i=1}^n f_X(x_i;\theta).$$

For fixed $\theta,\sigma^2$, to maximize $L(\beta_0,\beta_1,\sigma^2,\theta)$, it suffices to minimize $Q(\beta_0,\beta_1)$. So the MLE does not changes if we work with the setting of random predictors. 

You can imagine that $(x_i,y_i)$ pairs are generated somewhere and on one day you're given $x_1,\dots,x_n$ independent draws from $f_X$. At that point the data have not told you anything about $\beta_0$ or $\beta_1$. The next
day $y_i|x_i$ are revealed to you. That is informative about $\beta_0$ and $\beta_1$ using $f_{Y|X}(y_i|x_i;\beta_0,\beta_1,\sigma^2)$.
The easier analysis is with $x_i$ fixed, so that is the one we'll do.

---


Consider the linear model without intercept

$$y_i  = \beta x_i+\epsilon_i,\ i=1,\dots,n,$$

where $\epsilon_i$ are independent with $E[\epsilon_i]=0$ and $Var[\epsilon_i]=\sigma^2$.


- Write down the least square estimator $\hat \beta$ for $\beta$, and derive an unbiased estiamtor for $\sigma^2$.

- For fixed $x_0$, let $\hat{y}_0=\hat\beta x_0$. Work out $Var[\hat{y}_0]$.

`Solution`: Let $Q(\beta)= \sum_{i=1}^n (y_i-\beta x_i)^2$. Then we have

$$Q'(\beta) = -\sum_{i=1}^n 2x_i(y_i-\beta x_i).$$

Letting $Q'(\beta) = 0$, we work out the LSE for $\beta$, i.e.,

$$\hat\beta = \frac{\sum_{i=1}^n x_iy_i}{\sum_{i=1}^n x_i^2}.$$

Note that

$$\begin{align}
E[Q(\hat\beta)]&= E[\sum_{i=1}^n y_i^2+\hat\beta^2\sum_{i=1}^n x_i^2-2\hat\beta\sum_{i=1}^n x_iy_i]\\
&= \sum_{i=1}^n \lbrace Var[y_i]+(E[y_i])^2\rbrace-E[\hat\beta\sum_{i=1}^n x_iy_i]\\
&=\sum_{i=1}^n (\sigma^2+\beta^2 x_i^2)-\frac{E[(\sum_{i=1}^n x_iy_i)^2]}{\sum_{i=1}^n x_i^2}\\
&= n\sigma^2+\beta^2\sum_{i=1}^n x_i^2-\frac{Var[\sum_{i=1}^n x_iy_i]+\lbrace E[\sum_{i=1}^n x_iy_i]\rbrace^2}{\sum_{i=1}^n x_i^2}\\
&=(n-1)\sigma^2.
\end{align}
$$

So $\hat\sigma^2 = Q(\hat\beta)/(n-1)$ is an unbiased estimate of $\sigma^2$.

$$
\begin{align}
Var[\hat y_0] &= Var[x_0\hat\beta] = x_0^2Var[\hat\beta]\\
&=x_0^2Var\left[\frac{\sum_{i=1}^n x_iy_i}{\sum_{i=1}^n x_i^2}\right]\\&=\frac{x_0^2}{(\sum_{i=1}^n x_i^2)^2}\sum_{i=1}^n Var[x_i y_i]\\
&=\frac{x_0^2}{(\sum_{i=1}^n x_i^2)^2}\sum_{i=1}^n x_i^2\sigma^2\\
&=\frac{x_0^2\sigma^2}{\sum_{i=1}^n x_i^2}
\end{align}
$$

---


`Case study`: Genetic variability is thought to be a key factor in the survival of a species, the idea
being that “diverse” populations should have a better chance of coping with changing
environments. Table below summarizes the results of a study designed to test
that hypothesis experimentally. Two populations of fruit flies (Drosophila serrata)-one that was cross-bred (Strain A) and the other,
in-bred (Strain B)-were put into sealed containers where food and space were kept
to a minimum. Recorded every hundred days were the numbers of Drosophila alive
in each population.


Date | Day number | Strain A | Strain B |
-|-|-|-|
Feb 2 | 0  | 100  | 100 | 
May 13  | 100  | 250  | 203 | 
Aug 21  | 200  | 304  | 214 | 
Nov 29  | 300  | 403  | 295 | 
Mar 8  | 400  | 446  | 330 | 
Jun 16  | 500  |  482  | 324 | 


- Plot day numbers versus population sizes for Strain A and Strain B, respectively. Does the plot look linear? If so, please use least squares to figure out the coefficients and
their standard errors, and plot the two regression lines.

- Let $\beta_1$ and $\beta_1^*$ be the true slopes (i.e., growth rates) for Strain A and Strain B, respectively. Assume the population sizes for the two strains are independent.  Under the same assumptions of $\epsilon_i\stackrel{iid}{\sim} N(0,\sigma^2)$ for both strains, do we have enough evidence here
to reject the null hypothesis that $\beta_1\le \beta_1^*$ (significance level $\alpha=0.05$)? Or equivalently, do these data support the theory that genetically mixed populations have a
better chance of survival in hostile environments.

`Solution`: 

```{r}
day = seq(0,500,by=100)
A = c(100,250,304,403,446,482)
B = c(100,203,214,295,330,324)
matplot(day,cbind(A,B),pch=1:2,ylab="population size",ylim = c(100,550))
legend(0,550,c("Strain A", "Strain B"),pch = 1:2,col=c("black","red"))
lm.A = lm(A~day)
lm.B = lm(B~day)
abline(coef(lm.A),lty=2)
abline(coef(lm.B),lty=2,col="red")
text(120,300,expression(hat(y)[A] == 145.3 + 0.742 * x))
text(300,200,expression(hat(y)[B] == 131.3 + 0.452 * x),col="red")
output = rbind(summary(lm.A)$coef,summary(lm.B)$coef)
row.names(output) = c("A-Intercept","A-Slope","B-Intercept","B-Slope")
knitr::kable(output,"html",caption = "The coefficients and their standard errors")

```


We now test 

$$H_0:\beta_1\le \beta_1^*,\ H_1:\beta_1>\beta_1^*.$$

Note that $\hat\beta_1 \sim N(\beta_1,\sigma^2/\ell_{xx})$ and $S_{e}^2/\sigma^2\sim \chi^2(n-2)$, where $S_2^2$ is the sum of squared errors for Strain A. Similarly, $\hat\beta_1^* \sim N(\beta_1^*,\sigma^2/\ell_{xx})$ and $\tilde{S}_{e}^2/\sigma^2\sim \chi^2(n-2)$, where $\tilde{S}_{e}^2$ is the sum of squared errors for Strain B. By independence of A and B, we have

$$\hat\beta_1-\hat\beta_1^*\sim N(\beta_1-\beta_1^*,2\sigma^2/\ell_{xx}),$$

$$\frac{S_{e}^2+\tilde{S}_{e}^2}{\sigma^2}\sim \chi^2(2n-4).$$

As a result,

$$\frac{\hat\beta_1-\hat\beta_1^*-(\beta_1-\beta_1^*)}{\sqrt{ \frac{S_{e}^2+\tilde{S}_{e}^2}{(n-2)\ell_{xx}} }}\sim t(2n-4).$$

We thus choose the test statistic

$$T = \frac{\hat\beta_1-\hat\beta_1^*}{\sqrt{ \frac{S_{e}^2+\tilde{S}_{e}^2}{(n-2)\ell_{xx}} }}.$$

If $\beta_1=\beta_1^*$, we have $T\sim t(2n-4)$. The rejection region is $W = \{T>C\}$, where $C$ is satisfied 

$$\sup_{\beta_1\le\beta_1^*}P(T>C|\beta_1,\beta_1^*)=P(T>C|\beta_1=\beta_1^*)=\alpha.$$
We used the fact that the maximum is attainable at the boundary $\beta_1=\beta_1^*$ (WHY?), under which $T\sim t(2n-4)$. So the critical value $C=t_{1-\alpha}(2n-4)$. The observed test statistic is  
$$t = \frac{0.742	-0.452	}{\sqrt{ \frac{5512.14+3960.14}{(6-2)\times 175000} }}=2.50>t_{0.95}(8)=1.8595.$$

We therefore reject the null. These data, then, **do** support the theory that genetically mixed populations have a
better chance of survival in hostile environments.