Let us consider fitting a straight line,
*y* = *β*<sub>0</sub> + *β*<sub>1</sub>*x*, to points
(*x*<sub>*i*</sub>, *y*<sub>*i*</sub>), where *i* = 1, …, *n*.

1.  Write down the normal equations for the simple linear model via the
    matrix formalism.

2.  Solve the normal equations by tha matrix approach and see whether
    the solutions agree with the earlier calculation derived in the
    simple linear models.

`Solution`:

1.  Let *Y* = (*y*<sub>1</sub>, …, *y*<sub>*n*</sub>)<sup>⊤</sup>,
    *β* = (*β*<sub>0</sub>, …, *β*<sub>*p* − 1</sub>)<sup>⊤</sup>,
    *ϵ* = (*ϵ*<sub>1</sub>, …, *ϵ*<sub>*n*</sub>)<sup>⊤</sup>, and let
    *X* be the *n* × 2 matrix
    $$
    X=
    \\left\[
    \\begin{matrix}
    1 & x\_1\\\\
    1 & x\_2\\\\
    \\vdots & \\\\
    1 & x\_n\\\\
    \\end{matrix}
    \\right\].
    $$

The model can be rewritten as
*Y* = *X**β* + *ϵ*.

The normal equations are (*X*<sup>⊤</sup>*X*)*β̂* = *X*<sup>⊤</sup>*Y*.
By simple algebra, we have

$$X^\\top X = \\left\[
\\begin{matrix}
1 & 1 & \\dots & 1\\\\
x\_1 & x\_2 &\\dots & x\_n\\\\
\\end{matrix}
\\right\]\\left\[
\\begin{matrix}
1 & x\_1\\\\
1 & x\_2\\\\
\\vdots & \\\\
1 & x\_n\\\\
\\end{matrix}
\\right\]=\\left\[
\\begin{matrix}
n & \\sum\_{i=1}^n x\_i\\\\
\\sum\_{i=1}^n x\_i & \\sum\_{i=1}^n x\_i^2
\\end{matrix}
\\right\]$$

$$X^\\top Y=\\left\[\\begin{matrix}
1 & 1 & \\dots & 1\\\\
x\_1 & x\_2 &\\dots & x\_n\\\\
\\end{matrix}
\\right\]\\left\[\\begin{matrix}
y\_1\\\\
y\_2\\\\
\\vdots\\\\
y\_n
\\end{matrix}
\\right\]=\\left\[\\begin{matrix}
\\sum\_{i=1}^n y\_i\\\\
\\sum\_{i=1}^n x\_iy\_i
\\end{matrix}
\\right\].
$$

The normal equations turn out to be

$$\\left\[
\\begin{matrix}
n & \\sum\_{i=1}^n x\_i\\\\
\\sum\_{i=1}^n x\_i & \\sum\_{i=1}^n x\_i^2
\\end{matrix}
\\right\]\\left\[\\begin{matrix}
\\hat\\beta\_0\\\\
\\hat\\beta\_1
\\end{matrix}
\\right\]=\\left\[\\begin{matrix}
\\sum\_{i=1}^n y\_i\\\\
\\sum\_{i=1}^n x\_iy\_i
\\end{matrix}
\\right\]$$

As a result,

$$
\\begin{align}
\\left\[\\begin{matrix}
\\hat\\beta\_0\\\\
\\hat\\beta\_1
\\end{matrix}
\\right\]&=\\left\[\\begin{matrix}
n & \\sum\_{i=1}^n x\_i\\\\
\\sum\_{i=1}^n x\_i & \\sum\_{i=1}^n x\_i^2
\\end{matrix}
\\right\]^{-1}\\left\[\\begin{matrix}
\\sum\_{i=1}^n y\_i\\\\
\\sum\_{i=1}^n x\_iy\_i
\\end{matrix}
\\right\]\\\\
&=\\frac{1}{n\\sum\_{i=1}^n x\_i^2-(\\sum\_{i=1}^n x\_i)^2}\\left\[\\begin{matrix}
\\sum\_{i=1}^n x\_i^2 & -\\sum\_{i=1}^n x\_i\\\\
-\\sum\_{i=1}^n x\_i & n
\\end{matrix}
\\right\]\\left\[\\begin{matrix}
\\sum\_{i=1}^n y\_i\\\\
\\sum\_{i=1}^n x\_iy\_i
\\end{matrix}
\\right\]\\\\
&=\\frac{1}{n\\sum\_{i=1}^n x\_i^2-(\\sum\_{i=1}^n x\_i)^2}\\left\[\\begin{matrix}
\\sum\_{i=1}^n x\_i^2\\sum\_{i=1}^n y\_i-\\sum\_{i=1}^n x\_i\\sum\_{i=1}^n x\_iy\_i\\\\
n\\sum\_{i=1}^n x\_iy\_i-\\sum\_{i=1}^n x\_i\\sum\_{i=1}^n y\_i
\\end{matrix}
\\right\]\\\\
&=\\frac{1}{\\ell\_{xx}}\\left\[\\begin{matrix}
\\bar y\\sum\_{i=1}^n x\_i^2-\\bar x\\sum\_{i=1}^n x\_iy\_i\\\\
\\ell\_{xy}
\\end{matrix}
\\right\]\\\\
&=\\frac{1}{\\ell\_{xx}}\\left\[\\begin{matrix}
\\bar y\\ell\_{xx}-\\bar x\\ell\_{xy}\\\\
\\ell\_{xy}
\\end{matrix}
\\right\]=\\left\[\\begin{matrix}
\\bar y-\\bar x\\ell\_{xy}/\\ell\_{xx}\\\\
\\ell\_{xy}/\\ell\_{xx}
\\end{matrix}
\\right\],
\\end{align}
$$

where $\\ell\_{xx} = \\sum\_{i=1}^n(x\_i-\\bar x)^2,$
$\\ell\_{yy} = \\sum\_{i=1}^n(y\_i-\\bar y)^2,$
$\\ell\_{xy} = \\sum\_{i=1}^n(x\_i-\\bar x)(y\_i-\\bar y).$

The solutions agree with the earlier calculation derived in the simple
linear models.

------------------------------------------------------------------------

Prove that the projection matrix
*P* = *X*(*X*<sup>⊤</sup>*X*)<sup> − 1</sup>*X*<sup>⊤</sup> has an
eigenvalue 1, and (1, …, 1)<sup>⊤</sup> is one of the associated
eigenvectors.

`Proof`: Note that
*P**X* = *X*(*X*<sup>⊤</sup>*X*)<sup> − 1</sup>*X*<sup>⊤</sup>*X* = *X*.
The first column of *X* is **1** := (1, …, 1)<sup>⊤</sup>. This implies
that *P***1** = **1**, which completes the proof.

------------------------------------------------------------------------

(The QR Method) This problem outlines the basic ideas of an alternative
method, the QR method, of finding the least squares estimate *β̂*. An
advantage of the method is that it does not include forming the matrix
*X*<sup>⊤</sup>*X*, a process that tends to increase rounding error. The
essential ingredient of the method is that if *X*<sub>*n* × *p*</sub>
has *p* linearly independent columns, it may be factored in the form

$$
\\begin{align}
X\\quad &=\\quad Q\\quad \\quad R\\\\
n\\times p &\\quad  n\\times p\\quad p\\times p
\\end{align}
$$

where the columns of *Q* are orthogonal
(*Q*<sup>⊤</sup>*Q* = *I*<sub>*p*</sub>) and *R* is upper-triangular
(*r*<sub>*i**j*</sub> = 0, for *i* &gt; *j*) and nonsingular. For a
discussion of this decomposition and its relationship to the
Gram-Schmidt process, see
<https://en.wikipedia.org/wiki/QR_decomposition>.

Show that *β̂* = (*X*<sup>⊤</sup>*X*)<sup> − 1</sup>*X*<sup>⊤</sup>*Y*
may also be expressed as *β̂* = *R*<sup> − 1</sup>*Q*<sup>⊤</sup>*Y*, or
*R**β̂* = *Q*<sup>⊤</sup>*Y*. Indicate how this last equation may be
solved for *β̂* by back-substitution, using that *R* is upper-triangular,
and show that it is thus unnecessary to invert *R*.

`Solution`: Since *X* = *Q**R*,
(*X*<sup>⊤</sup>*X*)<sup> − 1</sup>*X*<sup>⊤</sup> = (*R*<sup>⊤</sup>*Q*<sup>⊤</sup>*Q**R*)<sup> − 1</sup>*R*<sup>⊤</sup>*Q*<sup>⊤</sup> = (*R*<sup>⊤</sup>*R*)<sup> − 1</sup>*R*<sup>⊤</sup>*Q*<sup>⊤</sup> = *R*<sup> − 1</sup>*Q*<sup>⊤</sup>.

Therefore, *β̂* = *R*<sup> − 1</sup>*Q*<sup>⊤</sup>*Y*, or equivalently,
*R**β̂* = *Q*<sup>⊤</sup>*Y* =  : *b* = (*b*<sub>1</sub>, …, *b*<sub>*p*</sub>)<sup>⊤</sup>.
Since *R* is upper-triangular, then the normal equations are

$$
\\begin{align}
r\_{pp} \\hat\\beta\_{p-1} &= b\_p\\\\
r\_{p-1,p-1} \\hat\\beta\_{p-2}+ r\_{p-1,p}\\hat\\beta\_{p-1} &= b\_{p-1}\\\\
\\vdots&\\\\
r\_{11} \\hat\\beta\_{0}+ r\_{12}\\hat\\beta\_{1} +\\dots +r\_{1p}\\hat\\beta\_{p-1}&= b\_{1}
\\end{align}.
$$

This can be sloved by back-substitution:

$$
\\begin{align}
\\hat\\beta\_{p-1} &= \\frac{b\_p}{r\_{pp}}\\\\
 \\hat\\beta\_{i} &=\\frac{b\_{i+1}}{r\_{i+1,i+1}} - \\frac{1}{r\_{i+1,i+1}}\\sum\_{j=i+2}^p r\_{i+1,j}\\hat\\beta\_{j-1},\\ i=p-2,\\dots,0
\\end{align}.
$$

------------------------------------------------------------------------

Consider fitting the curve
*y* = *β*<sub>0</sub>*x* + *β*<sub>1</sub>*x*<sup>2</sup> to points
(*x*<sub>*i*</sub>, *y*<sub>*i*</sub>), where *i* = 1, …, *n*.

1.  Use the matrix formalism to find expressions for the least squares
    estimates of *β*<sub>0</sub> and *β*<sub>1</sub>.

2.  Find an expression for the covariance matrix of the estimates.

`Solution`: Let
*Y* = (*y*<sub>1</sub>, …, *y*<sub>*n*</sub>)<sup>⊤</sup>,
*β* = (*β*<sub>0</sub>, …, *β*<sub>*p* − 1</sub>)<sup>⊤</sup>,
*ϵ* = (*ϵ*<sub>1</sub>, …, *ϵ*<sub>*n*</sub>)<sup>⊤</sup>, and let *X*
be the *n* × 2 matrix
$$
X=
\\left\[
\\begin{matrix}
x\_1 & x\_1^2\\\\
x\_2 & x\_2^2\\\\
\\vdots & \\\\
x\_n & x\_n^2\\\\
\\end{matrix}
\\right\].
$$

The model can be rewritten as
*Y* = *X**β* + *ϵ*.

The normal equations are (*X*<sup>⊤</sup>*X*)*β̂* = *X*<sup>⊤</sup>*Y*.
By simple algebra, we have

$$X^\\top X = \\left\[
\\begin{matrix}
x\_1 & x\_2 & \\dots & x\_n\\\\
x\_1^2 & x\_2^2 &\\dots & x\_n^2\\\\
\\end{matrix}
\\right\]\\left\[
\\begin{matrix}
x\_1 & x\_1^2\\\\
x\_2 & x\_2^2\\\\
\\vdots & \\\\
x\_n & x\_n^2\\\\
\\end{matrix}
\\right\]=\\left\[
\\begin{matrix}
\\sum\_{i=1}^n x\_i^2 & \\sum\_{i=1}^n x\_i^3\\\\
\\sum\_{i=1}^n x\_i^3 & \\sum\_{i=1}^n x\_i^4
\\end{matrix}
\\right\]$$

$$X^\\top Y=\\left\[\\begin{matrix}
x\_1 & x\_2 & \\dots & x\_n\\\\
x\_1^2 & x\_2^2 &\\dots & x\_n^2\\\\
\\end{matrix}
\\right\]\\left\[\\begin{matrix}
y\_1\\\\
y\_2\\\\
\\vdots\\\\
y\_n
\\end{matrix}
\\right\]=\\left\[\\begin{matrix}
\\sum\_{i=1}^n x\_iy\_i\\\\
\\sum\_{i=1}^n x\_i^2y\_i
\\end{matrix}
\\right\].
$$

The normal equations turn out to be

$$\\left\[
\\begin{matrix}
\\sum\_{i=1}^n x\_i^2 & \\sum\_{i=1}^n x\_i^3\\\\
\\sum\_{i=1}^n x\_i^3 & \\sum\_{i=1}^n x\_i^4
\\end{matrix}
\\right\]\\left\[\\begin{matrix}
\\hat\\beta\_0\\\\
\\hat\\beta\_1
\\end{matrix}
\\right\]=\\left\[\\begin{matrix}
\\sum\_{i=1}^n x\_iy\_i\\\\
\\sum\_{i=1}^n x\_i^2y\_i
\\end{matrix}
\\right\]$$

Let $s\_x^k = \\sum\_{i=1}^n x\_i^k$, $s\_y^k = \\sum\_{i=1}^n y\_i^k$,
$s\_{xy}^{jk}=\\sum\_{i=1}^n x\_i^jy\_i^k$. As a result,

$$
\\begin{align}
\\left\[\\begin{matrix}
\\hat\\beta\_0\\\\
\\hat\\beta\_1
\\end{matrix}
\\right\]&=\\left\[
\\begin{matrix}
s\_x^2 & s\_x^3\\\\
s\_x^3 & s\_x^4
\\end{matrix}
\\right\]^{-1}\\left\[\\begin{matrix}
s\_{xy}^{11}\\\\
s\_{xy}^{21}
\\end{matrix}
\\right\]\\\\
&=\\frac{1}{s\_x^2s\_x^4-(s\_x^3)^2}\\left\[\\begin{matrix}
s\_x^4 & -s\_x^3\\\\
-s\_x^3 & s\_x^2
\\end{matrix}
\\right\]\\left\[\\begin{matrix}
s\_{xy}^{11}\\\\
s\_{xy}^{21}
\\end{matrix}
\\right\]\\\\
&=\\left\[\\begin{matrix}
\\frac{s\_x^4s\_{xy}^{11}-s\_x^3s\_{xy}^{21}}{s\_x^2s\_x^4-(s\_x^3)^2}\\\\
\\frac{s\_x^2s\_{xy}^{21}-s\_x^3s\_{xy}^{11}}{s\_x^2s\_x^4-(s\_x^3)^2}
\\end{matrix}
\\right\].
\\end{align}
$$

Note that
*V**a**r*\[*β̂*\] = *V**a**r*\[(*X*<sup>⊤</sup>*X*)<sup> − 1</sup>*X*<sup>⊤</sup>*Y*\] = (*X*<sup>⊤</sup>*X*)<sup> − 1</sup>*X*<sup>⊤</sup>*V**a**r*\[*ϵ*\]*X*(*X*<sup>⊤</sup>*X*)<sup> − 1</sup>.

For the usual assumption
*V**a**r*\[*ϵ*\] = *σ*<sup>2</sup>*I*<sub>*n*</sub>, then
$$Var\[\\hat\\beta\]=\\sigma^2 (X^\\top X)^{-1} =\\frac{\\sigma^2}{s\_x^2s\_x^4-(s\_x^3)^2}\\left\[\\begin{matrix}
s\_x^4 & -s\_x^3\\\\
-s\_x^3 & s\_x^2
\\end{matrix}
\\right\].$$
