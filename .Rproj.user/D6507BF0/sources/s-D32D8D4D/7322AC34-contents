---
title: "第四次作业"
date: "2018-10-12"
categories: ["作业"]
Summary: "答案已上传！"
tags: ["作业", "数理统计"]
---
  
  ```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

设$X_1,\dots,X_n$为来自参数为$\lambda$的Poisson分布的样本. 在下列选项中选出用于估计参数$\lambda$的无偏估计量。**答案：ABCE**

A. $\bar X$

B. $S_n^{*2}=\frac{1}{n-1}\sum_{i=1}^{n}(X_i-\bar X)^2$

C. $\frac 1 {n-1}\sum_{i=1}^{n-1}X_i$

D. $S_n^2=\frac{1}{n}\sum_{i=1}^n(X_i-\bar X)^2$

E. $\frac{1}2 \bar X + \frac 12 S_n^{*2}$

---

设$X_1,\dots,X_n$为来自参数为$\lambda$的Poisson分布的样本, 已知$\bar X$是未知参数$\lambda$的完全统计量。在下列选项中选出用于估计参数$\lambda$的最有效的估计量。**答案：A**

A. $\bar X$

B. $S_n^{*2}=\frac{1}{n-1}\sum_{i=1}^n(X_i-\bar X)^2$

C. $\frac 1 {n-1}\sum_{i=1}^{n-1}X_i$

D. $\frac{1}2 \bar X + \frac 12 S_n^{*2}$

---

设$X,\dots,X_n$为来自参数为$\lambda$的Poisson分布的样本，求$\lambda^2$的无偏估计。已知$\bar X$是参数$\lambda$的完全统计量，能否找到$\lambda^2$的最小方差无偏估计量？

`解`: 1. $\lambda^2$的无偏估计有很多种答案：

- 因为$E[X]=Var[\lambda]=\lambda$, 所以$E[X^2]=\lambda+\lambda^2=E[X]+\lambda^2$，由矩法得到一种无偏估计量：
$$\hat{\lambda^2}_1 = \frac{1}{n}\sum_{i=1}^n(X_i^2-X_i)$$

- 因为$E[\bar X]=\lambda$, $$E(\bar X^2) = Var(\bar X)+(E[\bar X])^2=\lambda/n+\lambda^2=E[\bar X]/n+\lambda^2,$$
于是可以得到一种无偏估计量：
$$\hat{\lambda^2}_2 = (\bar X)^2-\bar X/n$$

- 当然还可以构造无穷多种无偏估计量：
$$\hat{\lambda^2}_3 = \alpha \hat{\lambda^2}_1+(1-\alpha)\hat{\lambda^2}_2,\forall \alpha\in [0,1].$$



(2) 虽然无偏统计量有很多，但是最小方差无偏估计量是唯一的（在概率意义下）。由于$\bar X$是充分完全统计量，所以由B-L-S定理知，$\hat{\lambda^2}_2$是最小方差无偏的。

> 有一部分同学想通过$\psi(\bar X)=E[\hat{\lambda^2}_1|\bar X]$的得到最小无偏估计量，思路是对的，但是如何计算$\psi(\bar X)$就没那么容易了。

为了解决这个问题，我们需要计算在给定$\bar X= t$下，样本$(X_1,\dots,X_n)$的条件分布。容易计算样本的联合分布为
$$P(X_1=x_1,\dots,X_n=x_n)=\prod_{i=1}^nP(X_i=x_i) = \prod_{i=1}^n \frac{e^{-\lambda}\lambda^{x_i}}{x_i!}=\frac{e^{-n\lambda}\lambda^{n\bar x}}{\prod_{i=1}^nx_i!}$$

由于Possion分布的可加性，有$\sum_{i=1}^n X_i\sim Possion(n\lambda)$, 所以
$$P(\bar X=t)=P(\sum_{i=1}^n X_i=nt)=\frac{e^{-n\lambda}(n\lambda)^{nt}}{(nt)!}$$

于是，给定$\bar X= t$下，样本$(X_1,\dots,X_n)$的条件分布为：当$\sum_{i=1}^nx_i=nt$时，
$$P(X_1=x_1,\dots,X_n=x_n|\bar X=t)=\frac{\frac{e^{-n\lambda}\lambda^{nt}}{\prod_{i=1}^nx_i!}}{\frac{e^{-n\lambda}(n\lambda)^{nt}}{(nt)!}}=\frac{(nt)!}{n^{nt}\prod_{i=1}^nx_i!}, $$
其他情况下，该条件概率为0. 我们发现
$$\psi(t)=E[\hat{\lambda^2}_1|\bar X=t]=E[\frac{1}{n}\sum_{i=1}^nX_i^2-\bar X|\bar X=t]=E[n(\bar X)^2-\frac{1}{n}\sum_{i\neq j}X_iX_j-\bar X|\bar X=t]=nt^2-t-\frac{1}{n}E[\sum_{i\neq j}X_iX_j|\bar X=t].$$

由于对称性，$E[\sum_{i\neq j}X_iX_j|\bar X=t]=n(n-1)E[X_1X_2|\bar X=t]$, 所以只需计算$E[X_1X_2|\bar X=t]$即可：
$$E[X_1X_2|\bar X=t]=\sum_{\vec x:\sum_{i=1}^nx_i=nt}\frac{x_1x_2(nt)!}{n^{nt}\prod_{i=1}^nx_i!}=\frac{(nt)!}{n^{nt}}\sum_{\vec x:\sum_{i=1}^nx_i=nt}\frac{x_1x_2}{\prod_{i=1}^nx_i!}$$

由于
$$\sum_{\vec x:\sum_{i=1}^nx_i=nt}\frac{x_1x_2}{\prod_{i=1}^nx_i!}=\sum_{\vec x:\sum_{i=1}^nx_i=nt,x_1\ge 1,x_2\ge 1}\frac{1}{(x_1-1)!(x_2-1)!\prod_{i=3}^nx_i!}=\sum_{\vec x:\sum_{i=1}^nx_i=nt-2}\frac{1}{\prod_{i=1}^nx_i!}=\frac{n^{nt-2}}{(nt-2)!}$$

所以，
$$\psi(t)=nt^2-t-(n-1)E[X_1X_2|\bar X=t]=nt^2-t-(n-1)\frac{(nt)!}{n^{nt}}\frac{n^{nt-2}}{(nt-2)!}=t^2-\frac{t}{n}$$

这表明，$\psi(\bar X)=(\bar X)^2-\bar X/n$, 也就是$\hat{\lambda^2}_2$. 两种方式得到的最小方差无偏估计量是一致的。这也印证了最小方差无偏估计量是唯一的。显然第一种方式比较简单，第二种方式需要求条件期望，这个比较复杂，一般情况下不容易求解。


---

设$X_1,\dots,X_n$为$N(\mu,\sigma^2)$分布的样本，参数$\mu,\sigma^2$未知。证明样本方差$S_n^2$与修正样本方差$S_n^{*2}$均为$\sigma^2$的弱相合估计量。

`解`： 由抽样分布定理知，$\frac{\sum_{i=1}^n (X_i-\bar X)^2}{\sigma^2}\sim \chi^2(n-1)$, 所以
$$Var[\sum_{i=1}^n (X_i-\bar X)^2]=2(n-1)\sigma^4$$

于是，

$$Var[S_n^2]=Var[\sum_{i=1}^n (X_i-\bar X)^2]/n^2=\frac{2(n-1)\sigma^4}{n^2}\to 0\text{ as }n\to \infty$$

$$Var[S_n^{*2}]=Var[\sum_{i=1}^n (X_i-\bar X)^2]/(n-1)^2=\frac{2\sigma^4}{n-1}\to 0\text{ as }n\to \infty$$

由于$E[S_n^{*2}]=\sigma^2,\lim_{n\to\infty}E[S_n^2]=\sigma^2$, 由弱相合性判别条件知，它们都是弱相合的。

---

设$X_1,\dots,X_n$为$N(\mu,\sigma^2)$分布的样本，参数$\mu,\sigma^2$未知。样本方差$S_n^2$与修正样本方差$S_n^{*2}$作为$\sigma^2$的两种估计量，哪个更有效？由B-L-S定理知，$S_n^{*2}$是最小方差无偏估计量，这是否与你所得的结论矛盾？由此你能得到什么启发？

`解`: 由于$S_n^{*2}$是无偏的，所以均方误差
$$M(S_n^{*2}) = Var[S_n^{*2}]=\frac{2\sigma^4}{n-1}$$

对$S_n^{2}$, 其均方误差为
$$M(S_n^{*2}) = Var[S_n^{2}]+(E[S_n^2]-\sigma^2)^2=\frac{2(n-1)\sigma^4}{n^2}+(\frac{(n-1)\sigma^2}{n}-\sigma^2)^2=\frac{(2n-1)\sigma^4}{n^2}$$

又
$$\frac{M(S_n^{*2})}{M(S_n^{2})}=\frac{2n^2}{(n-1)(2n-1)}>1$$

所以，$S_n^{2}$比$S_n^{*2}$有效。这与“$S_n^{*2}$是最小方差无偏估计量”不矛盾，因为$S_n^{2}$是有偏估计量。

> 启发：无偏估计量不一定是最有效的。

---


设$X_1,\dots,X_n$为总体$N(\mu,\sigma^2)$, 其中$\mu$已知，$\sigma^2$未知。证明$\sigma^2$的估计量
$$T(X_1,\dots,X_n)=\frac 1n\sum_{i=1}^n(X_i-\mu)^2$$
的方差达到C-R不等式的下界。

`解`: 令$\theta=\sigma^2,f(x;\theta)$为总体密度函数。于是， 
$$\log f(x;\theta)= \log  \frac{1}{\sqrt{2\pi}\sqrt{\theta}}e^{-\frac{(x-\mu)^2}{2\theta}}=-(1/2)\log(2\pi\theta)-\frac{(x-\mu)^2}{2\theta}$$

$$\frac{d\log f(x;\theta)}{d\theta}=-\frac{1}{2\theta}+\frac{(x-\mu)^2}{2\theta^2}$$

所以，Fisher信息量为：
$$I(\theta)=E[(\frac{d\log f(X;\theta)}{d\theta})^2]=\frac{1}{4\theta^2}E[(\frac{(X-\mu)^2}{\theta}-1)^2]=\frac{1}{2\theta^2}$$

所以，C-R不等式下界为:
$$\frac{1}{nI(\theta)}=\frac{2\sigma^4}{n}$$

因为$nT/\sigma^2\sim \chi^2(n)$, 所以
$$Var[nT/\sigma^2]=2n$$

于是，$Var[T]=2\sigma^4/n$达到C-R不等式下界。


